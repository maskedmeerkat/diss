% !TeX root = ../main.tex
\chapter{Research Approach}
\label{ch:research_approach}
The main objective of this thesis is to answer the question of how to extend an already verified \gls{ism} with the predictions of an unverified one. More specifically, the scenario of a given geo \gls{irm} is considered. It is assumed that this \gls{irm} is verified but produces sparse occupancy estimates since it relies on sparse radar detections. This leads to reduced coverage and slow convergence during occupancy mapping. To increase the coverage and convergence speed, the dense interpolations of a learned \gls{irm} shall be utilized.
\\\\
To achieve this, the objective can be divided into the following three goals. First, a model shall be learned from data which is capable of estimating the evidential occupancy state in the close vicinity of the ego vehicle. Here, the focus will lie on measurements in the form of sparse radar detections. However, to showcase the generalizability of the approach, the model will also be applied on two other sensors typically deployed for automated driving, namely camera and lidar data. Next, the deep \gls{irm}s estimates shall be fused over time into an evidential occupancy map. Finally, a fusion approach shall be defined to combine the data-driven model's predictions with a geo \gls{ism}.
\\\\
The consequent sections elaborate the requirements for each of the three aforementioned tasks which is followed by an analysis of the research gap and concluded by the method to solve the problem. 
%==========================================================================%
%
%==========================================================================%
\section{Requirements}
\label{sec:requirements}
This section details the requirements for both the trained \gls{ism} and the fusion approach.
%======================================%
%
%======================================%
\subsection{Requirements for trained ISMs}
\label{subsec:requirements_for_ev_representation}
The requirements for the learned \gls{ism} are of theoretical as well as practical nature. First, to obtain a generalized measurement representation, \gls{bev} grid maps shall be used as inputs. This is the de facto standard for trained \gls{ism}s in literature given point cloud inputs (see Section \ref{sec:deep_isms}). Moreover, while it might not be without loss of information, other sensor data e.g. provided by cameras can also be transformed into \gls{bev}. Additionally, the outputs shall also be provided as \gls{bev} grid maps to ease the later fusion into \gls{bev} occupancy maps. Therefore, the following requirement can be formulated.
\\
\setcounter{req}{1}
\setcounter{subreq}{0}
\begin{subreq}[R\ref{subreq:input_output}] \label{subreq:input_output}
	The trained \gls{ism} must be capable of utilizing the spatial coherence in \gls{bev} grid maps and deliver estimates also in the form of \gls{bev} grid maps.
\end{subreq}
Secondly, the sensor data can include many different sources of noise which is especially true when it comes to radar. In order for the model to learn all these effects, 
\\
\begin{subreq}[R\ref{subreq:big_data}] \label{subreq:big_data}
	The trained \gls{ism} must be capable to learn from big data.
\end{subreq}
Third, the trained \gls{ism} should be obtained as resource efficient as possible. Therefore, the following requirement arises
\\
\begin{subreq}[R\ref{subreq:min_requirements}] \label{subreq:min_requirements}
	Minimize the amount of manpower, work hours and equipment needed to create the trained \gls{ism}.
\end{subreq}
Also, the data-driven \gls{ism} should be able to run in parallel with the already existing geo \gls{ism} on the hardware of production-ready vehicles in real-time. The sensor with the highest capture frequency in the NuScenes dataset is the lidar sensor with 20 Hz. To provide some room to perform other computations, it is proposed to aim for a 100 Hz inference time of the trained \gls{ism}. To emulate these hardware restrictions, the requirement can be formulated as follows
\\
\begin{subreq}[R\ref{subreq:resource_efficient_inference}] \label{subreq:resource_efficient_inference}
	The trained \gls{ism} shall be executable with 100 Hz on a single core of a CPU (Intel Core Processor i7-10750H).
\end{subreq}
For this work, the width and height of input and output grid maps shall be $128 \times 128$ cells for an area of $40$ m $\times$ $40$ m. This is an acceptable range for low speed scenarios like parking. Additionally, the resolution of $31.25$ cm per cell is satisfactory for the trained \gls{ism}, as it is mainly used to enhance the geo \gls{ism}. Thus, leading to the following requirement
\\
\begin{subreq}[R\ref{subreq:grid_map_size}] \label{subreq:grid_map_size}
	The input and output grid maps shall cover an area of $40$ m $\times$ $40$ m with $128 \times 128$ cells.
\end{subreq}
Finally, on the theoretical side, the model shall estimate the evidential classes as defined in \ref{sec:occupancy_mapping}, leading to the following requirements
\\
\begin{subreq}[R\ref{subreq:ev_rep}] \label{subreq:ev_rep}
	The predicted output should capture the amount of free, occupied and unknown information.
\end{subreq}
\begin{subreq}[R\ref{subreq:unknown_mass}] \label{subreq:unknown_mass}
	The unknown mass should be an inverse measure for the overall information content capturing both uncertainty and lack of information.
\end{subreq}
\begin{subreq}[R\ref{subreq:conflicting_mass}] \label{subreq:conflicting_mass}
	The amount of conflicting information, which is realized by mass being evenly distributed both into the free and occupied class, shall be an indicator for dynamic objects.
\end{subreq}

%======================================%
%
%======================================%
\subsection{Requirements for Usage of trained ISMs as Priors in Occupancy Mapping}
\label{subsec:requirements_for_usage_of_deep_ims_as_priors_in_occmapping}
To enable the fusion of the trained with the geo \gls{ism}s into a common representation, the trained \gls{ism} estimates have to suffice the additional specification for \gls{ism}s used in occupancy mapping, as defined in Section \ref{sec:occupancy_mapping}, namely
\\
\setcounter{req}{2}
\setcounter{subreq}{0}
\begin{subreq}[R\ref{subreq:indep_info}] \label{subreq:indep_info}
	The trained \gls{ism} estimates have to be informational independent over time.
\end{subreq}
Additionally, trained \gls{ism}s do also provide predictions in regions further away from sensor measurements through means of data-driven interpolation. This poses the potential to overwrite high certain predictions close to data through many low certain predictions accumulated over time. Therefore, the accumulation of trained \gls{ism} estimates shall be performed in a way that
\\
\begin{subreq}[R\ref{subreq:no_falsification}] \label{subreq:no_falsification}
	Regions assigned with high certainty shall not be overwritten by many estimates with low certainty. 
\end{subreq}
Eventually, as mentioned above, the geo \gls{ism} is considered to be a verified, production-ready model which shall solely be enhanced by the trained \gls{ism} estimates to increase convergence speed and spatial coverage of the occupancy maps. Thus, given enough measurements, the geo \gls{ism} should be trusted over the trained \gls{ism} resulting in the following requirement
\\
\begin{subreq}[R\ref{subreq:initialize_with_deep_ism}] \label{subreq:initialize_with_deep_ism}
	The occupancy map shall be initialized with the trained \gls{ism}'s estimates up to the point when a definable amount of measurement information has been collected.
\end{subreq}
\begin{subreq}[R\ref{subreq:converge_to_geo_ism}] \label{subreq:converge_to_geo_ism}
	The occupancy map shall converge to the geo \gls{ism} with increasing amount of measurements.
\end{subreq}
%==========================================================================%
%
%==========================================================================%
\section{Review of the State-of-the-Art and Research Needs}
\label{sec:research_needs}
In this section, the \gls{sota} defined in Chapter \ref{ch:state_of_the_art} is analyzed with respect to the requirements defined in Section \ref{sec:requirements}.
%======================================%
%
%======================================%
\subsection{Research Needs and Review of State-of-the-Art for Geo ILMs and IRMs}
\label{subsec:research_needs_for_geo_ilms_n_irms}
The first step in order to analyze trained \gls{ism}s is the definition of a reference and baseline model. To generate \gls{bev} occupancy targets resource efficiently, as demanded in R\ref{subreq:min_requirements}, the \gls{sota} in all cases relies on automatic label generation using geo \gls{ilm}s from 360$^\circ$ spinning lidars. For most automated driving test vehicles, these types of lidars are already deployed for verification, making lidar data easily accessible. Moreover, after once manually defining the geo \gls{ilm}, no additional manual labor is required. Therefore, the automatic generation of \gls{bev} occupancy target via geo \gls{ilm}s suffices R\ref{subreq:min_requirements}.
\\\\
To generate geo \gls{ilm}s, the \gls{sota} procedure only contains one problem which revolves around the fact that the radar sensors used in this work have reduced perception capabilities when compared to the lidars. More specifically, the deployed radars can detect objects in 3D but, due to their antenna configuration, can only distinguish the 2D position and velocity. These measurements are additionally filtered and broken down to only a few detections, e.g. 64 detections for the radars in this work. Based on this, the question arises which lidar detections should be filtered out to obtain the best overlap between the two sensor modalities. So far in the literature, only threshold-based ground plane removal has been proposed to adapt the lidar detections and only for specific datasets not including NuScenes. Since a proper overlap between target and input information is important to reduce potential outliers and because the specific sensor orientation might have an influence on the perceptive capabilities, a more thorough investigation based on the sensor setup given in NuScenes shall be performed. To narrow the methods down, the often applied threshold-based method shall be compared with different semantic segmentation-based ground plane removal results. This can be formulated as follows
\\
\begin{requ}[RQ\ref{requ:what_is_best_gt}] \label{requ:what_is_best_gt}
	Which of the following methods results in the best overlap with respect to \gls{iou} between accumulated lidar and radar \gls{ism}s: threshold-based or semantic segmentation-based ground plane removal?
\end{requ}
To be able to investigate RQ\ref{requ:what_is_best_gt}, it is also necessary to define the geo \gls{irm} algorithm. This geo \gls{irm} will be used both, to define the best overlap between the radar and lidar modality and as a baseline to further evaluate trained \gls{ism} variants. Regarding the geo \gls{irm}'s \gls{sota}, two problems can be identified. First, the dampening factor, proposed in the \gls{sota}, reduces the influence of outliers and multi-path reflections depending on the detection range, angle and ego vehicle velocity. Preliminary experiments have shown that this results in a computationally heavy geo \gls{irm} with many hyperparameters. To tune this geo \gls{irm} for generalized urban scenarios, many variants of the computational heavy model have to be evaluated on big data. This makes the \gls{sota} geo \gls{irm} impractical to tune in practices. Therefore, a simplified version of the geo \gls{irm} is used in this work which does not specifically scale the \gls{idm} based on range, angle and ego vehicle velocity.
\\\\
The second problem revolves around the enrichment of free space. Here, the \gls{sota} proposes to define additional free space between the maximum absolute angles of the radar's \gls{fov} cone and its closest detection. This assumes high certainty of the sensor towards the edges of the \gls{fov} cone in order to make the implication of free space based on absence of detections. The remaining literature, however, does not support this assumption, which can be seen by the scaling factor $G_\varphi$ in Eq. \ref{eq:clarke_scaling} which reduces the \gls{idm}'s certainty towards the \gls{fov}'s edges in angular direction. Thus, the need arises to provide a free space enrichment method which better suits the sensor certainty
\\
\begin{requ}[RQ\ref{requ:fr_space_enrichment}] \label{requ:fr_space_enrichment}
	Define a procedure to enrich the baseline \gls{idm} ray casting free space predictions in regions lacking detections in a way that strictly increases the m\gls{iou} between radar- and lidar-based occupancy maps.
\end{requ}
%======================================%
%
%======================================%
\subsection{Research Needs for deep, evidential Inverse Sensor Models}
\label{subsec:research_needs_for_deep_isms}
Given the target and baseline model, the creation of trained \gls{ism}s can be studied in more detail. Here, the current \gls{sota} approaches all tackle the creation of trained \gls{ism}s by applying \gls{cnn}s in the form of UNets with skip connections. While the interpretation of the input varies between it being an image or some kind of multi-channel \gls{bev} grid map, \gls{cnn}s are an appropriate choice for they are designed to leverage spatial context from matrix-like data. Moreover, through breakthroughs like Dropout for regularization, BatchNorm for normalization and skip connections for conservation of information, current \gls{cnn} models can be designed with increased number of stacked layers. This results in increased modeling capacities allowing them to capture the information from big amounts of data. Thus, the application of UNets as the de facto standard model already suffices R\ref{subreq:input_output} and R\ref{subreq:big_data}.
\\\\
Regarding the resource efficiency during inference, the literature only discloses run time information on GPUs. Therefore, a rough architecture search shall be conducted for UNets with skip connections and ResNet layers to answer the following question
\\
\begin{requ}[RQ\ref{requ:network_search}] \label{requ:network_search}
	How should the amount of filters of a UNet architecture be chosen to maximize performance while sufficing the run time requirement as stated in R\ref{subreq:resource_efficient_inference}?
\end{requ}
Additionally, the majority of deep \gls{ism}s in the literature model the problem in the probabilistic framework which does not suffice R\ref{subreq:ev_rep}. On the evidential side, the problem is either modeled as a three class classification or a regression task, both of which suffice R\ref{subreq:ev_rep}. However, to the best of the authors knowledge, non of the published methods model dynamic objects by distributing mass equally to the free and occupied class. Training on dynamic object targets disqualifies the standard classification approach, since the dynamic object targets are not represented as a one-hot encoding. On the other hand, regression problems can cope with continuous targets. Thus, the training of deep \gls{ism}s will be defined as a regression problem in this work. However, no prior work has provided a thorough comparison of deep \gls{ism}'s capabilities to estimate evidential masses given different sensor inputs. Which is especially true for the dynamic class which is in most of the literature not modeled for occupancy mapping. Thus, the following research questions arise
\\
\begin{requ}[RQ\ref{requ:comparison_of_isms}] \label{requ:comparison_of_isms}
	To which extend, as measured by the normed confusion matrix (see Section \ref{subsec:confusion_matrix}), are the proposed deep \gls{ism}s capable to estimate the position of dynamic, free and occupied areas given radar, camera and lidar data respectively?
\end{requ}
\begin{requ}[RQ\ref{requ:comparison_of_isms_fusion}] \label{requ:comparison_of_isms_fusion}
	To which extent, as measured by the normed confusion matrix (see Section \ref{subsec:confusion_matrix}), does the capability of the proposed deep \gls{ism}s to estimate the position of dynamic, free and occupied areas given camera and lidar data respectively change, when radar information is added? 
\end{requ}
Regarding the encoding of radar detections for deep \gls{ism}s, the literature proposes to either encode the positions of a single timestep or use the temporally accumulated point cloud respectively projected into \gls{bev}. However, to the best of the author's knowledge, no investigation on how to encode the motion information of the detections has been conducted. Therefore, a rough investigation distinguishing three approaches shall be conducted which is formulated in the following research question\\
\begin{requ}[RQ\ref{requ:radar_dyn_encoding}] \label{requ:radar_dyn_encoding}
	To which extent, as measured by the normed confusion matrix (see Section \ref{subsec:confusion_matrix}), does the capability of the proposed deep \gls{ism}s to estimate the position of dynamic, free and occupied areas change choosing the input to be \gls{bev} projected radar detections...
	\begin{itemize}[noitemsep,nolistsep]
		\item ...of a single timestep encoding the dynamic detections with half intensity?
		\item ...accumulated over a certain time horizon only encoding the dynamic detections with half intensity of the latest timestep?
		\item ...accumulated over a certain time horizon linearly reducing the intensity of dynamic detections over time starting at half intensity?
	\end{itemize} 
\end{requ}
Looking at the uncertainty estimation, neither the classification nor the regression approaches for deep evidential \gls{ism}s do explicitly handle occurring aleatoric uncertainties, letting us arrive at the following hypothesis.
\\
\begin{hyp}[H\ref{hyp:sota_not_model_unc}] \label{hyp:sota_not_model_unc}
	In case of occurring aleatoric uncertainty, the current state of the art deep \gls{ism}s distribute the mass evenly into the free and occupied class rather than shifting it to the unknown class. 
\end{hyp}
In case H\ref{hyp:sota_not_model_unc} holds, these models would, thus, lack the possibility to distinguish between dynamic objects (per definition of evidential occupancy mapping indicated by mass equally distributed into free and occupied class) and regions of high uncertainty. Additionally, the unknown class cannot be used as a measure of information content, since some of the uncertainty is distributed into the free and occupied classes. This behavior would violate the requirements R\ref{subreq:unknown_mass}, hence, raising the following research questions. 
\\
\begin{requ}[RQ\ref{requ:how_to_sep_uncertainty}] \label{requ:how_to_sep_uncertainty}
	How can a deep, evidential \gls{ism} be defined to separate conflicting mass due to aleatoric uncertainty into the unknown class while leaving conflicting mass due to dynamic objects untouched. 
\end{requ}
%======================================%
%
%======================================%
\subsection{Research Needs for Usage of deep, evidential ISMs as Priors in Occupancy Mapping}
\label{subsec:research_needs_for_usage_of_deep_ims_as_priors_in_occmapping}
With regards to occupancy mapping with deep \gls{ism}s, not much literature is available. To the best of the authors knowledge, the only instances of occupancy mapping with deep \gls{ism}s use the standard Bayes filtering approach which does not cover evidential models. Thus, the first step consists in analyzing the characteristics and identifying short comings when applying the proposed evidential, deep \gls{ism}s for occupancy mapping.
\\\\
First, in contrast to geo \gls{ism}s which only provide estimates in regions directly affected by data, deep \gls{ism}s additionally perform interpolations in intermediate regions and even extrapolations in regions further away from data. To illustrate the issue arising from this behavior, consider the example depicted in Fig. \ref{fig:info_dependence}. Here, a scenario is shown in which the ego vehicle only partially observes a wall to its left-hand side for the first two time steps. Based on the majority of observations captured in the training dataset, the network might tend to extrapolate the wall as rectangular. In the standard occupancy formulation, this information is treated as independent and, thus, accumulated. When the vehicle finally obtains measurements of the wall's contour in the former occluded area, the extrapolation might have already be accumulated to high certainty. Therefore, many estimates based on measurements of this area would have to be accumulated to correct the assigned training data bias. In a similar way, this effect can also lead to overwriting area with predictions close to data with later occurring extrapolations. This thought experiment leads to the following hypothesis.
\begin{center}
	\import{imgs/06_research_approach}{info_dependence.pdf_tex}
	\captionof{figure}{\label{fig:info_dependence}Illustration of informational dependence between deep \gls{ism} predictions over time on the example of dataset bias. Here, the ego vehicle (black) drives along a wall and obtains radar measurements (orange) over three time steps. In each time step, the contour of the wall is estimated (blue).}
\end{center}
\begin{hyp}[H\ref{hyp:temporal_dependence}] \label{hyp:temporal_dependence}
	Due to inter- and extrapolation in areas not directly measured, deep \gls{ism}s contain informational dependence between time steps. This leads to accumulation of bias and or falsification of previously correct assigned areas when their estimates are fused into the occupancy maps using a combination rule that assumes informational independence.  
\end{hyp} 
In case H\ref{hyp:temporal_dependence} holds, the literature in Section \ref{subsec:combination_of_dependent_evidence} suggests to either remove the redundancy before combination or adapt the combination rule itself to account for the redundancy. This work will focus on removing the redundancy beforehand using Eq. \ref{eq:discount_op} since the Yager and Dempster combination rule, as defined in Section \ref{subsec:combination_of_independent_evidence}, provide well studied, often used fusion methods for evidential occupancy mapping. To remove the redundancy, half of the approaches in the literature focus on defining a constant redundancy weighting between sensor modalities. This is, however, not applicable for the setup in this work, since there is only one sensor modality and the dependency is on the environment.
\\\\
The other half proposes approaches to measure the amount of information in each to be fused mass and compare them using the mutual information, as stated in Eq. \ref{eq:mutual_info_with_entrop}. However, no general procedure is proposed to measure the mutual information in signals. Thus, the following questions emerges
\begin{requ}[RQ\ref{requ:how_to_meas_redund}] \label{requ:how_to_meas_redund}
	How can the mutual information be measured to asses the informational redundancy in evidential occupancy mapping?
\end{requ}
\begin{requ}[RQ\ref{requ:how_to_define_discount_fact}] \label{requ:how_to_define_discount_fact}
	How can a discount factor be defined based on the mutual information to remove the amount of redundant information between two evidential occupancy masses?
\end{requ}
Additionally, the problem arises that the evidential representation for occupancy mapping slightly differs from the standard in that it defines the conflict state as a meaningful transition state and not as another source of uncertainty based on contradictory information sources. Therefore, the regularly used entropy and the corresponding discount factors cannot be utilized in the proposed form. Also, the second commonly used measure, namely the specificity, quantifies how much mass is distributed into single element classes like the free and occupied class in contrast to the unknown which is both free and occupied. However, the specificity for evidential occupancy mapping is in the interval $[0.5,1.0]$ which can easily be seen by examining Eq. \ref{eq:specificity}. Thus, even for a total lack of information indicated by $\vec{m} = [0,0,1]^\top$, the specificity equals $0.5$ disqualifying it as a direct measure for information, too. This leads to the following research questions 
\begin{requ}[RQ\ref{requ:how_to_meas_info}] \label{requ:how_to_meas_info}
	How can the information content in evidential occupancy mapping be measured?
\end{requ}
Eventually, to utilize the deep \gls{ism} estimates as priors according to R\ref{subreq:initialize_with_deep_ism} and \ref{subreq:converge_to_geo_ism}, a procedure has to be developed to answer the following questions
\begin{requ}[RQ\ref{requ:disable_deep_ism_influence}] \label{requ:disable_deep_ism_influence}
	How can the influence of the deep \gls{ism} be disabled in case a definable amount of data has been collected, as defined in R\ref{subreq:initialize_with_deep_ism} and \ref{subreq:converge_to_geo_ism}?
\end{requ}
Finally, since the evidential occupancy mapping using deep \gls{ism}s has not been discussed in the literature so far, the resulting evidential maps given different sensor modalities shall be analyzed and compared. This can be formulated as follows
\\
\begin{requ}[RQ\ref{requ:comp_deep_occ_maps}] \label{requ:comp_deep_occ_maps}
	To which extent, as measured by the normed confusion matrix (see Section \ref{subsec:confusion_matrix}), does the capability of evidential occupancy maps to capture the ground-truth change using the proposed deep \gls{ism} with different sensor modalities?
\end{requ}
%======================================%
%
%======================================%
\subsection{Choice of Dataset}
\label{subsec:choice_of_dataset}
In order to answer the above posed research questions, a dataset containing big amounts of urban recordings of lidar with semantic information, camera, odometry and radar is required. Additionally, it would be nice to use a public dataset, for it allows comparability and reproducability. At the start of the thesis' implementation phase, the only dataset sufficing these requirements is the publicly available NuScenes dataset \cite{caesar2020nuscenes}, which can be seen in the dataset comparison provided in Table 1 in the paper.
\\\\
The dataset is separated into $1000$ so called scenes each containing the data of roughly a $20$ second drive during which data from each modality is recorded, which is referred to as sensor sweeps. Additionally, so called samples are defined every $0.5$ seconds containing annotations like bounding boxes and semantics for all sensor modalities. To enable comparability, the train-val-test split is predefined by the NuScenes creators.
\\\\
Regarding the sensor setup, six cameras three of which face front left, center, right and three of which face back left, center, right are installed. Each camera captures 1.4MP images at a rate of 12 Hz. Additionally, five 77 GHz \gls{fmcw} radars are mounted to the car's front left, center, right and back left and right. To obtain ground-truth depth information, a 32 beam spinning lidar is mounted to the roofs center that captures frames with 20 Hz. The pose information is based on a fusion of lidar odometry, \gls{gps} and \gls{imu}.  For more details, the reader is referred to Table 2 in the paper.
%==========================================================================%
%
%==========================================================================%
\section{Overview of Methodology}
\label{sec:framework_overview}
In this section, a framework is presented to address the research questions defined in Section \ref{subsec:research_needs_for_usage_of_deep_ims_as_priors_in_occmapping}. The framework extends the standard evidential occupancy mapping pipeline \cite{pagac1996evidential} to incorporate estimates of a data-driven \gls{ism}, as shown in Fig. \ref{fig:illustrative_system_overview}. The incorporation of the deep \gls{ism}s information is realized by fusing the estimates directly into the map rather than first fusing it with the geo \gls{ism}'s estimate. This is done, in order to enable an asynchronous fusion of information into the map, which allows for differing execution times of the \gls{ism}s. In order to tackle the problem of temporal redundancy, as mentioned in H\ref{hyp:temporal_dependence}, a specific fusion approach is defined for the deep \gls{ism}. This general procedure is detailed in the subsequent subsections as follows.
\begin{center}
	\import{imgs/06_research_approach/}{illustrative_system_overview.pdf_tex}
	\captionof{figure}{\label{fig:illustrative_system_overview}Structural overview of the proposed framework, showing how the \gls{bev} input is transformed both by the deep and geo \gls{ism} into occupancy estimates. These estimates are then fused into the occupancy map using a specialized method to fuse the deep \gls{ism} that accounts for the temporal redundancy.}
\end{center}
First, the creation of ground-truth lidar and baseline radar occupancy maps is discussed. Here, a method is shown in Section \ref{subsec:method_dyn_info_for_lidar} to interpolate the annotations of dynamic objects for the higher frequency intermediate lidar detections. This is followed by the definition of the geo \gls{ilm} and \gls{irm} used in this work in Section \ref{subsec:method_geo_ilm} and \ref{subsec:method_geo_irm}. Eventually, a method is described in \ref{subsec:method_choice_of_gt} to maximize the overlap between the ground-truth lidar and baseline radar occupancy maps.
\\\\
The definition of baseline and ground-truth data is followed by defining the deep \gls{ism}. Starting with the description of the different inputs and the target inspected in this work in Section \ref{subsec:def_of_targets_n_inputs}, followed by the architecture search in Section \ref{subsec:method_deep_ism_architecture}, the investigated methods to account for aleatoric uncertainty in Section \ref{subsec:method_al_uncert_in_deep_isms} and concluded by a description of the used metric in Section \ref{subsec:confusion_matrix}. The specific choice of the fusion methods for both geo and deep \gls{ism}s are detailed in Section \ref{subsec:method_to_use_deep_isms_in_occmaps} and \ref{subsec:method_to_use_deep_isms_as_priors_in_occmaps}.
%======================================%
%
%======================================%
\subsection{Method to provide dynamic Information for Lidar Sweeps}
\label{subsec:method_dyn_info_for_lidar}
To obtain the dynamic information for lidar sweeps, the sample's bounding boxes of dynamic objects are interpolated for intermediate sweeps and all lidar detections intersecting with the boxes are marked as dynamic. Here, the interpolations are obtained as follows. First, corresponding bounding boxes of dynamic objects are identified by their track ids for two subsequent samples. To avoid singularities, the bounding box poses of each identified pair are first transformed into the temporally first bounding box's coordinate frame. Next, a third degree polynomial is used to interpolate the poses. The polynomial is defined to intersects with the positions of the provided poses. Also, the polynomial's derivative at the interpolation points has to equal the tangents of the pose's angle. Thus intermediate poses should have  positions coinciding with the polynomial with orientations given by the arc-tangent of the polynomial's derivative.
\\\\
To finally obtain the interpolated bounding box pose, the way along the interpolated trajectory is numerically integrated and divided into equidistant segments. Under the assumption that the tracked, dynamic object travels with a constant velocity between the two samples, the interpolated pose is given at the point when the relative traveled distance between the first and second sample's pose is closest to the relative time of the sweep between the two samples. The so identified pose can then be transformed into the coordinate frame of the currently interpolated sweep. The interpolation procedure and the overlay of the resulting interpolated bounding boxes over a lidar sweep's detection image are illustrated in Fig. \ref{fig:dyn_objs_interpolation}.
\begin{center}
	\import{imgs/07_deep_ism_exp/dataset}{dyn_objs_interpolation.pdf_tex}
	\captionof{figure}{\label{fig:dyn_objs_interpolation}Illustration of the three steps to obtain interpolations of the 2d bounding box poses on the left hand side together with an example of resulting interpolated bounding boxes (yellow) overlayed on a lidar sweep's \gls{bev} detection image on the right. The three interpolation stages from left to right show the original bounding box poses of two consecutive samples, the poses transformed into the first poses coordinates together with the interpolated poses (gray) and finally the poses back in the original coordinate frame with the interpolations.}
\end{center}
%======================================%
%
%======================================%
\subsection{Definition of the Geo ILM}
\label{subsec:method_geo_ilm}
Both lidar and radar sensors use a radial measurement principal which is why it makes sense to model them using the ray casting \gls{ism} as explained in Section \ref{subsec:ray_casting}, however in a modified way. In case of the \gls{ilm}, first the ground detections have to be removed. To do so, either a height-threshold or a semantic segmentation based approach can be chosen. The specific choice used in this work is detailed in Section \ref{sec:choice_of_gt}.
\\\\
Afterwards, free space rays with opening angle $\varphi_\sphericalangle$ are cast for each angle in increments of $\varphi_\sphericalangle$ in the range $[0^\circ, 360^\circ]$ around the sensor center either up to a maximum distance $r_\text{max}$ or until intersecting with a detection. All \gls{bev} grid cells affected by the free space ray are set to $[M_F,0,1-M_F]^\top$ using the tunable free space weight $M_F \in [0,1]$. On the other hand, position of collided detections are either set to $[0,M_O,1-M_O]^\top$ for static detections with the occupied space weight $M_O \in [0,1]$ or to $[M_D, M_D, 1-2M_D]^\top$ for dynamic detections with the dynamic object weight $M_D \in [0,0.5]$. Here, the information about the motion state is not provided by the lidar itself but by external sources like additional labels in the dataset. The additional motion information will only be used to generate labels to train the deep \gls{ism} variants but not to generate deep \gls{ism} inputs! This procedure has the effect that detections occluded in the \gls{bev} projection are filtered out which better matches the radar's sensing capability. An algorithmic summary of the geo \gls{ilm} in pseudo code is shown in Alg. \ref{alg:geo_ilm}.
\\\\
\begin{algorithm}[H]
	\caption{\label{alg:geo_ilm}Geo ILM}
	\textbf{Hyperparameters:} $M_F, M_O, M_D, \varphi_\sphericalangle, N$\;
	\Function{castFreeSpace}{
		\Comment{see \eqref{eq:idm_radar} with $M_O = M_D = 0$}
	}
	remove ground-plane detections\;
	initialize all cells in the \gls{ilm} image to $m_u=1$\;
	\For{$\varphi_{\text{center}}$ \text{in} discretizedAngles360}{
		\If{(at least one detection in current cone area)}{
			$(r_{\text{det}}, \varphi_{\text{det}})$ = \FuncCall{getClosestDetectionInsideConeArea}{$\varphi_{\text{center}}$, $\varphi_\sphericalangle$, $r_{\text{max}}$}\;
			\FuncCall{castFreeSpaceRay}{$\varphi_{\text{center}}$, $r_{\text{det}}$, $\varphi_\sphericalangle$, $M_F$}\;
			\Comment{mark closest detection}
			\If{(detection is static)}{
				\FuncCall{assignCellValue}{$(r_{\text{det}}, \varphi_{\text{det}})$, $[0,M_O,1-M_O]$}\;
			}
			\Else{
				\FuncCall{assignCellValue}{$(r_{\text{det}}, \varphi_{\text{det}})$, $[M_D, M_D, 1-2M_D]$}\;
			}
		}
		\Else{
			\FuncCall{castFreeSpaceRay}{$\varphi_{\text{center}}$, $r_{\text{max}}$, $\varphi_\sphericalangle$, $M_F$}\;			
		}
	}
\end{algorithm}
%======================================%
%
%======================================%
\subsection{Definition of the Geo IRM}
\label{subsec:method_geo_irm}
For the geo \gls{irm}, a cone model which is based on the \gls{idm}$_{\text{B-Spline}}$ as proposed in \cite{loop2016closed} is used with the following adjustments. First, since the whole uncertainty ellipse of a radar detection falls within one of the 31,25 cm $\times$ 31,25 cm grid cells in the \gls{bev} grid, a scaled, evidential version of the \gls{idm}$_{\text{ideal}}$ can be used as an approximation of the \gls{idm}$_{\text{B-Spline}}$'s radial component.
\\\\
In angular direction, it is proposed to linearly approximate the influence of the Gaussian noise, again due to the coarse discretization. However, cones with enlarged opening angles are used to counteract cases of \gls{idm} rays passing between detections in sparsely measured areas. While the enlarged cones lead to a desired enrichment of free space they would also lead to blurring the occupied space and with it, object boundaries. Thus, only the free space follows the linearly approximated Gaussian noise in angular direction while the detection's influence is, again, restricted to its occurring cell. Thus, the following adapted evidential \gls{idm} is proposed to be used for radars
\begin{align}
	\label{eq:idm_radar}
	\text{\gls{idm}}_{radar}(r, \varphi, \mu_r, \mu_\varphi, d) &= 
	\begin{cases}
		[M_{F\sphericalangle},0,1-M_{F\sphericalangle}]^\top &, \Delta\varphi \leq \dfrac{\varphi_\sphericalangle}{2}, r < \mu_r\\
		[0,M_O,1-M_O]^\top &, \varphi = \mu_{\varphi}, r = \mu_r, d = \text{False}\\
		[M_D,M_D,1-2M_D]^\top &, \varphi = \mu_{\varphi}, r = \mu_r, d = \text{True}\\		
		[0,0,1]^\top &, \text{ else}\\
	\end{cases}\\
	%	
	\text{with } \quad M_{F\sphericalangle} &= M_F(1 - \dfrac{\Delta\varphi}{\varphi_\sphericalangle}) \quad\text{and}\quad \Delta\varphi = |\varphi - \mu_\varphi|\\
\end{align}
With $r$ and $\varphi$ being the cylindrical coordinates measured in the radar's sensor coordinate frame, $\mu_r$ and $\mu_{\varphi}$ being the cylindrical  coordinates of the detection and $d$ the Boolean flag indicating the dynamic state of the detection. However, for reasons detailed in Section \ref{subsec:geo_ism_radar}, \gls{idm}$_{radar}$ can not be directly applied on each detection to obtain the \gls{irm}. Here, the main problem is the assumption that only objects in line of sight are detected. This assumption is violated for the radar since objects can be detected in occluded areas due to multi-path reflections. This leads to big amounts of free space being assigned to occupied areas by \gls{idm} rays cast towards detections in occluded areas. 
\\\\
In this work, the occlusion problem is addressed similar to \cite{werber2015automotive} with the alteration that the free space part of the \gls{idm} is not only ignored in occupied regions of other rays, but ends at it. In case the contours of all objects are densely detected, this would suffice to solve the problem. However, radar detections are also sparse which is why measurements of preceding time steps are additionally used to densify object boundaries. In contrast to the \gls{ilm}, detections occluded in the \gls{bev} projection shall still be marked as occupied in the \gls{irm} in order not to further increase the sparseness. The time horizon, in which detections are accumulated, shall be set to a finite value to dampen the influence of potential outliers. This procedure only introduces the time horizon as hyperparameter and, thus, proposes a good candidate to answer RQ\ref{requ:fr_space_enrichment}.
\begin{center}
	\import{imgs/06_research_approach/method/}{idm_radar.pdf_tex}
	\captionof{figure}{\label{idm_radar}Illustration of the proposed \gls{idm}'s contours being applied on two detections. It shows the big and thing \gls{idm}s being cast for each detection. Also, it shows that the big \gls{idm} cone's range for detection two (stripped blue) is limited by the first detection. On the other hand, the thin \gls{idm} of the second detection is, for the given scenario, able to assign free space up to the second detection.}
\end{center}
Another problem connected with the sparsity is the low coverage of free space. In this work, the method proposed in \cite{prophet2018adaptions} is altered to find an answer for RQ\ref{requ:fr_space_enrichment} by instead casting the original rays again but with a much wider opening angle. This assumes that the manufacturer's prefiltering of the raw radar signal is done in away to preserve detections belonging to the object closest to the sensor. The justification for this assumption is based on the fact that radars are applied in safety critical collision avoidance systems. The second iteration of wider \gls{idm} rays shall only be used to enrich the free space and, thus, use $M_O = 0$. Additionally, it shall be fused with the first iterations \gls{irm} using Dempster's combination rule. An algorithmic summary of the geo \gls{irm} in pseudo code is shown in Alg. \ref{alg:geo_irm}.
\\\\
\begin{algorithm}[H]
	\caption{\label{alg:geo_irm}Geo IRM}
	\textbf{Hyperparameters:} $M_F^{(\text{\scriptsize thin})}, M_F^{(\text{\scriptsize big})}, \varphi_\sphericalangle^{(\text{\scriptsize thin})}, \varphi_\sphericalangle^{(\text{\scriptsize big})}, M_O, M_D, N$\;	
	\Function{castFreeSpace}{
		\Comment{see \eqref{eq:idm_radar} with $M_O = M_D = 0$}
	}
	initialize all cells in the \gls{irm} image to $m_u=1$\;
	transform previous $N$ radar point clouds of all radars to current \gls{irm} image\;
	\For{$(\mu_r, \mu_\varphi)$ \text{in} currentDetections}{
		\Comment{range at which the free space of the ray is stopped}
		$(r_{\text{det}}, \varphi_{\text{det}})$ = \FuncCall{getClosestDetectionInsideConeArea}{$\varphi_{\text{center}}$, $\varphi_\sphericalangle$, $r_{\text{max}}$}\;	
		\Comment{cast thin and big free space rays}
		\FuncCall{castFreeSpaceRay}{$\varphi_{\text{center}}$, $r_{\text{det}}$, $\varphi_\sphericalangle^{(\text{\scriptsize thin})}$, $M_F^{(\text{\scriptsize thin})}$}\;		
		\FuncCall{DempsterRule}{\FuncCall{castFreeSpaceRay}{$\varphi_{\text{center}}$, $r_{\text{det}}$, $\varphi_\sphericalangle^{(\text{\scriptsize big})}$, $M_F^{(\text{\scriptsize big})}$}}\;
		\If{(detection is static)}{
			\FuncCall{assignCellValue}{$(\mu_r, \mu_\varphi)$, $[0,M_O,1-M_O]$}\;
		}
		\Else{
			\FuncCall{assignCellValue}{$(\mu_r, \mu_\varphi)$, $[M_D, M_D, 1-2M_D]$}\;
		}
	}
\end{algorithm}
%======================================%
%
%======================================%
\subsection{Methodology to define the Geo ISM's Hyperparameters and Removal of Ground-Plane Detections}
\label{subsec:method_choice_of_gt}
The analysis of which method to chose as a baseline and which as ground truth, according to RQ\ref{requ:what_is_best_gt} and RQ\ref{requ:fr_space_enrichment}, is a chicken and egg problem since one has to be kept fixed to analyze the other. Hence, it is proposed to initially manually define and fix the geo \gls{ilm} using the threshold-based ground-plane removal as normally applied in the literature. This preliminary geo \gls{ilm} will act as reference to analyze the free space enrichment (see RQ\ref{requ:fr_space_enrichment}) of the geo \gls{irm}. 
\\\\
To do so, three variants of the geo \gls{irm} (as defined in Alg.\ref{alg:geo_irm}), are compared which subsequently add more of the proposed algorithm's components. First, the \gls{idm}$_\text{radar}$ shall only be applied without accumulation of radar detections over time and only as a thin cone. This corresponds to fixing the parameters in Alg. \ref{alg:geo_irm} as shown for the first variant in Fig. \ref{tab:geo_irm_variants}. Next, the additional influence of using accumulated detections is analyzed (see Fig. \ref{tab:geo_irm_variants} \#2), followed by the additional application of cones with big opening angles (see Fig. \ref{tab:geo_irm_variants} \#3). These variants are used to create occupancy maps which are compared with maps created from the fixed geo \gls{ilm}. Here, the maps are created using Yager's rule of combination and the metric for comparison is the m\gls{iou}, evaluated within the area reached by the \gls{ism}s. To obtain best competitive results for each method, the parameters labeled as "?" for each respective variant in Fig. \ref{tab:geo_irm_variants} are chosen via a grid search.
\begin{center}
	\begin{tabular}{cl|c|c|c|c|c|c|c}
		& geo \gls{irm} Variant & $M_F^{(\text{thin})}$ & $M_F^{(\text{big})}$ & $\varphi_\sphericalangle^{(\text{\scriptsize thin})}$ & $\varphi_\sphericalangle^{(\text{\scriptsize big})}$ & $M_O$ & $M_D$ & $N$ \\
		\hline
		\#1 &Ray Casting \gls{irm}& ? & \textcolor{myred}{0} & ? & \textcolor{myred}{0} & ? & ? & \textcolor{myred}{1} \\
		\#2 &... + Accumulation & ? & \textcolor{myred}{0} & ? & \textcolor{myred}{0} & ? & ? & ? \\
		\#3 &... + big Cones & ? & ? & ? & ? & ? & ? & ?
	\end{tabular}
	\captionof{table}{\label{tab:geo_irm_variants}Three investigated variants of the geo \gls{irm} that subsequently add more features of Alg. \ref{alg:geo_irm}. The respective parameters marked as "?" are chosen through a grid search. This search is based on m\gls{iou} comparison between occupancy maps created with the respective variants and the fixed geo \gls{ilm} maps.}
\end{center} 
After the evaluation of the geo \gls{irm}, the best performing variant is fixed and used as reference to analyze the effect of different ground-plane removal methods in the creation of lidar-based occupancy maps. Here, a dataset with semantic labels for the lidar detections shall be used to cover the semantic ground-plane filtering approaches. As an alternative, threshold-based filtering shall be considered by removing all lidar detections beneath a certain height threshold. The sensor specifics, considered semantic labels and specification of height thresholds are detailed in Section \ref{sec:choice_of_gt}. 
%======================================%
%
%======================================%
\subsection{Deep ISM Targets and investigated Inputs}
\label{subsec:def_of_targets_n_inputs}
To analyze the effects and performance of deep \gls{ism}s given different sensor inputs, lidar, camera and radar measurements have to be encoded into an input representation. For this work, a common representation for all sensor modalities and the training targets is chosen to be \gls{bev} images as further detailed in R\ref{subreq:grid_map_size}. In the following, the steps are detailed to create the lidar, camera and radar \gls{bev} image inputs together with the corresponding deep \gls{ism} labels. 
\\\\
Starting with the lidar \gls{bev} detection images, first, the ground-plane has to be removed. The specific removal method and its parameters are experimentally defined in Section \ref{sec:choice_of_gt}. Afterwards, the detection positions are discretized into the coordinates of a gray-scale image and marked with value $1.0$ as opposed to the default value of $0.0$. The so processed lidar \gls{bev} image will be referred to as \gls{l} and their \gls{ism}s as \gls{ilm}. An example is shown in Fig. \ref{fig:input_viz}.
\\\\
For the radar images, the sweep information of all corner and the front radar are used. All of the static detections, distinguishable through a flag provided in the NuScenes dataset, are marked as $1.0$ opposed to the default pixel value $0.0$. To analyze RQ\ref{requ:radar_dyn_encoding}, three encoding variants will be investigated. First, only the detection information of a single timestep shall be encoded marking the dynamic detections with half intensity ($0.5$). This encoding is referred to as \gls{r}. Alternatively, radar detections shall be accumulated over $N$ timesteps, marking the dynamic detections as $0.5(1-\dfrac{t}{N})$. This indicates the transition state of dynamic objects between free and occupied by marking the most recent detections as $0.5$ and linearly decreasing their influence over time. The time horizon $N$ is chosen by performing a grid search as shown in Section \ref{sec:choice_of_gt}. This encoding is defined as \gls{r_n}. Finally, the third considered encoding again accumulates the static detections over $N$ timesteps. However, only the most recent dynamic detections are marked with $0.5$, which is referred to as \gls{r_n_1}. A comparison between \gls{r}, \gls{r_n} and \gls{r_n_1} is presented in Section \ref{sec:analysis_dyn_encoding} and exemplary illustrations can be found in Fig. \ref{fig:input_viz}. The corresponding \gls{ism}s are referred to as \gls{irm} with the "R" being appended by the respective indices per variant.
\\\\
In case of the camera \gls{bev} images, the homography projection as well as the \gls{monodepth} projection are considered. To obtain the homography-based \gls{bev} images, lidar points are identified which are only $5$cm away from the ideal flat ground-plane. These lidar detections are then transformed both to the \gls{bev} and the camera image. Afterwards, the corresponding pixel coordinates in both representations are identified and used to compute the homography matrix using a RANSAC-based filter. After identifying the homography for each camera, the images are projected into the \gls{bev} image where overlapping areas are being replaced and areas with no detections remain black. This \gls{bev} projection is referred to as \gls{c_rgb}. Additionally, a variant named \gls{c_s} is defined where the semantic annotations are transformed using the homography transformation. The semantic labels are being obtained by applying DeepLab V3+ \cite{deeplabv3plus2018} using the Xceptin network \cite{chollet2017xception} as a backbone trained on the Cityscapes dataset \cite{cordts2016cityscapes} without further finetuning. The color coding is taken over from Cityscapes. Examples of \gls{c_rgb} and \gls{c_s} are shown in Fig. \ref{fig:input_viz}. The respective \gls{ism}s are referred to as \gls{ic_rgbm} and \gls{ic_sm}.
\\\\
For the \gls{monodepth} projection, the semi-supervised model as proposed in \cite{guizilini2020robust} is used. Here, a model pretrained on Cityscapes is finetuned in a semi-supervised way on the NuScenes data. The resulting point cloud of all cameras is then projected into the \gls{bev} image while the pixel intensities represent the scaled height information. More specifically, the height is clipped into the interval $[-0.5,1.0]$m and scaled to the intensity interval $[0,1]$. This encoding and \gls{ism} will be referred to as \gls{c_d} and \gls{ic_dm} and an example is shown in Fig. \ref{fig:input_viz}.
\\\\
To analyze the fusion of camera and lidar respectively with radar measurements, the afore described input representations will be concatenated channel-wise. The combined \gls{bev} image is further directly used as an input to perform an implicit fusion inside the deep \gls{ism}. For reasons explained in Section \ref{sec:cam_lidar_fusion_in_deep_isms}, only the fusion of R$_{20}$ with \gls{l} and \gls{c_d} respectively will be investigated in this work. The resulting deep \gls{ism}s are referred to as \gls{ilr_20m} and \gls{ic_dr_20m}. Examples of the combined input for \gls{l} \& R$_{20}$ and \gls{c_d} \& R$_{20}$ are visualized in Fig. \ref{fig:input_viz}.
\\\\
Finally, the occupancy map patches used as targets to train the deep \gls{ism}s are generated as follows. First, the geo \gls{ilm}, as defined in Section \ref{subsec:method_geo_ilm} and \ref{sec:choice_of_gt}, is used to create an occupancy map of the considered scene. Here, the mapping is conducted by using the interpolated dynamic information, as described in Section \ref{subsec:method_dyn_info_for_lidar}, to reduce the effect of artifacts due to dynamic objects. After the creation of the occupancy map, patches, centered around each ego vehicle's position during mapping, are cut from the map. As a last step, to regain the information of dynamic objects filtered out during mapping, the interpolated bounding boxes are marked in the occupancy map patches. An alternative to assigning the whole bounding box area as dynamic would be to only mark pixels corresponding to dynamic detections. This, however, provides only partial contours of the moving objects and is therefore disregarded. An example of this projection, which will be referred to as \gls{gt}, is shown in Fig. \ref{fig:input_viz}.
\begin{center}
	\import{imgs/06_research_approach/}{input_viz.pdf_tex}
	\captionof{figure}{\label{fig:input_viz}Illustration of the inputs and target images used to train the different deep \gls{ism} variants.}
\end{center}
%======================================%
%
%======================================%
\subsection{Methodology to define the deep ISM Architecture}
\label{subsec:method_deep_ism_architecture}
Next, the network architecture has to be analyzed to suffice RQ\ref{requ:network_search}. To restrict the search space to a feasible subset, the architecture as illustrated in Fig. \ref{fig:std_unet} is considered. Here, the initial grid dimension is halved after each of the four encoder steps using strided $3\times 3$ convolutions while the number of channels is doubled up to a maximum. This is inverted in the decoder using bilinear upsampling. All convolution layers are structured as suggested in the literature (see Section \ref{subsec:architecture} and Fig. \ref{fig:conv_n_resnet_layer}). Whenever the grid's height and width remain constant, ResNet layers, as shown in Fig. \ref{fig:conv_n_resnet_layer}, are deployed to increase efficiency. The last three layers consist of two additional convolution layers without Dropout and the output layer, specifically designed for the applied loss. A detailed description of the hyperparameters and the choices made for the architecture search are elaborated in Section \ref{sec:choice_of_unet_arch}. The layer depth of the network is chosen in a way that a receptive field of 125 is reached which almost covers the whole image (for receptive field computation please refer to \eqref{eq:receptive_field}). This receptive field is chosen to enable the network to learn the principle of occluded areas based on the ego vehicle positioned in the center. It shall be mentioned that a receptive field covering half the input image size might already suffice for the task at hand. However, since the computation cost shrinks quadratically when downsampling the feature size, adding the additional layers to arrive at the full input image's receptive field is almost negligible. 
\begin{center}
	\begin{minipage}{0.5\textwidth}
		\centering
		\begin{tabular}{c|c|c}
			\multicolumn{3}{c}{}             \\			
			\multicolumn{3}{c}{}             \\				
			layers & abbr. & parameters \\
			\hline
			\textcolor[rgb]{0.2,0.4,0.8}{ResNet} & res$(\cdot)$ & $K=3, C_{\text{out}}, D$ \\
			\textcolor[rgb]{0.18,0.76,0.49}{Convolution} & conv$_s(\cdot)$ & $K=3, C_{\text{out}}, s$\\
			\textcolor[rgb]{0.18,0.76,0.49}{conv without Dropout} & \underline{conv}$_s(\cdot)$ & $K=3, C_{\text{out}}, s$\\		
			\textcolor[rgb]{0.57,0.25,0.67}{bilinear upsampling} & up$_u(\cdot)$ & $K=3 ,C_{\text{out}}, u$\\
			Concatenation & cat$(\cdot ,\cdot)$ & ---\\
			Input / \textcolor[rgb]{1.0,0.4,0.0}{Output} & $x$ / y$(\cdot)$& --- / ---\\
			\multicolumn{3}{c}{}             \\					
			\multicolumn{3}{c}{\textbf{Layers}}             \\
		\end{tabular}
	\end{minipage}
	\hfill
	\begin{minipage}{0.47\textwidth}
		\begin{center}
			\import{imgs/06_research_approach/}{std_unet.pdf_tex}
		\end{center}
	\end{minipage}\\
	%
	%
	%
	\begin{minipage}{0.4\textwidth}
		\centering
		\begin{tabular}{l|c|l}
			layers & abbr. & dimension\\
			\hline
			$x$ & $e_{00}$ & $128 \times 128 \times C_\text{in}$\\	
			res$_D(e_{00})$ & $e_{01}$ & $128 \times 128 \times C_0$\\
			&&                                                       \\
			&&                                                       \\
			\underline{conv}$_1(e_{01})$ & $s_0$ & $128 \times 128 \times 4$ \\
			&&                                                   \\
			\hline
			conv$_2(e_{01})$ & $e_{10}$ & $64 \times 64 \times C_1$\\
			res$_D(e_{10})$ & $e_{11}$ & $64 \times 64 \times C_1$ \\
			\underline{conv}$_1(e_{11})$ & $s_1$ & $64 \times 64 \times 4$ \\
			&&                                                 \\
			\hline
			conv$_2(e_{11})$ & $e_{20}$ & $32 \times 32 \times C_2$\\
			res$_D(e_{20})$ & $e_{21}$ & $32 \times 32 \times C_2$ \\
			\underline{conv}$_1(e_{21})$ & $s_2$ & $32 \times 32 \times 4$ \\
			&&                                                 \\
			\hline
			conv$_2(e_{21})$ & $e_{30}$ & $16 \times 16 \times C_3$\\
			res$_D(e_{30})$ & $e_{31}$ & $16 \times 16 \times C_3$ \\
			\underline{conv}$_1(e_{31})$ & $s_3$ & $16 \times 16 \times 4$ \\
			&&                                                 \\
			\hline
			conv$_2(e_{31})$ & $e_{40}$ & $8 \times 8 \times C_4$\\
			res$_D(e_{40})$ & $e_{41}$ & $8 \times 8 \times C_4$ \\
			\multicolumn{3}{c}{}             \\
			\multicolumn{3}{c}{\textbf{Encoder Architecture}}             \\
		\end{tabular}
	\end{minipage}
	\hfill
	\begin{minipage}{0.02\textwidth}
		\centering
		\begin{tabular}{c}
			\\		
			\\		
			\\	
			\\	
			\\
			$\longrightarrow$\\	
			\\	
			\\	
			\\	
			$\longrightarrow$\\	
			\\	
			\\	
			\\	
			$\longrightarrow$\\	
			\\	
			\\	
			\\	
			$\longrightarrow$\\	
			\\	
			\\	
			$\longrightarrow$\\	
			\\	
			\\
		\end{tabular}
	\end{minipage}
	\hfill
	\begin{minipage}{0.55\textwidth}
		\centering
		\begin{tabular}{l|c|l}
			layers & abbr. & dimension\\
			\hline
			y$(d_{04})$ & $d_{05}$ & $128 \times 128 \times C_\text{out}$\\	
			\underline{conv}$_1(d_{03})$ & $d_{04}$ & $128 \times 128 \times 4$\\
			\underline{conv}$_1(d_{02})$ & $d_{03}$ & $128 \times 128 \times 4$\\
			res$_D(d_{01})$ & $d_{02}$ & $128 \times 128 \times C_0$\\
			cat$(d_{00},s_0)$ & $d_{01}$ & $128 \times 128 \times C_0$\\
			up$_2(d_{12})$ & $d_{00}$ & $128 \times 128 \times C_0$\\
			\hline
			&&\\
			res$_D(d_{11})$ & $d_{12}$ & $64 \times 64 \times C_1$\\
			cat$(d_{10},s_1)$ & $d_{11}$ & $64 \times 64 \times C_1$\\
			up$_2(d_{22})$ & $d_{10}$ & $64 \times 64 \times C_1$\\
			\hline
			&&\\
			res$_D(d_{21})$ & $d_{22}$ & $32 \times 32 \times C_2$\\
			cat$(d_{20},s_2)$ & $d_{21}$ & $32 \times 32 \times C_2$\\
			up$(d_{32})$ & $d_{20}$ & $32 \times 32 \times C_2$\\
			\hline
			&&\\
			res$_D(d_{31})$ & $d_{32}$ & $16 \times 16 \times C_3$\\
			cat$(d_{30},s_3)$ & $d_{31}$ & $16 \times 16 \times C_3$\\
			up$_2(d_{40})$ & $d_{30}$ & $16 \times 16 \times C_3$\\
			\hline
			&&\\
			res$_D(e_{41})$ & $d_{40}$ & $8 \times 8 \times C_4$\\
			\multicolumn{3}{c}{}             \\
			\multicolumn{3}{c}{\textbf{Decoder Architecture}} \\
		\end{tabular}
	\end{minipage}
\captionof{figure}{\label{fig:std_unet}Illustration of the UNet variant's architecture used in this work. The skip connections between encoder and decoder are realized with a convolution, compressing the amount of features to 4 channels and, afterwards, concatenating them with the features of a subsequent layer. Convolution and ResNet layers are structured as shown in Fig. \ref{fig:conv_n_resnet_layer}. The skip connection in the ResNet layer is realized by adding the input to the ResNet layers output before applying the non-linearity.}
\end{center}
%======================================%
%
%======================================%
\subsection{Methodology to account for aleatoric Uncertainties in deep ISMs}
\label{subsec:method_al_uncert_in_deep_isms}
As explained in Section \ref{subsec:research_needs_for_deep_isms}, the baseline deep \gls{ism} considered in this work shall model the evidential occupancy estimation using a Softmax output activation (see Eq. \ref{eq:softmax}) to transform the last layer's predictions $z$ of each of the $K$ classes into the normed network output $\tilde{y}$. The training loss between the prediction $\tilde{y}$ and the targets $\hat{y}$ shall be the standard \gls{mse} regression loss. For the first method, the predictions and targets directly equal the evidential masses ($\tilde{y} = \tilde{m}$ and $\hat{y} = \hat{m}$). This configuration is from here on referred to as \textbf{SoftNet} and is visualized in in the first row in Fig. \ref{fig:deep_ism_variants}. 
\begin{align}
	\label{eq:softmax}
	\tilde{y}(\vec{z})_i &= \dfrac{e^{z_i}}{\sum_{k=1}^{K}e^{z_k}}\\
	\label{eq:mse}
	\mathcal{L}_{\text{MSE}} &= \sum_{k=1}^{K}(\hat{y}_k - \tilde{y}_k)^2
\end{align}
In case of occurring aleatoric uncertainty, the network is confronted with both information indicating a pixel to be free and occupied, leading to H\ref{hyp:sota_not_model_unc}. To investigate H\ref{hyp:sota_not_model_unc} further, first, the architecture as specified in Section \ref{subsec:method_deep_ism_architecture} will be trained in the SoftNet configuration using the data as specified in Section \ref{subsec:def_of_targets_n_inputs}. Next, the normed confusion matrix, as detailed in Section \ref{subsec:confusion_matrix}, will be computed for the test data predictions. The normed confusion matrix is an expansion of the commonly used \gls{iou} and thus provides more detailed insights. Additionally, two alternatives to SoftNet shall be investigated to potentially answer RQ\ref{requ:how_to_sep_uncertainty}. 
\\\\
For the first variation, the evidential occupancy classes are extended to separately model the dynamic class instead of mixing it into the free and occupied classes(e.g. $[0.5, 0.5,0] \rightarrow [1,0,0,0]$). With regards to the network, the Softmax output as well as the \gls{mse} loss have to be extended by one dimension to realize this configuration. Additionally, to adapt the targets, an operation which shifts the occupancy labels from the evidential $\vec{m}=[b_f,b_o,u]^\top$ to the extended representation $\mathring{\vec{m}}=[\mathring{b_d},\mathring{b_f},\mathring{b_o},\mathring{u}]^\top$, sufficing \gls{r} \ref{subreq:ev_rep}, \ref{subreq:unknown_mass} and \ref{subreq:conflicting_mass}, can be defined as follows
\begin{align}
	\label{eq:extend_shift}
	\text{shift extension} \quad\mathring{\vec{m}} &\succ(\vec{m})\\
	\mathring{b}_d &= 2\cdot \min(b_f,b_o)\\
	\mathring{b}_{f/o} &= b_{f/o} - \min(b_f,b_o)\\
	\mathring{u} &= u
\end{align}
Here, $\min(b_f,b_o)$ describes the amount of mass being equal in both the free and occupied class. This portion is extracted from both classes, which doubles its amount, and shifted to the newly created dynamic class, leaving the unknown class untouched.\\ 
Moreover, to use estimates in the extended representation $\tilde{\mathring{\vec{m}}}$ in the evidential occupancy mapping pipeline, a compression operation can be defined as follows
\begin{align}
	\label{eq:compress_shift}
	\text{shift compression} \quad\vec{m} &\prec(\mathring{\vec{m}})\\
	b_{f/o} &= \mathring{b}_{f/o} - \min(\mathring{b}_f,\mathring{b}_o) + \dfrac{\mathring{b}_d}{2}\\
	u &= \mathring{u} + 2\cdot\min(\mathring{b}_f,\mathring{b}_o)
\end{align}
This operation quantifies the learned aleatoric uncertainty between the free and occupied class as the amount of mass equally distributed into free and occupied ($\min(\mathring{b}_f,\mathring{b}_o)$) following H\ref{hyp:sota_not_model_unc}. Afterwards, the equal portion is extracted from their respective classes and shifted to the unknown class. Also, the dynamic mass is split into equal portions and added to the free and occupied class respectively, to account for \gls{r} \ref{subreq:conflicting_mass}. The network trained on the extended labels $\hat{\mathring{\vec{m}}}$ and capable of producing evidential estimates $\tilde{\vec{m}}$ by applying the extend operation defined in Eq. \ref{eq:compress_shift} is from here on referred to as \textbf{ShiftNet}. The ShiftNet method is depicted in the second row of Fig. \ref{fig:deep_ism_variants}.
\\\\
The third method is heavily based on the method proposed by Sensoy et al. \cite{sensoy2018evidential}. In their method, a Dirichlet network is trained on the Bayes risk of the \gls{mse} (see Eq. \ref{eq:bayes_risk_mse}) to model aleatoric uncertainty and \gls{subj_logic} (see Eq. \ref{eq:e2m} and \ref{eq:e2p}) is used to transform the Dirichlet \gls{pdf} to evidential masses. This will be referred to as \textbf{DirNet} and is depicted in the last row of Fig. \ref{fig:deep_ism_variants}. However, in the original formulation of Sensoy et al., all unknown mass is solely due to aleatoric uncertainty. But, in the evidential occupancy formulation, the unknown mass both represents uncertainty and lack of information. Thus, to additionally provide labels for the unknown mass e.g. in unobserved areas, the loss from Eq. \ref{eq:bayes_risk_mse} shall be altered as follows
\begin{align}
	\mathcal{L}_i = (\hat{u}_i-\tilde{u}_{i})^2 + \sum_{k\in [f,o]}(b_{ik}-\tilde{p}_{ik})^2 + \dfrac{\tilde{p}_{ik}(1-\tilde{p}_{ik})}{S_i+1} 
\end{align}
\begin{center}
	\import{imgs/06_research_approach/method/}{deep_ism_variants.pdf_tex}
	\captionof{figure}{\label{fig:deep_ism_variants}Illustration of the three deep \gls{ism} configurations investigated in this work.}
\end{center}
To account for the class imbalance in all of the above mentioned deep \gls{ism} variants, the mean loss is computed separately for the labels of each class and afterwards summed up over all classes to obtain a final score. The performance of these variants is compared in Section \ref{sec:al_uncert_in_deep_isms} to analyze H\ref{hyp:sota_not_model_unc} and answer RQ\ref{requ:how_to_sep_uncertainty}.
%======================================%
%
%======================================%
\subsection{Metric to evaluate the ISM Variants}
\label{subsec:confusion_matrix}
To quantify the \gls{ism} and occupancy mapping results, a confusion matrix variant is deployed since it provides a more detailed means of analysis as compared to the mIoU score. The confusion matrix variant is constructed in the following steps  
\begin{enumerate}[noitemsep,nolistsep]
	\item obtain the current lidar occupancy map patch together with the current model's estimates
	\item define the ground-truth pixels by assigning each pixel to the class with the highest mass in the lidar occupancy map patch 
	\item for each ground-truth pixels belonging to a class, sum up how much weight has been estimated to belong to each class (e.g. for all ground-truth occupied pixels, how much mass is estimated to be free, how much occupied, ...)
	\item divide the sum of estimated mass for each class by the amount of ground-truth pixels for the current class 
	\item to cope with the amount of pixels per class over the whole dataset, the above is done on an image basis and averaged over all test samples
	\item[$\rightarrow$] this leads to the probability $p(\tilde{m}_k|\hat{m}_k)$ to estimate class $k$ for a pixel given the pixel belongs to the actual class $k$ with $k \in [d, f, o, u]$
\end{enumerate}
Since all of the investigated models behave severely different in the visible and occluded area, it is necessary to provide independent metrics for these two cases. Thus, the metrics are additionally computed for visible and occluded areas, while the occluded area is defined as the pixels where the geo \gls{ilm}'s unknown mass surpasses the other class' masses. Thus, only a stochastically insignificant portion of unknown labels remains in the visible area, which is why these scores are excluded from consideration.
\\\\
To help the interpretation of the confusion matrix, values are marked in green to show true positives while red shows the percentages of false predictions. The unknown class predictions, marked in black, are treated as the safe state and, as such, do not add to the false rates. Additionally, since the network is permitted to extrapolate the state in unknown areas, all predictions deviating from the unknown class given unknown labels should also not be treated as false.
%======================================%
%
%======================================%
\subsection{Methodology to use deep ISMs in Occupancy Mapping}
\label{subsec:method_to_use_deep_isms_in_occmaps}
In order to investigate H\ref{hyp:temporal_dependence}, an alternative to the standard combination rules for evidential occupancy mapping will be proposed in this section. The problem addressed in H\ref{hyp:temporal_dependence} revolves around accumulation of redundant information over time. This, directly leads to the question of how to measure the information content in deep \gls{ism}s in the first place, as formulated in RQ\ref{requ:how_to_meas_info}. As stated in Section \ref{subsec:research_needs_for_usage_of_deep_ims_as_priors_in_occmapping}, the commonly used entropy and specificity measure cannot be used to quantify the information in evidential occupancy mapping. However, since the deep \gls{ism}s in this work are constructed in a way to suffice R\ref{subreq:unknown_mass}, the unknown mass should correlate with the information content. This assumption is verified in the experiments discussed in sections \ref{sec:al_uncert_in_deep_isms} and \ref{sec:cam_lidar_fusion_in_deep_isms}.
\\\\
Given the unknown mass as a measure for information, the problem of how to quantify the temporal redundancy shall be tackled (RQ\ref{requ:how_to_meas_redund}). In this work, the redundancy shall be assessed through mutual information, for reasons explained in Section \ref{subsec:research_needs_for_usage_of_deep_ims_as_priors_in_occmapping}. Alternatives are briefly discussed in Section \ref{subsec:discussion_of_red_analy}. To obtain the mutual information, this work proposes to use temporally accumulated measurement signals as inputs for the deep \gls{ism}s. In doing so, the deep \gls{ism} learns to directly approximate the occupancy state $\vec{m}_{0:T}$ given the information $I_{0:T}$ of time step zero up to the current step $T$. On the other hand, before fusion, the map provides a successively obtained estimate of the environment state $\vec{m}_{0:T-1}$ of all previous measurements. Ideally, this means that the deep \gls{ism} prediction contains all of the information stored in the occupancy map around the current ego vehicle's location with the addition of the information captured in the current time step. Thus, using the unknown mass as an inverse measure for information, the non-redundant part of the information can be approximated as follows
\begin{align}
	\label{eq:non_redund_info_}
	I^{T|0:T} &= I^{0:T} - I^{0:T-1} \approx \underbrace{m^{u,0:T-1} - m^{u,0:T}}_{\equiv\Delta m_{u}}
\end{align}
However, a problem arises in case the static environment assumption of occupancy mapping is violated. When solely using $\Delta m_u$ as a measure for information, a shift between free and occupied can only be achieved if the new signal is more certain. To allow for class shifts even if measurement signals are equally certain, it is proposed to add the conflict $K$ as defined in \eqref{eq:conflict} to the difference in unknown mass leading to the following improved approximation of novel information 
\begin{align}
	\label{eq:non_redund_info}
	I^{T|0:T} &\approx \Delta m_{u} + K \in [-1,1]
\end{align}
In case a new measurement contains conflicting information $K$ but is also less certain than the current map state, $\Delta m_u$ will become negative and reduce the amount of novelty in the update. This effect is desired to reduce the influence of outliers. The qualitative evaluation in the bottom plot of Fig. \ref{fig:ds_comb_rule_comparison} in Section \ref{sec:exp_choice_comb_rule} shows the approximated amount of novel information in several fusion constellations.
\\\\
At this point, it should be mentioned that the map region around the vehicle could simply be updated by replacing the old map state with the new deep \gls{ism} estimate since it already predicts $m_{u,0:T}$. This, however, would remove all of the geo \gls{ism}'s influences. Therefore, it is rather proposed to discount the deep \gls{ism} estimate according to the amount of non-redundant information and combine it with the current map state. To do so, RQ\ref{requ:how_to_define_discount_fact} has to be answered to obtain a discount factor for the discount operation defined in Eq. \ref{eq:discount_op}. The requirements for the discount factor $\gamma$ can be formulated as follows
\begin{align}
	\label{eq:discount_factor_behavior}
	\gamma &= 
	\begin{cases}
		0, I^{T|0:T} \leq 0\\
		0, \lim\limits_{I^{T|0:T} \rightarrow 0}\\
		1, \lim\limits_{I^{T|0:T} \rightarrow 1}\\
		1, I^{T|0:T} \geq 1
	\end{cases}
\end{align}
It should be noted that, ideally, $I_{T|0:T} < 0$ never occurs since the next time step's estimate is based on at least the same amount of information as the previous one. Since the proposed approximation of $I_{T|0:T}$ in \eqref{eq:non_redund_info} lies within the interval $[-1,1]$, the behavior defined in Eq. \ref{eq:discount_factor_behavior} between 0 and 1 can e.g. be achieved in a linear way as follows
\begin{align}
	\label{eq:relu_discount_factor}
	\gamma &= \mathrm{ReLU}(\Delta m_u + K)
\end{align}
However, alternatives like a scaled hyperbolic tangent non-linearity can also be applied to e.g. dampen the influence of highly certain and highly redundant estimates. Since no clear preference is given, this work will focus on the discount factor as defined in Eq. \ref{eq:relu_discount_factor}. Finally, the decision of which combination rule to chose to combine the deep \gls{ism} estimate after discounting the redundancy will be detailed in Section \ref{subsec:method_to_use_deep_isms_as_priors_in_occmaps}.
%======================================%
%
%======================================%
\subsection{Methodology to use deep ISMs as Priors in Occupancy Mapping}
\label{subsec:method_to_use_deep_isms_as_priors_in_occmaps}
In this section, the requirements R\ref{subreq:initialize_with_deep_ism} and \ref{subreq:converge_to_geo_ism} to initialize a map with the deep \gls{ism} while later converge to the geo \gls{ism} shall be tackled. To do so, it shall be first discussed how the prior information can be integrated into the map in a way that its influence is disabled after enough data has been collected (see RQ\ref{requ:disable_deep_ism_influence}). Normally, as explained in Section \ref{sec:occupancy_mapping}, the map's state is initially set to the prior e.g. obtained through previous mapping of the environment. In this work, however, the map shall be initialized using a deep \gls{ism} during mapping. A simple procedure to do so would be to initially set the state of all grid cells to unknown. Afterwards, if a grid cell is still in the initial state and falls in the deep \gls{ism}'s field of view, replace the grid cell's value by the deep \gls{ism}'s estimate and leave it untouched otherwise. However, since the deep \gls{ism}'s estimates are noisy, potentially prone to errors and to account for the fact that the estimates might improve due to better measurement coverage at later time steps, the following three stepped procedure is proposed to filter these effects. 
\\\\
\textbf{Start Phase}\\
First, in the start phase, all cells shall be set to $m_u = 1$ as illustrated on the left side in Fig. \ref{fig:three_phased_approach}. 
\\\\
\textbf{Initialization Phase}\\
Afterwards, in the initialization phase, both the geo and deep \gls{ism} estimates are integrated into the map. This phase lasts until a definable amount of measurement information has been collected. In this work, the information content is described using the unknown mass (see Section \ref{subsec:method_to_use_deep_isms_in_occmaps}). Therefore, a threshold on the unknown mass $\underline{m}_u$ is introduced to quantify whether enough data has been collected in a parameterizable way. Since the deep \gls{ism} shall only be used to initialize the map, its estimates should be integrated in a way that the unknown mass never falls below $\underline{m}_u$. At the same time, in case the true occupancy state changes, the combination should be conducted in a way to allow shifting mass between occupied and free while keeping or even recuperating unknown mass. The possibility to recuperate unknown mass is important since once the unknown mass has reached $\underline{m}_u$, the deep \gls{ism}'s influence is suppressed given the redundancy removal is applied as proposed in Section \ref{subsec:method_to_use_deep_isms_in_occmaps}. This potentially prohibits the state change to be fully successful. To achieve the above described properties, the following procedure is proposed.
\\\\
First, restrict the certainty of the deep \gls{ism} estimates to $\underline{m}_u$ using the discounting operation as follows
\begin{align}
	\underline{\tilde{\vec{m}}}^{0:T} &= (1-\underline{m}_u) \otimes \tilde{\vec{m}}^{0:T}
\end{align}
with $\tilde{\vec{m}}^{0:T}$ being the evidential mass estimate of one grid cell provided by the deep \gls{ism} given accumulated measurement information from timestep zero till the current timestep $T$. This restriction of certainty, however, does not influence the amount of redundancy in the deep \gls{ism} estimates. Thus, the next step consists in removing the redundancy as proposed in Section \ref{subsec:method_to_use_deep_isms_in_occmaps} by an additional discounting operation. The restricted deep \gls{ism} certainty together with the discounting of non-redundant information makes sure that once a map cell's unknown mass has fallen beneath $\underline{m}_u$, all further deep \gls{ism} estimates are ignored. However, it is still possible for the deep \gls{ism} to reduce the unknown mass below the lower limit in the combination step as long as it has not been reached. Therefore, the discount factor to remove the redundancy has to be adapted to suffice the following condition 
\begin{align}
	\vec{m}^{0:T} &= \vec{m}^{0:T-1} \oplus \underbrace{(\gamma \otimes \underline{\tilde{\vec{m}}}^{0:T})}_{\approx \tilde{\vec{m}}^{T|0:T}}\\
	\underline{m}_u &\leq m^{0:T}_{u}
	\label{eq:lower_bound_condition}
\end{align} 
with $\vec{m}^{0:T-1}$ and $\vec{m}^{0:T}$ being the occupancy map's state accumulated from timestep zero to timestep $T$ and $T-1$ respectively and $\tilde{\vec{m}}^{T}$ being the redundancy-removed evidential mass estimate at timestep $T$ given the former timesteps information. Here, the adaption of $\gamma$, defined in \eqref{eq:relu_discount_factor}, in a way that the fusion respects the lower bound $\underline{m}_{u}$, depends on the choice of combination rule. Since the combination requires the possibility to recuperate unknown mass, Yager's rule is chosen over Dempster's (for details on the properties of evidential combination rules see Section \ref{subsec:combination_of_independent_evidence} and Section \ref{sec:exp_choice_comb_rule}). Given Yager's combination rule, $\gamma$ shall be chosen as follows
\begin{align}
	\underline{m}_u &\leq m^{0:T}_{u} \underbrace{=}_{\eqref{eq:yagers_rule}} m_{u}^{0:T-1}\tilde{m}_{u}^{T}+K \\
\end{align}
Including the definition of $K$ and the discount operation, it follows that
\begin{align}
	\underline{m}_u & \underbrace{\leq}_{\eqref{eq:discount_op}\text{ \& }\eqref{eq:conflict}} m_{u}^{0:T-1}(1-\gamma+\gamma\tilde{m}_{u}^{0:T}) + m_{o}^{0:T-1}\gamma\tilde{m}_{f}^{0:T} + m_{f}^{0:T-1}\gamma\tilde{m}_{o}^{0:T}\\
%
	\underline{m}_u &\leq m_{u}^{0:T-1} + \gamma (\underbrace{m_{u}^{0:T-1}\tilde{m}_{u}^{0:T} - m_{u}^{0:T-1} + m_{o}^{0:T-1}\tilde{m}_{f}^{0:T} + m_{f}^{0:T-1}\tilde{m}_{o}^{0:T}}_{=: \xi}) \quad|-m_{u}^{0:T-1} |\div\xi
\end{align}
Since the above derivation is made for the case of combination in the initialization phase, both the map cell's and deep \gls{ism} estimate's unknown masses are in the interval $m_u^{0:T-1},\quad \tilde{m}_u^{0:T}\in[\underline{m}_u,1]$. Thus, for $\xi\equiv 0$ \eqref{eq:lower_bound_condition} is satisfied.\\
In case $\xi \neq 0$, the two following solutions can be found 
\begin{align}
	\text{case: } \xi > 0 \rightarrow \dfrac{\underline{m}_u -m_{u}^{0:T-1}}{\xi} &\leq \gamma\\
	\text{case: } \xi < 0 \rightarrow \dfrac{\underline{m}_u -m_{u}^{0:T-1}}{\xi} &\geq \gamma 
\end{align}
Here, the case of $\xi > 0$, can be further simplified. Since $\underline{m}_u -m_{u}^{0:T-1} \leq 0$ and $\gamma \in [0,1]$, $\gamma$ already fulfills the requirement and, thus, does not need to be adapted. Therefore, the final discount factor to both reduce the informational redundancy in the deep \gls{ism} estimate and, at the same time, respects a lower bound on the unknown mass $\underline{m}_{u}$ after combination using Yager's rule can be written as 
\begin{align}
	\text{case: } \xi > 0 \rightarrow \tilde{\gamma} &= \gamma\\
%
	\text{case: } \xi < 0 \rightarrow \tilde{\gamma} &= \min \left(\gamma, \dfrac{\underline{m}_u -m_{u}^{0:T-1}}{\xi} \right)
\end{align}
This modified Yager rule shall be referred to as lower bounded Yager rule. To fuse the geo \gls{ism} estimates into the map during the initialization phase, it is again proposed to use Yager's combination rule. The reason is the same as for the deep \gls{ism} fusion and revolves around the fact that Yager's rule is better suited to cope with changes in the true occupancy state.
\\\\
In the center of Fig. \ref{fig:three_phased_approach}, the main properties of the initialization phase to alter the map's state up the the lower bound $\underline{m}_{u}$ are depicted. It also shows the capability to move mass between free and occupied while recuperating unknown mass. Furthermore, a summary of the combination rule's desired properties both for the deep and geo \gls{ism} in the initialization phase are listed in Fig. \ref{tab:desired_comb_rule_properties}.
\begin{center}
	\import{imgs/06_research_approach/method/}{three_phased_approach.pdf_tex}
	\captionof{figure}{\label{fig:three_phased_approach}Illustration of the three phases of a grid cell's occupancy state. Beginning with the start phase where the evidential mass is set to be all unknown. Afterwards, in the initialization phase, the unknown mass can be shifted into the free and occupied class and be recuperated using the deep and geo \gls{ism}'s predictions. Finally, in the convergence phase, the geo \gls{ism}'s estimates are used to strictly reduce the unknown mass while the deep \gls{ism} is being disabled.}
\end{center}
\textbf{Convergence Phase}\\
Once the unknown mass has fallen below the threshold $\underline{m}_u$, the convergence phase starts. Here, the geo \gls{ism} should still be integrated into the map while the influence of the deep \gls{ism} should be disabled, guaranteeing a convergence towards the geo \gls{ism}. The combination rule for the deep \gls{ism} as defined for the initialization phase already makes sure that the influence of the deep \gls{ism} is being disabled in the convergence phase. 
\\\\
For the geo \gls{ism}, a combination rule has to be chosen that integrates the estimates in a way to strictly reduce the unknown mass but at the same time is capable to shift mass between the free and occupied class to, again, account for changes in the occupancy state or for correction purposes. To suffice these requirements, the usage of an adapted Yager's rule is proposed. The adaptation seeks to disable the capability to recuperate unknown mass to ensure that the unknown mass always remains below $\underline{m}_{u}$. To do so, it is proposed to assign the conflict $K$ in equal portions to the free and occupied mass instead of assigning it to the unknown mass. This can be written as follows
\begin{align}
	\label{eq:yaders_rule}
	\vec{m}_1 \oplus_B \vec{m}_2 &=  
	\begin{bmatrix} 
		m_{f1}m_{f2} + m_{f1}m_{u2} + m_{u1}m_{f2} + K/2\\
		m_{o1}m_{o2} + m_{o1}m_{u2} + m_{u1}m_{o2} + K/2\\
		m_{u1}m_{u2}\\
	\end{bmatrix}
\end{align}
This combination rule shall be referred to as \textbf{YaDer}'s rule since it combines the properties of Yager's rule to model conflicting mass and of Dempster's rule to strictly reduce unknown mass. 
\\\\
An overview of the desired combination rule properties in each phase is provided in Fig. \ref{tab:desired_comb_rule_properties}. For a qualitative evaluation of the combination approaches for each phase together with a comparison against the two baseline combination rules, please be referred to Section \ref{sec:exp_choice_comb_rule}. Furthermore, Section \ref{sec:exp_analyze_prior_properties} shows the experimental results of applying this procedure for occupancy mapping on real world data. 
\begin{center}
	\begin{tabular}{c|c|l|l}
		\multicolumn{1}{c|}{\bfseries Phase} & 
		\multicolumn{1}{c|}{\bfseries Model} &  	
		\multicolumn{1}{c|}{\bfseries No Conflict} &  	
		\multicolumn{1}{c}{\bfseries Conflict}\\
\hline
		\parbox[t]{5mm}{\multirow{4}{*}{\rotatebox[origin=c]{90}{init.}}}
		& geo \gls{ism}
		& $\bullet$ fully converge & $\bullet$ fully converge \\
		&
		& $\bullet$ strictly reduce $m_u$& $\bullet$ recuperate $m_u$ \\
\cline{2-4}
		& deep \gls{ism}
		& $\bullet$ converge up to $\Delta m_u = 0$ & $\bullet$ converge up to $\Delta m_u = 0$ \\
		&
		& $\bullet$ strictly reduce $m_u$ & $\bullet$ recuperate $m_u$ \\
\hline
\hline
	\parbox[t]{5mm}{\multirow{4}{*}{\rotatebox[origin=c]{90}{converg.}}} 
	& geo \gls{ism}
	& $\bullet$ fully converge & $\bullet$ fully converge \\
	& 
	& $\bullet$ strictly reduce $m_u$ & $\bullet$ strictly reduce $m_u$ \\
\cline{2-4}
	&deep \gls{ism}
	& $\bullet$ unchanged & $\bullet$ unchanged \\
	&&&
	\end{tabular}
	\captionof{table}{\label{tab:desired_comb_rule_properties}Overview of the desired combination rules' properties in each mapping phase.}
\end{center} 