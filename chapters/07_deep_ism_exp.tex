% !TeX root = ../main.tex
\chapter{Deep ISM Experiments}
\label{ch:deep_ism_exp}
As explained in \ref{subsec:choice_of_dataset}, the experiments in this work will be conducted based on the NuScenes dataset. Here, the train-val-test split as proposed in NuScenes is used, while some of the scenes have been removed. Specifically, all scenes tagged with "night" or "difficult lighting" have been filtered out since they are relatively rare and thus, the networks with camera inputs could not properly adapt. Additionally, some scenes contain little to no ego vehicle movement (e.g. ego vehicle waiting at a red traffic light) which are great scenarios for tracking tasks but largely violate the static environment assumption in occupancy mapping. Thus, only scenes in which the ego vehicle moved at least $20$ m are considered.
\\\\
To obtain denser measurements for occupancy mapping, the sweeps are used to create the occupancy mapping dataset. Here, the sensor modality with the fewest sweeps per scene is identified and chosen as reference. Next, the temporally closest sweeps of the remaining sensors towards the reference are processed. Afterwards, sensor-dependent procedures are applied to create the different baselines, inputs and targets for the investigated geometric and deep \gls{ism}s in form of a $128 \times 128$ grid map centered around the hind axle of the ego vehicle and spanning an area of $40 \times 40$ m$^2$.
%==========================================================================%
%
%==========================================================================%
\section{Parameterization of Geo ILM and IRM}
\label{sec:choice_of_gt}
This section details the comparison of different approaches to adapt lidar to radar \gls{bev} information. First, the sensor characteristics in the chosen dataset will be listed together with the applied methods to adapt the lidar. Afterwards, the different lidar filtering approaches will be evaluated based on the overlap between the lidar and radar maps in the mapped areas quantified by the m\gls{iou} score. 
%======================================%
%
%======================================%
\subsection{Experimental Setup}
\label{subsec:exp_setup_gt}
In the following, evidential occupancy maps of scenes as defined in the NuScenes dataset are created based on the geometric \gls{irm} and \gls{ilm} as described in Section \ref{subsec:method_geo_ilm} and \ref{subsec:method_geo_irm}. Because of the large space of parameter configurations, the search is only conducted on the first 10\% of the available training scenes. To fix the parameters and identify the best performing \gls{ism} variant, a two stepped approach is proposed.
\\\\
The first steps consists of estimating the best parameters for the geo \gls{ilm} and \gls{irm}. Since no additional occupancy ground truth is available to tune both the lidar and radar models, a temporary \gls{ilm} is manually configured by the author to provide a reference. This reference is further used to perform a parameter grid search for each of the geo \gls{irm} variants mentioned in Section \ref{subsec:method_choice_of_gt}. Here, the parameters, if not fixed for the given variant, are searched as shown in Fig. \ref{tab:geo_irm_variants_gird_search_range}. These ranges are based on prior experience of the author. The temporary geo \gls{ilm} uses the height-threshold-based ground-plane removal since it is the most widely used method in the literature.
\begin{figure}
	\begin{center}
		\begin{tabular}{c|c|c|c}
			$M_F^{(\text{thin})}$ & $M_F^{(\text{big})}$ & $\varphi_\sphericalangle^{(\text{\scriptsize thin})}$ & $\varphi_\sphericalangle^{(\text{\scriptsize big})}$\\
			\hline
			$[0.1,0.2,...,0.6]$ & $[0.3,0.4,...,0.8]$ & $[1^{\circ},1^{\circ},...,5^{\circ}]$ & $\{10^{\circ}, 20^{\circ},30^{\circ},40^{\circ}\}$\\
			\multicolumn{4}{c}{}\\
			$M_O$ & $M_D$ & \multicolumn{2}{c}{$N$}\\
			\hline
			$[0.3, 0.4, ..., 0.8]$ & $[0.1, 0.2, ..., 0.5]$ & \multicolumn{2}{c}{$\{1,5,10,15,20,25\}$}
		\end{tabular}
	\end{center}
	\caption{\label{tab:geo_irm_variants_gird_search_range}Ranges for geo \gls{irm} variants' parameter grid search given the parameters are not fixed for the respective variant.}
\end{figure} 
In the second step, the afore best performing geo \gls{irm} variant will be kept fixed and used as a reference to analyze the ground-plane removal variants. Here, the two filters under investigation are a purely geometric height threshold-based and a semantic filter. The height thresholds are compared for different heights in the interval $[0,0.1]$ with step size $0.025$ and in the interval $[0.1,2.0]$ with step size $0.1$ in order to have a higher resolution close to zero. For the semantic filters, in the majority of cases the street detections are closest to the ego vehicle in the \gls{bev} projection followed by sidewalks and terrain. Since the removal of detections in occluded areas has little to no effect for geometric \gls{ism}s, a three stepped removal of the semantics is proposed which removes pixels of classes on average increasingly further away from the vehicle. The three removal steps are as follows $[$no street; no street or sidewalk; no street, sidewalk or terrain$]$.
\\\\
In order to obtain lidar occupancy maps closer resembling the ground-truth occupancy state, the dynamic state provided by the bounding box labels is propagated to corresponding lidar detections. All detections marked as dynamic are only used to provide boundaries for free space rays but not to define occupied space. Since the sweep information is used for mapping but the labels are only available on a sample level, the bounding box poses of dynamic objects have to be interpolated as described in \ref{subsec:method_dyn_info_for_lidar}. 
%======================================%
%
%======================================%
\subsection{Experimental Results for Geo ILM and IRM Parameter Tuning}
\label{subsec:exp_results_params_ilm_irm}
In order to parameterize the temporary geo \gls{ilm}, the height-threshold will be defined first. Here, the parameter is increased up to the point where additional structure besides street is being removed. Using the so found height threshold of 0.6 cm, the free and occupied weight $M_F, M_O, M_D$ and the opening angle $\varphi_\sphericalangle$ of Alg. \ref{alg:geo_ilm} are balanced in a way that 
\begin{itemize}[noitemsep,nolistsep]
	\item remaining ground points and dynamic object artifacts are being filtered out
	\item \gls{idm} rays don't cut through object boundaries
	\item object boundaries are as dense as possible
	\item free and occupied space has maximal assigned evidential mass
\end{itemize}
leading to the parameters summarized in Tab. \ref{tab:ilm_irm_tuning}. For an illustration of the \gls{ilm} tuning, refer to Fig. \ref{fig:ilm_irm_tuning}.
\begin{figure}[H]
	\begin{center}
		\resizebox{\textwidth}{!}{
		\import{imgs/07_deep_ism_exp/choice_of_gt}{ilm_irm_tuning.pdf_tex}}
		\caption{\label{fig:ilm_irm_tuning}Examples of the chosen \gls{ism} parameterization. First three rows showing samples of the geo \gls{irm} variants applied on a specific frame of a scene with the resulting occupancy map of the scene on the right. It can be seen that the additional detections help preserve the occupied space while also restrict free space being assigned behind object boundaries. Moreover, the additional free space rays help to densify the estimated free space while leaving the occupied space unaltered. At the bottom, the used geo \gls{ilm} is applied to the lidar data of the same frame again with the final occupancy map to its right. Additionally, examples of the \gls{ilm} with sub-optimal free mass $M_F$ and ray opening angle $\varphi_\sphericalangle$ are shown on the right. It can be seen that too small $M_F$ leads to remaining occupied outliers due to e.g. imperfect dynamic object removal while too narrow $\varphi_\sphericalangle$ leads to rays being cast through object boundaries.}
	\end{center}
\end{figure} 
After fixing the temporary \gls{ilm}, it can be used to tune the \gls{irm} using the mIoU between their occupancy maps. As mentioned in Section \ref{subsec:method_geo_irm}, three \gls{irm} variants are separately tuned and compared against each other. As shown in Tab. \ref{tab:geo_irm_comparison}, the accumulation of detections results in additional information about occupied space and helps to correct the free space leading to a better overall mIoU. Additionally, enriching the free space with the larger \gls{idm} rays keeps the occupied score untouched while moving unknown mass to the free class, leading to the best mIoU score of the considered \gls{irm} variants. These variants together with resulting occupancy maps are illustrated in Fig. \ref{fig:ilm_irm_tuning}.
\begin{center}
	\resizebox{\textwidth}{!}{
	\begin{tabular}{c|ccc|c}
		& \multicolumn{4}{c}{mIoU}\\
		\gls{irm} variants & free & occupied & unknown & overall \\
		\hline
		ray casting & 67.0 & 16.8 & 21.5 & 35.1\\
		ray casting + acc. detections & 70.6 & 26.6 & 21.5 & 39.9\\
		ray casting + acc. detections + free rays & 76.2 & 26.6 & 22.6 & 42.1\\		
	\end{tabular}}
	\captionof{table}{\label{tab:geo_irm_comparison}Comparison of mIoU of occupancy maps generated using three \gls{irm} variants and lidar occupancy maps.}
\end{center}
\begin{center}
	\begin{tabular}{c|c|c|c|c|c|c|c}
		\gls{ism} & $M_F^{(\text{thin})}$ & $M_O$ & $M_D$ & $\varphi_\sphericalangle^{(\text{thin})}$ & $M_F^{(\text{big})}$ & $\varphi_\sphericalangle^{(\text{big})}$ & N\\
		\hline
		\gls{ilm} & 0.025 & 0.5 & 0.3 & 3$^\circ$ & 0 & 0 \\
		\gls{irm} \#1 & 0.1 & 0.8 & 0.3 & 5$^\circ$ & 0 & 0 & 1\\
		\gls{irm} \#2& 0.2 & 0.3 & 0.3 & 5$^\circ$ & 0 & 0 & 20\\
		\gls{irm} \#3& 0.2 & 0.3 & 0.3 & 5$^\circ$ & 0.2 & 30$^\circ$ & 20 
	\end{tabular}
	\captionof{table}{\label{tab:ilm_irm_tuning}Parameters used for geo \gls{ilm} and \gls{irm} (see Section \ref{subsec:method_choice_of_gt}) to produce the qualitative and quantitative results in Fig. \ref{fig:ilm_irm_tuning} and \ref{tab:geo_irm_comparison}. The parameters are chosen via grid search.}
\end{center}
%======================================%
%
%======================================%
\subsection{Experimental Results for Lidar Ground Plane Removal Variants}
\label{subsec:exp_results_gt}
\begin{figure}[H]
	\begin{center}
		\resizebox{\textwidth}{!}{
		\import{imgs/07_deep_ism_exp/choice_of_gt}{qual_results_gt_choice_scene040.pdf_tex}}
		\caption{\label{fig:qual_results_gt_choice_scene040}Example of lidar maps created by successively removing ground-plane semantics and three threshold-based filters, together with the mapped area (white) and the radar map. The white box in the bottom left corner shows that for lower height threshold and up to semantics of sidewalks, many structures detected in the radar map are occluded. On the other hand, setting the height threshold too high (here demonstrated for threshold $1.7$ m) removes too many points at the forefront, as shown in the white box in the upper right corner. Height threshold of about $0.5$ m or the semantic-based removal up to the terrain level show a good compromise.}
	\end{center}
\end{figure} 
The quantitative comparison in Fig. \ref{fig:miou_results_gt_choice} shows that the successive semantic-based removal up to the terrain level leads to increasingly better overlap up to the best reached m\gls{iou} score of $28.18\%$. On the other hand, the height threshold-based filters show improved performance up to a height threshold of $0.5$ m with a score of $26.96\%$ after which the performance starts to decrease. This suggests that for the street and sidewalk level semantics as well as low height threshold large portions of the areas detected by the radar are occluded in the lidar \gls{bev}. This is also qualitatively shown in the lower left white boxes in Fig. \ref{fig:qual_results_gt_choice_scene040}. Moreover, when the height threshold is set too high, portions of the areas detected by the radar are increasingly filtered out, as illustrated in the upper right boxes in Fig. \ref{fig:qual_results_gt_choice_scene040}. Thus, a height threshold of about $0.5$ m or the semantic-based removal up to the terrain level provide the best compromise of the compared methods. However, since the semantic information is only available for keyframes in the NuScenes dataset and because the $0.5$ m height threshold filter rivals the best semantic filter in its performance, it is proposed to use the height threshold-based filter to obtain the labels for further experiments.
\begin{figure}[htp!]
	\begin{center}
		\resizebox{\textwidth}{!}{
		\import{imgs/07_deep_ism_exp/choice_of_gt}{mIouGtVerification.tex}}
		\caption{\label{fig:miou_results_gt_choice}Results of mIoU between different evidential occupancy maps create with variations of geometric \gls{ilm}s and the geometric \gls{irm} computed in the mapped area.}
	\end{center}
\end{figure}
%======================================%
%
%======================================%
\subsection{Discussion}
\label{subsec:discussion_gt}
Looking at the results of the geo \gls{irm} variant comparison, it becomes clear that the two proposed improvements, namely the accumulation of detections over time and the additional application of the \gls{idm} with a wide cone, indeed outperform the baseline by a margin of $7\%$ in m\gls{iou}. However, there have been cases in which the accumulation lead to a persistence of outliers increasing their influence. However, those cases are statistically outweighed by the benefit of restricting rays from cutting through occupied regions. It shall also be mentioned that there are still large areas marked as unknown which are clearly free. An example of which is the vehicle's trajectory. Here, the geo \gls{irm} can be changed to set the region beneath the ego vehicle to free. For the other regions further away from detections and ego vehicle further investigations can be done to improve the free space coverage. Overall, this procedure has shown to suffice RQ\ref{requ:fr_space_enrichment}, since it strictly improves the m\gls{iou} scores of all classes over the baseline model.
\\\\
With regards to the analysis of ground-plane removal techniques according to RQ\ref{requ:what_is_best_gt}, the comparison of lidar ground plane removal methods demonstrates that the removal of detections up to the terrain level increases the overlap between lidar and radar sensing modalities and provides the best result. Additionally, it can be seen that the application of the simple height threshold removal method can reach similar performance when it comes to aligning the sensing overlap. Thus, since the lidar labels are only provided with low frequency on the sample level and an interpolation down to the higher frequent sweep level is non-trivial, it is proposed to use the height threshold method for the further experiments. 
%==========================================================================%
%
%==========================================================================%
\section{Choice of UNet Architecture}
\label{sec:choice_of_unet_arch}
%======================================%
%
%======================================%
\subsection{Experimental Setup}
\label{subsec:exp_setup_unet_arch}
Since the focus of this work lies on the investigation of radar \gls{ism}s, the architecture search is performed based on radar input images with occupancy map patches for targets (see Section \ref{subsec:def_of_targets_n_inputs}). More specifically, radar \gls{bev} images based on one sweep's information are used, since they contain the least information and, thus, provide the most difficult task for the network to handle. Moreover, the considered UNet (see Section \ref{subsec:method_deep_ism_architecture}) is trained in SoftNet configuration (see Section \ref{subsec:method_al_uncert_in_deep_isms}), since it is the baseline configuration as proposed by the literature.
\\\\
The hyperparameters searched with the following procedure are the downsample factor $D$ of each ResNet layer in the UNet and the amount of filters for each stage $C_k, k \in [0,4]$. With regards to the filters, the approach proposed in the literature to double the amount after each encoder and halve it after each decoder stage respectively up to a maximum number of filters is adapted in this work (see Section \ref{subsec:architecture}). Thus, reducing the filter search to the initial amount of filters $C_0$ and the maximum amount of filter $C_{\max}$. These hyperparameters shall be investigated in a two stepped approach. First, $D$ is set to $0.5$ which is half of the most conservative compression rate reported to work without loss in performance (see Section \ref{subsec:architecture}). On the other hand, for $C_0$, the following variations are evaluated $[4,8,16,24,32,40]$ with $C_{\max} = \infty$. Given the results of these variations, a configuration with a good trade-off between inference speed and accuracy is chosen for further optimization. Here, based on personal experience and the architectures reported in the literature, $C_{\max}$ is set to $128$ filters. Additionally, $D$ is successively halved starting from $0.25$, which is still reported throughout the literature to work without significant loss of accuracy, up to the point of collapse in performance.
\\\\
All these experiments are conducted using Tensorflow 2.1 \cite{abadi2016tensorflow}, the ADAM optimizer \cite{kingma2014adam} with a learning rate of $0.001$ and a dropout rate of $0.3$. The layers are initialized using the HeNormal initializer \cite{he2015delving} and trained until convergence, as indicated by the validation set, to remove the bias due to the random initialization. The experiments are conducted on two NVIDIA Tesla V100 (32 GB) GPUs and evaluated on a single core of an Intel Core Processor i7-10750H CPU.
%======================================%
%
%======================================%
\subsection{Experimental Results}
\label{subsec:exp_results_unet_arch}
As explained above, the architecture search is split into two parts. Here, the first part fixes the ResNet layer's downsample rate $D$ and alternates the initial number of filters $C_0$ with $C_{\max} = \infty$. These hyperparameter configurations are summarized in the experiments $1-6$ in Fig. \ref{tab:networking_tuning_exp_setup} while the results are shown in Fig. \ref{fig:network_tuning} marked in orange. The experiments show an exponential decrease in inference time to improve the mIoU. With regards to the time till convergence of training, the time remains about the same up to $C_0 = 24$. Afterwards, for $C_0 = 32$, it more than triples and doubles again for $C_0 = 40$.
\\\\
Based on these results, $C_0 = 32$ is chosen and further used to fine tune $D$ for the following reasons. First, further increasing $C_0$ shows to move the inference speed too far away from the goal of 100 Hz as defined in R\ref{subreq:resource_efficient_inference}. However, increasing $C_0$ from $24$ to $32$ showed a significant improvement in qualitative predictions as can be seen in Fig. \ref{fig:gt_exp_qual_results}.
\begin{center}
\begin{tabular}{c|c|c|c}
	experiment number & $C_0$ & $C_{\max}$ & $D$ \\
	\hline
	1 & 4 & $\infty$ & 0.5 \\
	2 & 8 & $\infty$ & 0.5 \\
	3 & 16 & $\infty$ & 0.5 \\
	4 & 24 & $\infty$ & 0.5 \\
	5 & 32 & $\infty$ & 0.5 \\
	6 & 40 & $\infty$ & 0.5 \\
	\hline
	7 & 32 & 128 & 0.25 \\
	8 & 32 & 128 & 0.125
\end{tabular}
\captionof{table}{\label{tab:networking_tuning_exp_setup}Hyperparameter setting for all network tuning experiments}
\end{center}
To fine tune the capacity of the network, $C_0$ is fixed to 32 while an upper limit on the number of filters is set to $C_{\max}=128$. Given this setup, $D$ is halved, starting from 0.5 up to the point where the performance collapses. It can be seen that this is the case after the second reduction, as shown by the blue dots in the left plot of Fig. \ref{fig:network_tuning}. The parameter settings for the two experiments labeled "7" and "8" are shown in the lower part of \ref{tab:networking_tuning_exp_setup}. The experimental results show that a reduction of $D$ from 0.5 to 0.25 brings the network about $6\%$ closer to the goal of 100 Hz while roughly keeping its performance. Also, the interpolation capability seems to deteriorate only after the second reduction step, as shown qualitatively in Fig. \ref{fig:gt_exp_qual_results}. At the same time, the first reduction more than halves the training time.
\begin{figure}[H]
	\begin{center}
		\resizebox{\textwidth}{!}{
		\import{imgs/07_deep_ism_exp/choice_of_gt}{qualitative_comparison.pdf_tex}}
		\caption{\label{fig:gt_exp_qual_results}Qualitative results of the different models trained in the experiments as numbered in Fig. \ref{tab:networking_tuning_exp_setup} with the radar input shown on the right. It can be seen that up to $C_0=32$ in experiments $\#5$ only little interpolation of occupied space is provided by the models. A prominent example is the interpolated edge marked by the white box. Increasing $C_0$ further provides even clearer edges. Also, shrinking the bottle neck $D$ seems to keep the interpolation capability in $\#7$ while loosing it in experiment $\#8$.}
	\end{center}
\end{figure}
\begin{center}
\begin{minipage}{.5\textwidth}
	\import{imgs/07_deep_ism_exp/network_tuning}{mIoU_network_tuning.tex}
\end{minipage}
%
\hfill
\begin{minipage}{.47\textwidth}
	\import{imgs/07_deep_ism_exp/network_tuning}{train_time_network_tuning.tex} 
\end{minipage}
	\captionof{figure}{\label{fig:network_tuning}Experimental results of the network tuning experiments. On the left-hand-side, the mIoU over the CPU inference time is shown while on the right-hand-side, the training time is plotted for each experiment. Here, orange marks the first part of the experiments in which the downsamlping rate $D$ is fixed and the initial amount of filters $C_0$ is searched. On the other hand, blue marks the second stage of the parameter search in which $D$ is finetuned.}
\end{center}
%======================================%
%
%======================================%
\subsection{Discussion}
\label{subsec:discussion_unet_arch}
First of all, the choice of only conducting each experiment once shall be discussed. It shall be argued that this choice is justified since the trend of the experiments clearly follows the expected and reported behavior in the literature for experiments $\#1$ - $\#7$. Moreover, all models have been trained until convergence, thereby reducing the influence of random initialization. Regarding experiment $\#8$, the severe drop in performance can be explained as the point for which the bottleneck starts to impede the flow of information. However, since the experiments have been conducted only once for each parameter setting, the effect might also be due to outlier behavior. In any case, since the mean inference time did not significantly increase over its predecessor, the configuration in $\#7$ is chosen and $\#8$ is not repeated.  
\\\\
The experiments to fine-tune the architecture for this work clearly verify the results as stated in the state-of-the-art. It can be seen that the UNet architecture is capable to learn geometric interpolations (see interpolated corner in white box in Fig. \ref{fig:gt_exp_qual_results}). Also, the choice of network depth, and with it the network's receptive field, suffice to learn the concept of occluded areas. This can be seen in Fig. \ref{fig:gt_exp_qual_results} by the unknown areas (blue) behind occluded areas (green) given the reference point is the image center. Therefore, the model can clearly utilize the spatial coherence in huge amounts of data and, thus, suffices R\ref{subreq:input_output}, \ref{subreq:big_data} and \ref{subreq:ev_rep}.
\\\\
Moreover, it has been shown that the reduction in the ResNet layer's bottleneck can be done up to factor of $0.25$ while almost keeping the performance, which validates the results reported in the state-of-the-art. A further reduction leads to serious performance degradation (might be due to outlier behavior). Here, the variant $\#7$ is still able to predict geometric interpolations while providing an inference speed close enough to suffice R\ref{subreq:resource_efficient_inference}. While the performed architecture search used many assumptions to reduce the search space and only performed the search on rough intervals, clear trends can be shown leading to $\#7$ as the best performing model according the the given requirements. Thus, it can be argued that this procedure suffices to answer RQ\ref{requ:network_search}.
%==========================================================================%
%
%==========================================================================%
\section{Aleatoric Uncertainties in deep ISMs}
\label{sec:al_uncert_in_deep_isms}
%======================================%
%
%======================================%
\subsection{Experimental Setup}
\label{subsec:exp_setup_aleat_uncert}
To investigate how to best learn and shift the aleatoric uncertainty as described in RQ\ref{requ:how_to_sep_uncertainty}, the basic UNet architecture as tuned in Section \ref{sec:choice_of_unet_arch} is modified and trained in the SoftNet, ShiftNet and DirNet configuration as described in Section \ref{subsec:method_al_uncert_in_deep_isms}. These configurations are trained using radar inputs and occupancy map targets as described in Section \ref{subsec:def_of_targets_n_inputs}. To better investigate the effect of data uncertainty, radar inputs based on a single timestep's information $R_1$ and based on accumulation of 20 timesteps $R_{20}$ are compared. Additionally, the geo \gls{irm}'s performance for both radar inputs is evaluated to provide a reference. The evaluation is based on the confusion matrix variant as explained in Section \ref{subsec:confusion_matrix} and computed for the whole test set defined in NuScenes.
%======================================%
%
%======================================%
\subsection{Experimental Results}
\label{subsec:exp_results_aleat_uncert}
For the following interpretation of quantitative results, it shall be noted that unknown mass in other classes is not seen as false predictions but rather as an indicator for certainty. Thus, the overall false rate only equals the sum over the red scores per row for each class. 
\\\\
Starting with the R$_1$ scores, the usage of the modified confusion matrix, as opposed to the mIoU used in \ref{sec:choice_of_gt}, allows a more detailed analysis of the geo \gls{irm}. It can be seen that the geo \gls{irm} provides less then $1\%$ correct predictions both for dynamic and occupied space due to the sparseness in detections, which can also be seen in Fig. \ref{fig:irm_qual_comp}. On top of that, the false free space predictions in these two categories are almost $20\%$ showing the effect of rays cutting through occupied space due to multi-path reflections. On the other hand, while the free space predictions are sparse with about $70\%$ of the space being predicted as unknown, the rest is predicted correctly. In an overall comparison with the deep \gls{irm} variants, the geo \gls{irm} provides the least false rates in all categories but also the smallest positive rates. This experimentally shows the reason for the approach to use a deep \gls{irm} to initialize the maps and later converge to the geo \gls{irm}.
\\\\
Next, the SoftNet configuration treats the aleatoric uncertainty by equally distributing mass into the two classes between which the uncertainty occurs, as stated in H \ref{hyp:sota_not_model_unc}. In this case, the majority of uncertainty in the visible area is at the boundaries between occupied and free areas leading to huge portions of the actually free and occupied class respectively being estimated as dynamic. This effect can be seen in Fig. \ref{fig:irm_qual_comp} by the blurriness of the occupied space. Due to this bias towards the dynamic class, the true rate for dynamic objects is also the best among the investigated methods. In occluded areas, in addition to huge false rates of dynamic objects, an increase of unknown mass can be observed over all classes. This might be due to a combination of bias towards the unknown class in occluded areas and the fact, that the aleatoric uncertainty now also occurs at boundaries between free, occupied and the unknown class. 
\\\\
In contrast to SoftNet, DirNet is capable of shifting the aleatoric uncertainty into the unknown class which can be seen by less than half of the overall false dynamic predictions in the free and occupied categories while at the same time increasing the unknown mass portion over all classes. Moreover, in the occluded area, where more aleatoric uncertainty can be expected, larger portions are shifted into the unknown class which also results in smaller false rates for free and occupied cells. These effects can also been seen in Fig. \ref{fig:irm_qual_comp} by the clearer boundaries between free and occupied space and overall more unknown space in occluded regions. This, in contrast to the almost steady false rates in the visible and occluded area for SoftNet, additionally highlighting the uncertainty awareness of the DirNet configuration. On top of that, DirNet surpasses the positive rates of SoftNet in all but the biased dynamic class. The improvement in performance might be due to the effect that by modeling the aleatoric uncertainty, the loss function is weighted in a way to increasingly ignore predictions for which the network cannot find a sufficient solution. This leads to more network capacity being focused on the majority of data which leads to better average performance and, thus, better scores. It shall be noted that, while the scores improve, this behavior might lead to a neglection of edge cases.
\\\\
Finally, ShiftNet demonstrates even better capabilities in shifting the aleatoric uncertainties to the unknown class as compared to DirNet which is indicated by the highest unknown mass rates of all model variants. This uncertainty-awareness can again also be observed by higher unknown mass rates for the occluded as compared to the visible areas. The effect of shifted uncertainty is so dominant that it can even be seen in the qualitative results in Fig. \ref{fig:irm_qual_comp} by unknown space (blue) around all occupied boundaries. Regarding the occupied and free space, ShiftNet provides the least false rates and best free space predictions for all classes compared to all learned \gls{irm}s. However, it lacks the capability to predict occupied areas with high certainty.
\\\\
For the R$_{20}$ scores, an expected overall improvement of all models can be observed. Here, the free space predictions of SoftNet even improve to the point of surpassing the other variants. With regards to aleatoric uncertainty, the unknown mass portions for the DirNet and ShiftNet decrease compared to the R$_1$ scores as is desired in the light of more input information. Additionally, similar to the R$_1$ scores, an increase in unknown mass can be observed in occluded as compared to visible areas. Another notable change is the improvement in occupied predictions given by ShiftNet, now surpassing SoftNet and almost closing the gap to DirNet. The geo IRM is also capable of improving the dynamic true rate by about seven times and the occupied rate by about 13 times while reducing the respective overall false rates. This is as expected since it can leverage the information of 20 times as many timesteps. Also, the free space true rate reaches about 50$\%$ while only minimally increasing the false rates. These general improvements over the R$_1$-based predictions are also clearly visible in Fig. \ref{fig:irm_qual_comp}.
%==========================%
% R01
%==========================%
\begin{figure}[H]
	\footnotesize
	\begin{tabular}{c|c|cccc|cccc|cccc}
		&$k$ & $d$ & $f$ & $o$ & $u$ & $d$ & $f$ & $o$ & $u$ & $d$ & $f$ & $o$ & $u$\\
		\hline
		\parbox[t]{2mm}{\multirow{4}{*}{\rotatebox[origin=c]{90}{\scriptsize{geo IRM}}}}&$p(k|d)$ & \textcolor{mygreen}{0.6} & \textcolor{myred}{19.6} & \textcolor{myred}{0.4} & 79.3 & \textcolor{mygreen}{0.6} & \textcolor{myred}{34.3} & \textcolor{myred}{0.4} & 64.5 & \textcolor{mygreen}{0.6} & \textcolor{myred}{12.6} & \textcolor{myred}{0.4} & 86.3\\
		&$p(k|f)$ & \textcolor{myred}{0.0} & \textcolor{mygreen}{28.9} & \textcolor{myred}{0.0} & 70.9 & \textcolor{myred}{0.0} & \textcolor{mygreen}{35.9} & \textcolor{myred}{0.0} & 63.9 & \textcolor{myred}{0.0} & \textcolor{mygreen}{9.6} & \textcolor{myred}{0.0} & 90.2\\
		&$p(k|o)$ & \textcolor{myred}{0.2} & \textcolor{myred}{18.4} & \textcolor{mygreen}{0.8} & 80.5 & \textcolor{myred}{0.3} & \textcolor{myred}{30.6} & \textcolor{mygreen}{0.9} & 68.1 & \textcolor{myred}{0.2} & \textcolor{myred}{14.5} & \textcolor{mygreen}{0.8} & 84.4\\
		&$p(k|u)$ & 0.0 & 5.4 & 0.1 & 94.4 & - & - & - & - & 0.0 & 5.4 & 0.1 & 94.5\\
		\hline
%		\multicolumn{13}{c}{}\\
%		\hline
		\parbox[t]{2mm}{\multirow{4}{*}{\rotatebox[origin=c]{90}{\scriptsize{SoftNet}}}}&$p(k|d)$ & \textcolor{mygreen}{49.7} & \textcolor{myred}{16.7} & \textcolor{myred}{10.5} & 23.2 & \textcolor{mygreen}{52.5} & \textcolor{myred}{18.8} & \textcolor{myred}{11.5} & 17.2 & \textcolor{mygreen}{48.8} & \textcolor{myred}{15.8} & \textcolor{myred}{9.6} & 25.8\\
		&$p(k|f)$ & \textcolor{myred}{35.8} & \textcolor{mygreen}{37.0} & \textcolor{myred}{3.3} & 24.0 & \textcolor{myred}{36.2} & \textcolor{mygreen}{42.6} & \textcolor{myred}{3.1} & 18.2 & \textcolor{myred}{35.5} & \textcolor{mygreen}{21.5} & \textcolor{myred}{4.0} & 39.1\\
		&$p(k|o)$ & \textcolor{myred}{37.8} & \textcolor{myred}{9.6} & \textcolor{mygreen}{17.2} & 35.5 & \textcolor{myred}{43.7} & \textcolor{myred}{11.5} & \textcolor{mygreen}{20.0} & 24.8 & \textcolor{myred}{35.9} & \textcolor{myred}{9.0} & \textcolor{mygreen}{16.2} & 38.9\\
		&$p(k|u)$ & 25.7 & 8.7 & 4.5 & 61.0 & - & - & - & - & 25.7 & 8.7 & 4.5 & 61.1\\
		\hline
%		\multicolumn{13}{c}{}\\
%		\hline
		\parbox[t]{2mm}{\multirow{4}{*}{\rotatebox[origin=c]{90}{\scriptsize{DirNet}}}}&$p(k|d)$ & \textcolor{mygreen}{31.0} & \textcolor{myred}{21.9} & \textcolor{myred}{12.8} & 34.3 & \textcolor{mygreen}{35.4} & \textcolor{myred}{27.4} & \textcolor{myred}{13.0} & 24.2 & \textcolor{mygreen}{29.0} & \textcolor{myred}{19.9} & \textcolor{myred}{12.3} & 38.7\\
		&$p(k|f)$ & \textcolor{myred}{13.6} & \textcolor{mygreen}{47.5} & \textcolor{myred}{5.6} & 33.3 & \textcolor{myred}{15.5} & \textcolor{mygreen}{56.8} & \textcolor{myred}{4.6} & 23.2 & \textcolor{myred}{9.0} & \textcolor{mygreen}{22.3} & \textcolor{myred}{8.4} & 60.3\\
		&$p(k|o)$ & \textcolor{myred}{13.5} & \textcolor{myred}{9.7} & \textcolor{mygreen}{20.7} & 56.1 & \textcolor{myred}{22.4} & \textcolor{myred}{14.3} & \textcolor{mygreen}{22.9} & 40.3 & \textcolor{myred}{10.8} & \textcolor{myred}{8.4} & \textcolor{mygreen}{19.9} & 60.9\\
		&$p(k|u)$ & 2.7 & 3.7 & 12.0 & 81.6 & - & - & - & - & 2.7 & 3.6 & 12.0 & 81.7\\
		\hline
%		\multicolumn{13}{c}{}\\
%		\hline
		\parbox[t]{2mm}{\multirow{4}{*}{\rotatebox[origin=c]{90}{\scriptsize{ShiftNet}}}}&$p(k|d)$ & \textcolor{mygreen}{31.8} & \textcolor{myred}{19.0} & \textcolor{myred}{7.2} & 41.9 & \textcolor{mygreen}{30.7} & \textcolor{myred}{23.3} & \textcolor{myred}{7.5} & 38.6 & \textcolor{mygreen}{33.3} & \textcolor{myred}{17.2} & \textcolor{myred}{6.9} & 42.7\\
		&$p(k|f)$ & \textcolor{myred}{7.4} & \textcolor{mygreen}{48.6} & \textcolor{myred}{2.0} & 42.0 & \textcolor{myred}{7.5} & \textcolor{mygreen}{56.0} & \textcolor{myred}{1.6} & 34.9 & \textcolor{myred}{7.6} & \textcolor{mygreen}{28.1} & \textcolor{myred}{3.2} & 61.1\\
		&$p(k|o)$ & \textcolor{myred}{8.4} & \textcolor{myred}{15.1} & \textcolor{mygreen}{13.7} & 62.8 & \textcolor{myred}{10.0} & \textcolor{myred}{19.2} & \textcolor{mygreen}{14.9} & 55.9 & \textcolor{myred}{7.9} & \textcolor{myred}{13.9} & \textcolor{mygreen}{13.4} & 64.8\\
		&$p(k|u)$ & 4.0 & 9.4 & 4.7 & 81.9 & - & - & - & - & 4.0 & 9.3 & 4.7 & 82.0\\
		\hline
		\multicolumn{2}{c|}{\textbf{R$_1$ Scores}} & \multicolumn{4}{c|}{overall} & \multicolumn{4}{c|}{visible} & \multicolumn{4}{c}{occluded}\\
		\multicolumn{13}{c}{}\\
%==========================%
% R20
%==========================%
		&$k$ & $d$ & $f$ & $o$ & $u$ & $d$ & $f$ & $o$ & $u$ & $d$ & $f$ & $o$ & $u$\\
		\hline
		\parbox[t]{2mm}{\multirow{4}{*}{\rotatebox[origin=c]{90}{\scriptsize{geo IRM}}}}&$p(k|d)$ & \textcolor{mygreen}{7.1} & \textcolor{myred}{24.5} & \textcolor{myred}{6.8} & 61.4 & \textcolor{mygreen}{4.5} & \textcolor{myred}{43.7} & \textcolor{myred}{7.5} & 44.1 & \textcolor{mygreen}{8.2} & \textcolor{myred}{15.9} & \textcolor{myred}{6.5} & 69.3\\
		&$p(k|f)$ & \textcolor{myred}{0.4} & \textcolor{mygreen}{47.0} & \textcolor{myred}{0.7} & 51.7 & \textcolor{myred}{0.4} & \textcolor{mygreen}{57.8} & \textcolor{myred}{0.5} & 41.1 & \textcolor{myred}{0.6} & \textcolor{mygreen}{17.5} & \textcolor{myred}{1.2} & 80.6\\
		&$p(k|o)$ & \textcolor{myred}{2.6} & \textcolor{myred}{16.8} & \textcolor{mygreen}{13.5} & 67.0 & \textcolor{myred}{2.7} & \textcolor{myred}{31.0} & \textcolor{mygreen}{15.4} & 50.7 & \textcolor{myred}{2.5} & \textcolor{myred}{12.3} & \textcolor{mygreen}{13.0} & 72.1\\
		&$p(k|u)$ & 0.6 & 3.6 & 1.7 & 94.1 & - & - & - & - & 0.6 & 3.5 & 1.7 & 94.1\\
		\hline
		\parbox[t]{2mm}{\multirow{4}{*}{\rotatebox[origin=c]{90}{\scriptsize{SoftNet}}}}&$p(k|d)$ & \textcolor{mygreen}{52.4} & \textcolor{myred}{21.8} & \textcolor{myred}{12.0} & 13.7 & \textcolor{mygreen}{54.6} & \textcolor{myred}{25.2} & \textcolor{myred}{12.7} & 7.6 & \textcolor{mygreen}{51.9} & \textcolor{myred}{20.6} & \textcolor{myred}{11.3} & 16.2\\
		&$p(k|f)$ & \textcolor{myred}{21.6} & \textcolor{mygreen}{64.1} & \textcolor{myred}{2.0} & 12.2 & \textcolor{myred}{19.5} & \textcolor{mygreen}{72.0} & \textcolor{myred}{1.6} & 6.9 & \textcolor{myred}{28.1} & \textcolor{mygreen}{42.6} & \textcolor{myred}{3.3} & 26.0\\
		&$p(k|o)$ & \textcolor{myred}{36.2} & \textcolor{myred}{12.9} & \textcolor{mygreen}{22.8} & 28.1 & \textcolor{myred}{43.2} & \textcolor{myred}{15.3} & \textcolor{mygreen}{25.8} & 15.6 & \textcolor{myred}{33.9} & \textcolor{myred}{12.2} & \textcolor{mygreen}{21.9} & 32.0\\
		&$p(k|u)$ & 19.2 & 9.9 & 4.6 & 66.3 & - & - & - & - & 19.1 & 9.8 & 4.6 & 66.5\\
		\hline
%		\multicolumn{13}{c}{}\\
%		\hline
		\parbox[t]{2mm}{\multirow{4}{*}{\rotatebox[origin=c]{90}{\scriptsize{DirNet}}}}&$p(k|d)$ & \textcolor{mygreen}{37.1} & \textcolor{myred}{22.7} & \textcolor{myred}{17.4} & 22.8 & \textcolor{mygreen}{39.6} & \textcolor{myred}{29.2} & \textcolor{myred}{18.3} & 12.8 & \textcolor{mygreen}{36.2} & \textcolor{myred}{20.2} & \textcolor{myred}{16.5} & 27.1\\
		&$p(k|f)$ & \textcolor{myred}{10.9} & \textcolor{mygreen}{61.5} & \textcolor{myred}{5.1} & 22.4 & \textcolor{myred}{11.3} & \textcolor{mygreen}{71.4} & \textcolor{myred}{3.9} & 13.5 & \textcolor{myred}{10.6} & \textcolor{mygreen}{35.4} & \textcolor{myred}{8.3} & 45.7\\
		&$p(k|o)$ & \textcolor{myred}{15.2} & \textcolor{myred}{10.5} & \textcolor{mygreen}{31.8} & 42.6 & \textcolor{myred}{25.0} & \textcolor{myred}{15.1} & \textcolor{mygreen}{35.2} & 24.6 & \textcolor{myred}{12.0} & \textcolor{myred}{9.1} & \textcolor{mygreen}{30.7} & 48.2\\
		&$p(k|u)$ & 3.0 & 5.4 & 12.7 & 78.9 & - & - & - & - & 3.0 & 5.4 & 12.6 & 79.0\\
		\hline
%		\multicolumn{13}{c}{}\\
%		\hline
		\parbox[t]{2mm}{\multirow{4}{*}{\rotatebox[origin=c]{90}{\scriptsize{ShiftNet}}}}&$p(k|d)$ & \textcolor{mygreen}{38.5} & \textcolor{myred}{18.1} & \textcolor{myred}{12.7} & 30.7 & \textcolor{mygreen}{35.3} & \textcolor{myred}{22.9} & \textcolor{myred}{13.7} & 28.1 & \textcolor{mygreen}{40.9} & \textcolor{myred}{15.9} & \textcolor{myred}{12.0} & 31.2\\
		&$p(k|f)$ & \textcolor{myred}{4.7} & \textcolor{mygreen}{62.9} & \textcolor{myred}{3.2} & 29.2 & \textcolor{myred}{4.3} & \textcolor{mygreen}{71.1} & \textcolor{myred}{2.4} & 22.1 & \textcolor{myred}{6.1} & \textcolor{mygreen}{40.8} & \textcolor{myred}{5.3} & 47.8\\
		&$p(k|o)$ & \textcolor{myred}{6.0} & \textcolor{myred}{13.6} & \textcolor{mygreen}{28.5} & 51.8 & \textcolor{myred}{7.4} & \textcolor{myred}{17.6} & \textcolor{mygreen}{30.9} & 44.1 & \textcolor{myred}{5.7} & \textcolor{myred}{12.4} & \textcolor{mygreen}{27.8} & 54.1\\
		&$p(k|u)$ & 3.0 & 9.0 & 7.8 & 80.2 & - & - & - & - & 3.0 & 8.9 &7.8 & 80.3\\
		\hline
		\multicolumn{2}{c|}{\textbf{R$_{20}$ Scores}} & \multicolumn{4}{c|}{overall} & \multicolumn{4}{c|}{visible} & \multicolumn{4}{c}{occluded}
	\end{tabular}
	\caption{\label{tab:deep_ism_r20_results}Results of deep \gls{ism} variants trained on R$_1$ and R$_{20}$ images for visible, occluded and overall areas. Here, the visible area is defined as the area which is not unknown in the geometric \gls{ilm}. Thus, only a stochastically insignificant portion of unknown labels remains in the visible area, which is why these scores are excluded from consideration. The scores are given in the form of a confusion matrix for each model where each row shows the probabilities in percentage of estimates given the true class. Values in green correspond to true positives, red shows the percentages of false predictions. The unknown class predictions (black) are treated as the safe state and, as such, do not add to the false rates. Additionally, since the network is permitted to extrapolate the state in unknown areas, all predictions deviating from the unknown class given unknown labels should also not be treated as false.}
\end{figure} 
\begin{figure}[H]
	\begin{center}
	\resizebox{\textwidth}{!}{
		\import{imgs/07_deep_ism_exp/aleat_uncert}{irm_qual_comp.pdf_tex}}
		\caption{\label{fig:irm_qual_comp}Qualitative results of the different compared \gls{irm}s (column-wise) for three validation scenes showing the predictions based on R$_1$ and R$_{20}$ in the respective rows with the corresponding label. Scene A illustrates the models capabilities to predict moving objects of any shape (e.g. two cars, one bus and a cyclist). Additionally, the lower box encloses an example of small distinct objects being situated close to each other. Scene B shows an effect of a bias in the radar data (marked in the box). Most of the time, moving objects only occlude parts of the scene temporarily leading to detections of the occluded areas at other times (e.g. upper box in scene A). Thus, if a wall is erroneously detected as moving and no further detections are provided behind it, the models predicts the area behind the wall as free. Finally, scene C illustrates the capacity of the methods to predict the occupancy state in a general parking space, which is one of the main application scenarios of the proposed system.}
	\end{center}
\end{figure}
%======================================%
%
%======================================%
\subsection{Discussion}
\label{subsec:discussion_aleat_uncert}
The experiments have shown that the current state-of-the-art deep \gls{ism} (SoftNet) indeed models occurring uncertainty as conflicting mass leading to a huge bias in the dynamic class (see Fig. \ref{tab:deep_ism_r20_results}), proving H\ref{hyp:sota_not_model_unc}. The two compared methods to shift the uncertainty to the unknown class both clearly reduce the amount of bias in the dynamic class. Additionally, the amount of shifted mass to the unknown class clearly correlates with the amount of certainty in the data which can be seen by comparing the unknown mass assigned in visible and occluded areas and also the overall assigned unknown mass between $R_1$ and $R_{20}$ inputs, providing an answer to RQ\ref{requ:how_to_sep_uncertainty}. 
\\\\
Since ShiftNet provides the least false rates per class over all classes for the $R_{20}$ compared to all models while providing similar true positive rates compared to DirNet it largely suffices R\ref{subreq:unknown_mass} and \ref{subreq:conflicting_mass}. Thus, ShiftNet will be used as the baseline method for further experiments and analysis.
\\\\
Regarding the \gls{ism}s capability to estimate dynamic objects, the qualitative results in Fig. \ref{fig:irm_qual_comp} Scene A shall be analyzed. These show that the $R_1$ inputs contain sometimes only a single detection belonging to a dynamic object. This leads the networks to predict a dynamic object prior with size and shape of a car oriented in the direction of the road (compare lower right dynamic object in upper marked box in input and ShiftNet prediction of Scene A in Fig. \ref{fig:irm_qual_comp}). For $R_{20}$, all of the dynamic object's detections over time are available leading to an overall improvement in dynamic class true rates. However, the false occupied predictions in the dynamic class are also consistently increased and in case of ShiftNet almost doubled. This might hint that the networks can decide that there is an object but are uncertain about its motion state. This problem might be due to the choice of encoding the dynamic detection or due to errors in the dynamic flags of the radar detections. Additionally, the approach to decay the dynamic detections over time to encode the motion direction seems to work for some vehicles (see ShiftNet prediction and label of car in lower left for Scene A) while other objects are elongated (see ShiftNet prediction bicycle in the lower right of the upper marked box in Scene A). Thus, further improvement needs to be done in the future to find better ways of encoding the dynamic detections over time.
\\\\
The above analysis and experimental results provide a partial answer to RQ\ref{requ:comparison_of_isms}, by analyzing only the deep \gls{ism}'s capability to estimate evidential masses for radar inputs. The remaining analysis is provided in the following Section \ref{sec:analysis_dyn_encoding}. Moreover, two of the three possible radar encodings have been analyzed providing a partial answer to RQ\ref{requ:radar_dyn_encoding}. The analysis of the remaining variant is provided in Section \ref{sec:analysis_dyn_encoding} to provide the full answer. Regarding RQ\ref{requ:how_to_meas_info}, it can be seen that the unknown mass, as proposed in Section \ref{subsec:method_to_use_deep_isms_in_occmaps} indeed correlates with the information content in the ShiftNet inputs. To further verify the validity to use the unknown estimated by the ShiftNet, the correlation is additionally analyzed for camera, lidar and the fused inputs in Section \ref{subsec:discussion_deep_icm_ilm}.
%==========================================================================%
%
%==========================================================================%
\section{Analysis Dynamic Detection Encoding for R$_{20}$ ShiftNet Inputs}
\label{sec:analysis_dyn_encoding}
%======================================%
%
%======================================%
\subsection{Experimental Setup}
In this section, the investigation of the effect of radar input encodings on the deep \gls{ism} performance is finalized. The first two variants, namely the radar \gls{bev} projection of a single timestep $R_1$ and the temporally accumulated radar projection with decay on the dynamic detections' intensity $R_{20}$ are already discussed in Section \ref{sec:al_uncert_in_deep_isms}. Thus, this section focuses on encoding temporally accumulated radar detections while only marking the latest dynamic detections, abbreviated as $R_{20|1}$. The three variants are explained in more detail in Section \ref{subsec:def_of_targets_n_inputs}. For the analysis, the ShiftNet deep \gls{ism} is trained until convergence as marked by the validation set defined in NuScenes. The qualitative and quantitative results are purely based on the test set.
%======================================%
%
%======================================%
\subsection{Experimental Results}
First, the scores shown in Fig. \ref{tab:deep_ism_r20_results} for the inputs $R_1$ and $R_{20}$ are compared with the scores for $R_{20|1}$, shown in Fig. \ref{tab:deep_ism_r20_1_results}. Here, deep \gls{ism}s trained on $R_{20}$ show an overall increased performance for the dynamic class over $R_{20|1}$. This shows that the encoding of dynamic detections with decay is beneficial over only including the latest dynamic detection for moving object prediction. Qualitatively, the improvement of the estimated dynamic object shapes is clearly visible in the white box in scene A and the left white box in scene B of Fig. \ref{fig:qual_comp_radar_encoding}. More specifically, it is shown that the deep \gls{ism} is capable of producing reasonable shape, position and orientation predictions of the dynamic objects based on a single detection point and the scene layout for $R_1$ and $R_{20|1}$ compared to the cloud of dynamic detections for $R_{20}$. Therefore, the improvement for dynamic objects is to be expected. Nevertheless, accumulation of dynamic detections in $R_{20}$ also leads to failure cases. Here, the right white box in scene B of Fig. \ref{fig:qual_comp_radar_encoding} shows a predicted dynamic object for $R_{20}$ which is already outside the \gls{ism}'s \gls{fov}. This is caused by the trailing dynamic predictions which does not occur for the other deep \gls{irm} variants.
\begin{figure}[H]
\begin{center}
	\resizebox{\textwidth}{!}{
	\begin{tabular}{c|c|cccc|cccc|cccc}
	&$k$ & $d$ & $f$ & $o$ & $u$ & $d$ & $f$ & $o$ & $u$ & $d$ & $f$ & $o$ & $u$\\
	\hline
	\parbox[t]{2mm}{\multirow{4}{*}{\rotatebox[origin=c]{90}{ShiftNet}}}&$p(k|d)$ & \textcolor{mygreen}{29.2} & \textcolor{myred}{21.7} & \textcolor{myred}{14.4} & 34.6 & \textcolor{mygreen}{27.8} & \textcolor{myred}{26.3} & \textcolor{myred}{15.4} & 30.6 & \textcolor{mygreen}{31.0} & \textcolor{myred}{20.3} & \textcolor{myred}{13.2} & 35.5\\
	&$p(k|f)$ & \textcolor{myred}{4.7} & \textcolor{mygreen}{63.0} & \textcolor{myred}{3.3} & 29.0 & \textcolor{myred}{4.7} & \textcolor{mygreen}{71.9} & \textcolor{myred}{2.5} & 20.9 & \textcolor{myred}{5.0} & \textcolor{mygreen}{38.8} & \textcolor{myred}{5.7} & 50.5\\
	&$p(k|o)$ & \textcolor{myred}{4.5} & \textcolor{myred}{14.1} & \textcolor{mygreen}{30.0} & 51.4 & \textcolor{myred}{5.7} & \textcolor{myred}{18.4} & \textcolor{mygreen}{32.4} & 43.5 & \textcolor{myred}{4.2} & \textcolor{myred}{12.8} & \textcolor{mygreen}{29.3} & 53.6\\
	&$p(k|u)$ & 2.1 & 8.2 & 7.7 & 82.0 & - & - & - & - & 2.1 & 8.1 & 7.7 & 82.1\\
	\hline
	\multicolumn{2}{c|}{$R_{20|1}$ Scores}& \multicolumn{4}{c|}{overall} & \multicolumn{4}{c|}{visible} & \multicolumn{4}{c}{occluded}
\end{tabular}}
\caption{\label{tab:deep_ism_r20_1_results}Normed confusion matrix evaluated on ShiftNet model which was trained on $R_{20|1}$ inputs. For a detailed explanation of the table format kindly refer to Fig. \ref{tab:deep_ism_r20_results}.}
\end{center}
\end{figure}
On the other hand, the overall occupied class performance is slightly but consistently improved by only taking the latest dynamic detection for $R_{20|1}$. One potential cause can be seen in the lower white box in scene B of Fig \ref{fig:qual_comp_radar_encoding}. Here, static detections are missing in $R_{20}$ which are present in $R_{20|1}$. This is caused by some detections almost outside of the time horizon that have been falsely identified as dynamic and are being overlayed over the static predictions. Hence, the accumulation of dynamic detections can cause outliers to deteriorate static detections. Eventually, the free scores are mainly the same, showing that the changes only affect the occupied and dynamic class.
\begin{figure}[H]
	\begin{center}
		\resizebox{\textwidth}{!}{
		\import{imgs/07_deep_ism_exp/radar_dyn_encoding}{qual_comp_radar_encoding.pdf_tex}}
		\caption{\label{fig:qual_comp_radar_encoding}Qualitative comparison of ShiftNet predictions (middle) trained on radar inputs with three different dynamic detection encodings (bottom) shown for two scenes with the respective lidar occupancy map patch as ground-truth (top).}
	\end{center}
\end{figure}
Finally, comparing $R_{20|1}$ and $R_{20}$ with $R_1$ it can be seen that the problem of decrease in dynamic prediction performance, indicated by the quantitative results while the qualitative results seem to be sharper, remains.
%======================================%
%
%======================================%
\subsection{Discussion}
The above results show that the radar encoding $R_{20|1}$ significantly decreases the capability to estimate dynamic objects, slightly increases the occupied area predictions while maintaining about constant for free areas. Here, the decrease for the dynamic class is to be expected, since in some cases only a single detection point is assigned to a dynamic object. Therefore, the $R_{20|1}$, similar to the $R_1$, based deep \gls{ism}s need to rely on scene understanding and prior knowledge from the dataset to estimate the dynamic object shapes. 
\\\\
Considering the occupied class, the qualitative results indicated one of the causes to be the increased influence of detections being falsely marked as dynamic. Through the temporal accumulation in $R_{20}$, the outliers influence can persist over time and cause static boundaries to deteriorate through the temporal decay. Also, the quantitative results both for $R_{20|1}$ and $R_{20}$ show decreased quantitative performance over the $R_1$ inputs while the qualitative results seem to get sharper. This might be caused by overall more occupied mass being estimated that might leak into dynamic predictions. Here, deep \gls{ism} based on $R_{20|1}$ and $R_{20}$ increase their occupied true rates by more than double the amount that is predicted for $R_1$ inputs.
\\\\
Overall, the quantitative scores for $R_{20}$ exceed the ones for $R_{20|1}$ and $R_1$ or are in a similar range. Also, the qualitative results show the arguably best edge accuracy for occupied and shape estimation for dynamic objects. Thus, the $R_{20}$ encoding will be used for further comparisons and experiments, concluding the answer to RQ\ref{requ:radar_dyn_encoding}.
%==========================================================================%
%
%==========================================================================%
\section{Analysis of Camera, Lidar and Fused Inputs for deep ISMs}
\label{sec:cam_lidar_fusion_in_deep_isms}
%======================================%
%
%======================================%
\subsection{Experimental Setup}
\label{subsec:setup_cam_lidar_fusion_in_deep_isms}
To analyze the performance and occurring effects using a deep \gls{icm} and \gls{ilm}, the homography projection into \gls{bev} of all camera images, their semantics, the \gls{monodepth} and the lidar \gls{bev} projection are considered as inputs. Details of the chosen inputs are described in Section \ref{subsec:def_of_targets_n_inputs}. For reasons discussed in Section \ref{subsec:discussion_aleat_uncert}, the ShiftNet configuration is chosen for the experiments. Additionally, the fusion of camera and lidar respectively with radar inputs shall be investigated. Here, only the monocular depth projection is used on the camera side for the experiments since it is shown in Section \ref{subsec:exp_cam_fusion_in_deep_isms} to provide the best performance among the investigated camera input encodings. Since the \gls{monodepth}, lidar and radar signals are provided in the same projection, the fusion can be performed by concatenating the inputs channel-wise to form a new one.
%======================================%
%
%======================================%
\subsection{Experimental Results Camera and Camera-Radar Inputs}
\label{subsec:exp_cam_fusion_in_deep_isms}
Based on the false rates, the monocular depth input model slightly outperforms the other purely camera-based counterparts with an accumulated overall false rate of 52.7 as compared to 54.1, both for the RGB and semantic projections. Looking at the true rates, this distinction becomes even clearer with the RGB-based model providing the worst true rates in all categories (true rate sum of 103.1), followed by the semantic model (true rate sum 111.7) and, finally, with the \gls{monodepth} model as the best purely camera-based model (true rate sum 126.1). This overall better performance of the \gls{monodepth} \gls{ism} is also reflected in the overall decreased unknown mass compared to the other purely camera-based \gls{ism}s. The improved performance of the \gls{monodepth} model is also reflected in the qualitative results in Fig. \ref{fig:icm_qual_comp} by the more highlighted occupied space and, more importantly, the increased correctness of free and occupied space contours compared to the remaining purely camera-based models.
\begin{figure}[H]
\begin{center}
	\resizebox{\textwidth}{!}{
\begin{tabular}{c|c|cccc|cccc|cccc}
	&$k$ & $d$ & $f$ & $o$ & $u$ & $d$ & $f$ & $o$ & $u$ & $d$ & $f$ & $o$ & $u$\\
	\hline
	\parbox[t]{2mm}{\multirow{4}{*}{
\rotatebox[origin=c]{90}{\scriptsize{Homog. RGB}}}}&$p(k|d)$ & \textcolor{mygreen}{33.2} & \textcolor{myred}{23.0} & \textcolor{myred}{5.3} & 38.6 & \textcolor{mygreen}{28.3} & \textcolor{myred}{33.0} & \textcolor{myred}{5.6} & 33.0 & \textcolor{mygreen}{36.3} & \textcolor{myred}{18.9} & \textcolor{myred}{4.6} & 40.2\\
	&$p(k|f)$ & \textcolor{myred}{4.0} & \textcolor{mygreen}{58.9} & \textcolor{myred}{3.1} & 33.9 & \textcolor{myred}{3.1} & \textcolor{mygreen}{70.1} & \textcolor{myred}{2.7} & 24.1 & \textcolor{myred}{6.6} & \textcolor{mygreen}{29.8} & \textcolor{myred}{4.3} & 59.4\\
	&$p(k|o)$ & \textcolor{myred}{4.1} & \textcolor{myred}{14.6} & \textcolor{mygreen}{11.0} & 70.3 & \textcolor{myred}{4.3} & \textcolor{myred}{22.6} & \textcolor{mygreen}{12.2} & 60.8 & \textcolor{myred}{4.1} & \textcolor{myred}{12.2} & \textcolor{mygreen}{10.6} & 73.1\\
	&$p(k|u)$ & 3.2 & 4.7 & 5.0 & 87.2 & - & - & - & - & 3.2 & 4.6 & 4.9 & 87.3\\
	\hline
	\parbox[t]{2mm}{\multirow{4}{*}{
\rotatebox[origin=c]{90}{\scriptsize{Homog. SemSeg}}}}&$p(k|d)$ & \textcolor{mygreen}{38.1} & \textcolor{myred}{22.3} & \textcolor{myred}{4.6} & 35.0 & \textcolor{mygreen}{34.6} & \textcolor{myred}{30.2} & \textcolor{myred}{4.8} & 30.5 & \textcolor{mygreen}{40.9} & \textcolor{myred}{18.6} & \textcolor{myred}{4.2} & 36.2\\
	&$p(k|f)$ & \textcolor{myred}{4.0} & \textcolor{mygreen}{62.4} & \textcolor{myred}{2.5} & 31.2 & \textcolor{myred}{3.2} & \textcolor{mygreen}{73.9} & \textcolor{myred}{1.8} & 21.1 & \textcolor{myred}{6.3} & \textcolor{mygreen}{32.2} & \textcolor{myred}{4.2} & 57.3\\
	&$p(k|o)$ & \textcolor{myred}{4.8} & \textcolor{myred}{15.9} & \textcolor{mygreen}{11.2} & 68.1 & \textcolor{myred}{5.7} & \textcolor{myred}{23.7} & \textcolor{mygreen}{11.6} & 58.9 & \textcolor{myred}{4.6} & \textcolor{myred}{13.6} & \textcolor{mygreen}{11.0} & 70.8\\
	&$p(k|u)$ & 2.6 & 6.5 & 5.4 & 85.5 & - & - & - & - & 2.6 & 6.4 & 5.4 & 85.6\\
	\hline
	\parbox[t]{2mm}{\multirow{4}{*}{
\rotatebox[origin=c]{90}{\scriptsize{\gls{monodepth}}}}}&$p(k|d)$ & \textcolor{mygreen}{40.8} & \textcolor{myred}{16.8} & \textcolor{myred}{7.0} & 35.3 & \textcolor{mygreen}{36.1} & \textcolor{myred}{25.5} & \textcolor{myred}{6.6} & 31.7 & \textcolor{mygreen}{44.3} & \textcolor{myred}{12.0} & \textcolor{myred}{7.1} & 36.6\\
	&$p(k|f)$ & \textcolor{myred}{3.4} & \textcolor{mygreen}{69.3} & \textcolor{myred}{2.4} & 25.0 & \textcolor{myred}{2.3} & \textcolor{mygreen}{81.4} & \textcolor{myred}{1.5} & 14.9 & \textcolor{myred}{6.4} & \textcolor{mygreen}{38.3} & \textcolor{myred}{4.7} & 50.5\\
	&$p(k|o)$ & \textcolor{myred}{6.9} & \textcolor{myred}{16.2} & \textcolor{mygreen}{16.0} & 60.9 & \textcolor{myred}{8.2} & \textcolor{myred}{25.2} & \textcolor{mygreen}{15.5} & 51.2 & \textcolor{myred}{6.6} & \textcolor{myred}{13.5} & \textcolor{mygreen}{16.1} & 63.7\\
	&$p(k|u)$ & 3.3 & 6.5 & 8.0 & 82.1 & - & - & - & - & 3.3 & 6.4 & 8.0 & 82.2\\
	\hline
	\parbox[t]{2mm}{\multirow{4}{*}{\rotatebox[origin=c]{90}{\scriptsize{\gls{monodepth} \& R$_{20}$}}}}&$p(k|d)$ & \textcolor{mygreen}{42.4} & \textcolor{myred}{14.3} & \textcolor{myred}{12.6} & 30.7 & \textcolor{mygreen}{38.0} & \textcolor{myred}{20.6} & \textcolor{myred}{13.0} & 28.5 & \textcolor{mygreen}{45.7} & \textcolor{myred}{11.3} & \textcolor{myred}{12.3} & 30.7\\
	&$p(k|f)$ & \textcolor{myred}{2.9} & \textcolor{mygreen}{69.6} & \textcolor{myred}{2.5} & 25.0 & \textcolor{myred}{2.2} & \textcolor{mygreen}{80.0} & \textcolor{myred}{1.6} & 16.1 & \textcolor{myred}{4.9} & \textcolor{mygreen}{42.3} & \textcolor{myred}{4.9} & 47.9\\
	&$p(k|o)$ & \textcolor{myred}{5.0} & \textcolor{myred}{12.0} & \textcolor{mygreen}{28.8} & 54.1 & \textcolor{myred}{6.4} & \textcolor{myred}{17.2} & \textcolor{mygreen}{30.0} & 46.4 & \textcolor{myred}{4.7} & \textcolor{myred}{10.5} & \textcolor{mygreen}{28.4} & 56.4\\
	&$p(k|u)$ & 2.0 & 8.1 & 8.0 & 81.9 & - & - & - & - & 1.9 & 8.0 & 8.0 & 82.1\\
	\hline
	\multicolumn{2}{c|}{\textbf{ShiftNet}} & \multicolumn{4}{c|}{overall} & \multicolumn{4}{c|}{visible} & \multicolumn{4}{c}{occluded}
\end{tabular}}
\caption{\label{tab:deep_icm_results}Normed confusion matrix evaluated on the ShiftNet model which was trained on the homographic \gls{bev} projected $360^{\circ}$ camera images, their semantics, the \gls{monodepth}-based projection and, finally, the fusion of the \gls{monodepth} and radar \gls{bev} images. For a detailed explanation of the table format kindly refer to Fig. \ref{tab:deep_ism_r20_results}.}
\end{center}
\end{figure}
More specifically, the only prominent deficit of the \gls{monodepth} \gls{ism} lies in its increased false rates for $p(d|o)$ and $p(o|d)$. This shows that, compared to the other purely camera-based models, the \gls{monodepth} \gls{ism} is better in estimating whether there is an object present but not in which state (static or dynamic) it is. On the other hand, both homography-based \gls{ism}s struggle to distinguish free from dynamic space. But, the incorporation of semantic information leads to a jump in the true positive rate for dynamic objects. This is to be expected since the semantics provide class information which is directly linked to the possibility of a pixel being dynamic.
\\\\ 
Since the \gls{monodepth} \gls{ism} shows an on average increased performance over all classes, it will be used to further analyze the effect of fusion with radar. These experiments show that, compared with the \gls{monodepth} \gls{ism}, the additional radar information leads to less unknown mass and an overall improvement of positive and false rates throughout all classes. While this improvement is only marginal for the free class, the occupied true positive rate is increased by $12.8\%$ while also reducing the false rates. The only worsening is observed with regards to false occupied predictions in the dynamic class. Here, the false rate is almost doubled compared to the other camera \gls{ism}s. Comparing to the purely radar-based \gls{ism}, the \gls{monodepth}-radar fusion provides better performance in each of the overall scores without exception.
\\\\
A qualitative example for the improvement of the fused over the pure radar \gls{ism} can be seen in the white box in Scene B of Fig. \ref{fig:icm_qual_comp}. Here, the \gls{monodepth} provides the additional information that the wall proceeds around the corner, leading the model to assign the area behind the wall as unknown rather than free. Also, the shape of occupied space in the fused \gls{ism} is highly improved over the predictions of the purely camera-based \gls{ism}s.
\begin{figure}[H]
\begin{center}
	\resizebox{\textwidth}{!}{
	\import{imgs/07_deep_ism_exp/aleat_uncert}{icm_qual_comp.pdf_tex}}
	\caption{\label{fig:icm_qual_comp}Qualitative comparison of ShiftNets trained on different inputs shown for three scenes with the respective lidar occupancy map patch as ground-truth to the right. The two channeled fused camera \& R$_{20}$ input is, for visualization purposes, split into three channels, one for camera, static radar and dynamic radar detections. These channels are colored with RGB leading to a mixture of colors wherever they coincide.}
\end{center}
\end{figure}
%======================================%
%
%======================================%
\subsection{Experimental Results Lidar and Lidar-Radar Inputs}
\label{subsec:exp_lidar_fusion_in_deep_isms}
When comparing the scores of the deep lidar \gls{ism}s in Fig. \ref{tab:ilm_quant_comp} with the ones solely based on camera and radar (see Fig. \ref{tab:deep_icm_results} and \ref{tab:deep_ism_r20_results}) the lidar based deep \gls{ism} obtains the overall best results. The only exception is with regards to dynamic objects. Here, it obtain the highest false rates of dynamic predictions in the occupied class. Also, the unknown mass is reduced for each category, further illustrating ShiftNet's capability to adapt the unknown mass to account for the higher accuracy in the lidar data. For the geo \gls{ilm}, the importance of treating the visible and occluded metrics separately becomes more evident than for all other \gls{ism}s. Here, the occluded area, as per definition, is mainly assigned to the unknown class. Hence, only the visible area will be considered for the following discussion.
\\\\
In the visible area, it can be seen in Fig. \ref{tab:ilm_quant_comp} that there are still predictions for dynamic objects remaining. This is due to the fact that not all occupied pixels belonging to the dynamic objects get removed since the bounding box ground-truth in NuScenes often does not fully enclose the objects. The remaining occupied pixels get mixed with the free space rays leading to half occupied half free, and hence, dynamic pixels. This effect can also be seen in Fig. \ref{fig:ilm_qual_comp} in the upper white box in scene A where some contour points of the bus remain as dynamic detections. Regarding the free predictions in the visible area, they provide an almost perfect overlap with the resulting occupancy maps. Thus, the mapping doesn't change the free space much in the visible area. However, looking at the occupied class, the false free rates are among the highest of all models. This is due to the \gls{ilm}'s parameterization allowing free space rays to cut through occupied areas. Here, the \gls{ilm} is parameterized to correct these errors during mapping resulting in good maps but sub-optimal \gls{ilm} performance. This effect can be seen qualitatively in Fig. \ref{fig:ilm_qual_comp} looking at the perforated contours of occupied areas.
\\\\
When comparing the deep with the geo \gls{ilm} scores in Fig. \ref{tab:ilm_quant_comp}, it can be seen that the deep variant outperforms the geometric one in the dynamic and occupied class by a large margin. This can be verified in the qualitative results by looking at the continuous occupancy boundaries and the well shaped dynamic vehicle prediction in scene A. However, large portions of dynamic objects are estimated as being static. This is to be expected since the deep \gls{ilm} can only guess the dynamic state based on the position of the vehicles on the road for it is not measured by the lidar sensor. An example of dynamic objects being falsely predicted as static is shown in Fig. \ref{fig:ilm_qual_comp} scene A. On the other hand, the deep \gls{ilm} does not reach the same amount of correct free space mass. This might be due to the more continuous probabilistic prediction of free space as opposed to the binary one in the geo \gls{ilm}, which is e.g. shown in the white box of scene B in Fig. \ref{fig:ilm_qual_comp} close to the image boundary. Additionally, estimates for the occluded area are provided which, on average, are correct. Thus, the deep \gls{ilm} provides a valid alternative over the geometric counterpart.
\begin{figure}[H]
\begin{center}
	\resizebox{\textwidth}{!}{
	\begin{tabular}{c|c|cccc|cccc|cccc}
		&$k$ & $d$ & $f$ & $o$ & $u$ & $d$ & $f$ & $o$ & $u$ & $d$ & $f$ & $o$ & $u$\\
		\hline
		\parbox[t]{2mm}{\multirow{4}{*}{\rotatebox[origin=c]{90}{\scriptsize{geo ILM}}}}&$p(k|d)$ & \textcolor{mygreen}{0.2} & \textcolor{myred}{27.6} & \textcolor{myred}{1.7} & 70.3 & \textcolor{mygreen}{1.8} & \textcolor{myred}{73.2} & \textcolor{myred}{3.6} & 21.0 & \textcolor{mygreen}{0.0} & \textcolor{myred}{0.0} & \textcolor{myred}{0.0} & 100.0\\
		&$p(k|f)$ & \textcolor{myred}{0.0} & \textcolor{mygreen}{72.0} & \textcolor{myred}{0.2} & 27.5 & \textcolor{myred}{0.0} & \textcolor{mygreen}{98.7} & \textcolor{myred}{0.3} & 0.6 & \textcolor{myred}{0.0} & \textcolor{mygreen}{0.0} & \textcolor{myred}{0.0} & 100.0\\
		&$p(k|o)$ & \textcolor{myred}{0.1} & \textcolor{myred}{15.4} & \textcolor{mygreen}{7.7} & 76.7 & \textcolor{myred}{0.3} & \textcolor{myred}{60.6} & \textcolor{mygreen}{32.6} & 6.1 & \textcolor{myred}{0.0} & \textcolor{myred}{0.0} & \textcolor{mygreen}{0.0} & 100.0\\
		&$p(k|u)$ & 0.0 & 0.7 & 0.1 & 99.2 & - & - & - & - & 0.0 & 0.0 & 0.0 & 100.0\\
		\hline
		%	\multicolumn{13}{c}{}\\	
		%	\hline
		\parbox[t]{2mm}{\multirow{4}{*}{\rotatebox[origin=c]{90}{\scriptsize{deep ILM}}}}&$p(k|d)$ & \textcolor{mygreen}{47.3} & \textcolor{myred}{10.5} & \textcolor{myred}{16.3} & 25.8 & \textcolor{mygreen}{42.0} & \textcolor{myred}{15.1} & \textcolor{myred}{17.9} & 25.1 & \textcolor{mygreen}{51.1} & \textcolor{myred}{7.5} & \textcolor{myred}{16.4} & 25.0\\
		&$p(k|f)$ & \textcolor{myred}{2.4} & \textcolor{mygreen}{78.8} & \textcolor{myred}{1.9} & 16.9 & \textcolor{myred}{1.6} & \textcolor{mygreen}{89.3} & \textcolor{myred}{0.9} & 8.1 & \textcolor{myred}{4.5} & \textcolor{mygreen}{51.1} & \textcolor{myred}{4.6} & 39.8\\
		&$p(k|o)$ & \textcolor{myred}{8.7} & \textcolor{myred}{8.4} & \textcolor{mygreen}{43.6} & 39.3 & \textcolor{myred}{11.0} & \textcolor{myred}{10.8} & \textcolor{mygreen}{46.5} & 31.7 & \textcolor{myred}{8.1} & \textcolor{myred}{7.5} & \textcolor{mygreen}{42.9} & 41.6\\
		&$p(k|u)$ & 3.0 & 8.2 & 8.7 & 80.1 & - & - & - & - & 3.0 & 8.0 & 8.7 & 80.3\\
		\hline
		\parbox[t]{2mm}{\multirow{4}{*}{\rotatebox[origin=c]{90}{\scriptsize{deep ILRM}}}}&$p(k|d)$ & \textcolor{mygreen}{47.3} & \textcolor{myred}{9.7} & \textcolor{myred}{18.3} & 24.8 & \textcolor{mygreen}{43.5} & \textcolor{myred}{13.6} & \textcolor{myred}{20.6} & 22.2 & \textcolor{mygreen}{50.6} & \textcolor{myred}{7.4} & \textcolor{myred}{17.4} & 24.6\\
		&$p(k|f)$ & \textcolor{myred}{1.8} & \textcolor{mygreen}{80.8} & \textcolor{myred}{1.9} & 15.5 & \textcolor{myred}{1.2} & \textcolor{mygreen}{89.8} & \textcolor{myred}{1.1} & 7.9 & \textcolor{myred}{3.6} & \textcolor{mygreen}{56.8} & \textcolor{myred}{4.1} & 35.4\\
		&$p(k|o)$ & \textcolor{myred}{6.0} & \textcolor{myred}{7.6} & \textcolor{mygreen}{46.8} & 39.5 & \textcolor{myred}{8.1} & \textcolor{myred}{9.6} & \textcolor{mygreen}{52.8} & 29.4 & \textcolor{myred}{5.5} & \textcolor{myred}{6.8} & \textcolor{mygreen}{45.2} & 42.5\\
		&$p(k|u)$ & 1.6 & 7.6 & 7.4 & 83.3 & - & - & - & - & 1.6 & 7.5 & 7.3 & 83.5\\
		\hline
		\multicolumn{2}{c|}{\textbf{ShiftNet}} & \multicolumn{4}{c|}{overall} & \multicolumn{4}{c|}{visible} & \multicolumn{4}{c}{occluded}
	\end{tabular}}
	\caption{\label{tab:ilm_quant_comp}Normed confusion matrix evaluated on the geo \gls{ilm} and the ShiftNet model, trained on the lidar \gls{bev} projection and the fusion of the lidar and radar \gls{bev} images. For a detailed explanation of the table format kindly refer to Fig. \ref{tab:deep_ism_r20_results}.}
\end{center}
\end{figure}
Finally, when looking at the fused lidar-radar \gls{ism}, the overall scores in Fig. \ref{tab:ilm_quant_comp} among all classes are improved as expected. For the occupied and free class, the fused \gls{ism} even manages to improve the true rate while at the same time decrease the false rates. These improvements can also be qualitatively verified looking at the white box in Fig. \ref{fig:ilm_qual_comp} scene B. Here, the area behind the wall both for the deep \gls{ilm} and \gls{irm} is partially or, in case of the radar, fully assigned as free. This error is corrected using the fused input.
\\\\
For the dynamic class, the false and true rates remain about the same which shows the lack of distinguishability for dynamic objects provided by the radar encoding. This can again be qualitatively verified looking at the upper white box in scene A. Here, the deep \gls{irm} formerly fully recognized bus as being dynamic while the deep \gls{ilm} interpreted it as occupied. In the fused result, the bus is only partially predicted as dynamic which, thus, worsened the prediction. However, the static car in the deep \gls{ilm} is assigned dynamic in the fusion and the dynamic cyclist's contour becomes more accurate in the fusion.
\begin{figure}[H]
	\begin{center}
		\resizebox{\textwidth}{!}{
		\import{imgs/07_deep_ism_exp/aleat_uncert}{ilm_qual_comp.pdf_tex}}
		\caption{\label{fig:ilm_qual_comp}Qualitative comparison of ShiftNets trained on different inputs shown for three scenes with the respective lidar occupancy map patch as ground-truth to the right. The two channeled fused lidar \& R$_{20}$ input is, for visualization purposes, split into three channels, one for lidar, static radar and dynamic radar detections. These channels are colored with RGB leading to a mixture of colors wherever they coincide.}
	\end{center}
\end{figure}
%======================================%
%
%======================================%
\subsection{Discussion}
\label{subsec:discussion_deep_icm_ilm}
Based on the observations in Section \ref{subsec:exp_cam_fusion_in_deep_isms}, first, the \gls{ism}s using the homography-based inputs shall be discussed. Here, directly projecting the camera images into \gls{bev} resulted in the worst performance. This might be another indicator to proof the flaws in using homography over \gls{monodepth} or the other deep learning based methods mentioned in the state-of-the-art review in Section \ref{subsec:deep_ism_camera} to obtain environment perception from camera images in the \gls{bev} projection. However, it needs to be mentioned that the decreased performance might be due to the fact that the network's capacity has been adjusted in Section \ref{sec:choice_of_unet_arch} for the R$_1$ input. Thus, it might be possible to eliminate some of the deficits using a bigger model and more data.
\\\\
When comparing the homography-based \gls{ism} with and without semantic information, it can be seen that the semantic information leads to an overall better performance. This, again, might be due to the fact that the network capacity is not big enough to fully learn the mapping based on the camera image input. The biggest performance gap, however, is between the true positive rates of dynamic predictions which shows that including semantic information helps in identifying dynamic objects. Another prominent observation is the increased false rate of $p(f|d)$ showing the confusion of homography-based \gls{ism}s to distinguish free from dynamic space. A possible explanation for this characteristic might be the shape distortion of non-flat objects like cars during the homography projection leading to free space in areas where the camera captures space beneath the car. A qualitative example showing the distortion is depicted in the upper left white box in Scene A of Fig. \ref{fig:icm_qual_comp}. Eventually, it needs to be mentioned that, up to a certain degree, the decreased performance of the semantic homography \gls{ism} can be explained by the errors of the used semantic segmentation model. Since the model has not been retrained, its performance is limited for some classes like the only partially correct classified bus depicted in the upper right white box in Scene A of Fig. \ref{fig:icm_qual_comp}.
\\\\
For the case of the deep \gls{monodepth} \gls{ism}, objects can be detected way more clearly as compared to the homography variants. However, the model is confused whether the object is indeed static or dynamic. Here, the implicit assumption to only utilize the height information in the \gls{monodepth} projected input might not suffice to provide this information. Here, enhancing the \gls{monodepth} input with semantic information, e.g. using an additional channel, at least for potentially dynamic classes like e.g. pedestrians or vehicles might lead to a better distinction.
\\\\
Eventually, moving to the geo and deep \gls{ilm}s as discussed in Section \ref{subsec:exp_lidar_fusion_in_deep_isms}. Here, the geo \gls{ilm}'s free predictions provide almost perfect overlap with the ground-truth maps', showing that the mapping process does not alter the free space much. However, the occupied space shows lots of free space ray breaches which can also be verified qualitatively. This either shows deficits of the chosen parameterization (e.g. opening angle of cone $\varphi_\sphericalangle$ might be too small) or hint to potential improvements of the geo \gls{ilm}'s algorithm. With regards to the deep \gls{ilm}, the occupied contours are more continuous compared to the geo \gls{ilm}'s. Also, the capability to initialize areas occluded for the geo \gls{ilm} can be quantitatively as well as qualitatively shown.This shows the potential to provide better ground-truth occupancy maps when applying the deep over the geo \gls{ilm}. Nevertheless, the dynamic objects can only be estimated partially correct and the scores indicate lots of cases where dynamic objects are predicted as static. This, though, is to be expected since the lidar does not provide measured information about the motion status, leading the deep \gls{ilm} to estimate the motion state purely based on their position on the road. One example is shown in the white box in scene C of Fig. \ref{fig:ilm_qual_comp}. Here, the opening in the parking lot can also be interpreted as a street with the parked vehicle moving along it.
\\\\
The above examination of the deep \gls{ism} properties provides the remaining answer to RQ\ref{requ:comparison_of_isms} with regards to the proposed deep \gls{ism}'s properties given camera and lidar inputs respectively. Eventually, to answer the question about the proposed deep \gls{ism}'s properties for fused inputs, as formulated in RQ\ref{requ:comparison_of_isms_fusion}, the findings in Section \ref{subsec:exp_cam_fusion_in_deep_isms} and \ref{subsec:exp_lidar_fusion_in_deep_isms} regarding the fusion with radar shall be examined.
\\\\
Starting with the fusion of \gls{monodepth} with radar R$_{20}$ images, the quantitative results show that the radar information improves the overall performance in all classes. For the occupied space, the true rate rises by over $12\%$ which can be verified by the improved edge quality of occupied space in the qualitative results. For free space, the amount of false dynamic predictions is decreased further showing the improvement of edge information delivered by the radar. In case of dynamic areas, however, an increase in $5\%$ of false occupied rate can be seen. This problem has been examined already in Section \ref{subsec:exp_results_aleat_uncert} and seems to originate from either the proposed encoding of dynamic detections or the overall quality of the provided motion status flag of the detections provided in NuScenes.Thus, the radar helps in identifying that there is indeed an object but shows confusion whether it is indeed dynamic or not.  
\\\\
Looking at the lidar-radar fusion in deep \gls{ism}s, as discussed in Section \ref{subsec:exp_lidar_fusion_in_deep_isms} similar behavior can be observed. Again, the addition of radar detections overall improves all scores while it also leads to an increase in false occupied rate in the dynamic class. Qualitatively, it can be seen in Fig. \ref{fig:ilm_qual_comp} that the radar indeed resolves some of the false dynamic predictions (see white box in scene C) while other areas are falsified (see dynamic area above white box in scene B that is static in deep \gls{ilm} and becomes dynamic in the fusion).
\\\\
Finally, with regards to RQ\ref{requ:how_to_meas_info}, the experiments again show that the unknown mass predicted by the ShiftNet is correlated with the information content in the inputs by decreasing starting with the camera input, over camera-radar, to lidar and being overall smallest for lidar-radar inputs. This qualifies the unknown mass estimated by ShiftNet to be used as a measure for information in the subsequent occupancy mapping experiments, as proposed in Section \ref{subsec:method_to_use_deep_isms_in_occmaps}.