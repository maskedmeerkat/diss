% !TeX root = ../main.tex
\chapter{Deep ISM Experiments}
\label{ch:deep_ism_exp}
As explained in \ref{subsec:choice_of_dataset}, the experiments in this work will be conducted based on the NuScenes dataset. Here, the train-val-test split as proposed in NuScenes is used, while some of the scenes have been removed. Specifically, all scenes tagged with "night" or "difficult lighting" have been filtered out since they are relatively rare and thus, the networks with camera inputs could not properly adapt. Additionally, some scenes contain little to no ego vehicle movement (e.g. ego vehicle waiting at a red traffic light) which are great scenarios for tracking tasks but largely violate the static environment assumption in occupancy mapping. Thus, only scenes in which the ego vehicle moved at least $20$m are considered.
\\
To obtain denser measurements for occupancy mapping, the sweeps are used to create the occupancy mapping dataset. Here, the sensor modality with the fewest sweeps per scene is identified and chosen as reference. Next, the temporally closest sweeps of the remaining sensors towards the reference are processed. Afterwards, sensor-dependent procedures are applied to create the different baselines, inputs and targets for the investigated geometric and deep \gls{ism}s in form of a $128\times 128$ grid map centered around the hind axle of the ego vehicle and spanning an area of $40 \times 40$ m$^2$.
%==========================================================================%
%
%==========================================================================%
\section{Parameterization of Geo ILM and IRM}
\label{sec:choice_of_gt}
This section details the comparison of different approaches to adapt lidar to radar \gls{bev} projections. First, the sensor characteristics in the chosen dataset will be listed together with the applied methods to adapt the lidar. Afterwards, the different lidar filtering approaches will be evaluated based on the overlap between the lidar and radar maps in the mapped areas quantified by the m\gls{iou} score. 
%======================================%
%
%======================================%
\subsection{Experimental Setup}
\label{subsec:exp_setup_gt}
In the following, evidential occupancy maps of scenes as defined in the NuScenes dataset are created based on the geometric \gls{irm} and \gls{ilm} as described in Sec. \ref{subsec:method_geo_isms}. Because of the large space of parameter configurations, the search is only conducted on a small subset of the available training scenes. To identify the parameters, a two stepped approach is proposed.
\\
The first steps consists of estimating the best parameters for the Geo ILM and IRM. Since no additional occupancy ground truth is available to tune both the lidar and radar models, a temporary ILM is manually configured by the author to provide a reference to further perform a parameter grid search for the \gls{irm}. This temporary Geo \gls{ilm} uses the height-threshold-based ground-plane removal since it is the most widely used method in the literature.
\\
In the second step, to find the best overlap between the two types of maps, the removal of ground detections in the Geo \gls{ilm} will be altered. The two filters under investigation are a purely geometric height threshold-based and a semantic filter. Here, height thresholds are compared for different heights in the interval $[0,0.1]$ with step size $0.025$ and in the interval $[0.1,2.0]$ with step size $0.1$ in order to have a higher resolution close to zero. For the semantic filters, in the majority of cases the street detections are closest to the ego vehicle in the \gls{bev} projection followed by sidewalks and terrain. Since the removal of detections in occluded areas has little to no effect for geometric \gls{ism}s, a three stepped removal of the semantics is proposed which removes pixels of classes on average increasingly further away from the vehicle. The three removal steps are as follows $[$no street; no street or sidewalk; no street, sidewalk or terrain$]$.
\\
In order to obtain lidar occupancy maps closer resembling the ground-truth occupancy state, the dynamic state provided by the bounding box labels is propagated to corresponding lidar detections. All detections marked as dynamic are only used to provide boundaries for free space rays but not to define occupied space. Since the sweep information is used for mapping but the labels are only available on a sample level, the bounding box poses of dynamic objects have to be interpolated as described in \ref{subsec:method_dyn_info_for_lidar}. Thus, the following experiments also briefly cover the quality and challenges connected to the interpolation approach.
%======================================%
%
%======================================%
\subsection{Experimental Results for Geo ILM and IRM Parameter Tuning}
\label{subsec:exp_results_params_ilm_irm}
In order to parameterize the temporary geo \gls{ilm}, the height-threshold will be defined first. Here,the parameter is increased up to the point where additional structure besides street is being removed. Using the so found height threshold of 0.6 cm, the free and occupied weight $M_F, M_O, M_D$ and the opening angle $\varphi_\sphericalangle$ of Alg. \ref{alg:geo_ilm} are balanced in a way that 
\begin{itemize}
	\item remaining ground points and dynamic object artifacts are being filtered out
	\item \gls{idm} rays don't cut through object boundaries
	\item object boundaries are as dense as possible
	\item free and occupied space has maximal assigned evidential mass
\end{itemize}
leading to the parameters summarized in Tab. \ref{tab:ilm_irm_tuning}. For an illustration of the \gls{ilm} tuning, refer to Fig. \ref{fig:ilm_irm_tuning}.
\begin{figure}[H]
	\begin{center}
		\import{imgs/07_deep_ism_exp/choice_of_gt}{ilm_irm_tuning.pdf_tex}
		\caption{\label{fig:ilm_irm_tuning}Examples of the chosen \gls{ism} parameterization. First three rows showing samples of the geo \gls{irm} variants applied on a specific frame of a scene with the resulting occupancy map of the scene on the right. It can be seen that the additional detections help preserve the occupied space while also restrict free space being assigned behind object boundaries. Moreover, the additional free space rays help to densify the estimated free space while leaving the occupied space unaltered. At the bottom, the used geo \gls{ilm} is applied to the lidar data of the same frame again with the final occupancy map to its right. Additionally, examples of the \gls{ilm} with too little free mass and too small ray opening angle are shown on the right. It can be seen that too small $M_F$ leads to remaining occupied outliers due to e.g. imperfect dynamic object removal while too small $\varphi_\sphericalangle$ leads to rays being cast through object boundaries.}
	\end{center}
\end{figure} 
After fixing the temporary \gls{ilm}, it can be used to tune the \gls{irm} using the mIoU between their occupancy maps. As mentioned in Sec. \ref{subsec:method_geo_irm}, three \gls{irm} variants are separately tuned and compared against each other. These variants are the standard ray casting \gls{irm}, an extension using accumulated detections to improve the \gls{idm} ray cutoff at object boundaries and an \gls{irm} with accumulated detections and additional rays with larger opening angle for free space enrichment. As shown in Tab. \ref{tab:geo_irm_comparison}, the accumulation of detections brings additional information about occupied space and helps to correct the free space leading to a better overall mIoU. Additionally, enriching the free space with the larger \gls{idm} rays keeps the occupied score untouched while moving falsely assigned unknown mass to the free class, leading to the best mIoU score of the considered \gls{irm} variants. These variants together with resulting occupancy maps are illustrated in Fig. \ref{fig:ilm_irm_tuning}.
\begin{center}
	\begin{tabular}{c|ccc|c}
		& \multicolumn{4}{c}{mIoU}\\
		\gls{irm} variants & free & occupied & unknown & overall \\
		\hline
		ray casting & 67.0 & 16.8 & 21.5 & 35.1\\
		ray casting + acc. detections & 70.6 & 27.5 & 21.5 & 39.9\\
		ray casting + acc. detections + free rays & 76.2 & 27.5 & 22.6 & 42.1\\		
	\end{tabular}
	\captionof{table}{\label{tab:geo_irm_comparison}Comparison of mIoU of occupancy maps generated using three \gls{irm} variants and lidar occupancy maps.}
\end{center}
\begin{center}
	\begin{tabular}{c|c|c|c|c|c|c}
		\gls{ism} & $M_F$ & $M_O$ & $M_D$ & $\varphi_\sphericalangle$ & $M_F^{(2)}$ & $\varphi_\sphericalangle^{(2)}$ \\
		\hline
		\gls{ilm} & 0.025 & 0.5 & 0.3 & 3$^\circ$ & - & - \\
		\gls{irm} & 0.1 & 0.3 & 0.3 & 5$^\circ$ & 0.1 & 30$^\circ$ \\
	\end{tabular}
	\captionof{table}{\label{tab:ilm_irm_tuning}Parameter setting for geo \gls{ilm} and \gls{irm}}
\end{center}
%======================================%
%
%======================================%
\subsection{Experimental Results for Lidar Ground Plane Removal Variants}
\label{subsec:exp_results_gt}
\begin{figure}[H]
	\begin{center}
		\import{imgs/07_deep_ism_exp/choice_of_gt}{qual_results_gt_choice_scene040.pdf_tex}
		\caption{\label{fig:qual_results_gt_choice_scene040}Example of lidar maps created by successively removing ground-plane semantics and three threshold-based filters, together with the mapped area (white) and the radar map. The white box in the bottom left corner shows that for lower height threshold and up to semantics of sidewalks, many structures detected in the radar map are occluded. On the other hand, setting the height threshold too high (here demonstrated for threshold $1.7m$) removes too many points at the forefront, as shown in the white box in the upper right corner. Height threshold of about $0.5m$ or the removal of semantics up to the terrain level show a good compromise.}
	\end{center}
\end{figure} 
The quantitative comparison in Fig. \ref{fig:miou_results_gt_choice} shows that the successive removal of semantics up to the terrain level leads to increasingly better overlap up to the best reached m\gls{iou} score of $11.27\%$. On the other hand, the height threshold-based filters show improved performance up to a height threshold of $0.5m$ with a score of $10.78\%$ after which the performance starts to decrease. This suggests that for the street and sidewalk level semantics as well as low height threshold large portions of the areas detected by the radar are occluded in the lidar \gls{bev}. This is also qualitatively shown in the lower left white boxes in Fig. \ref{fig:qual_results_gt_choice_scene040}. Moreover, when the height threshold is set too high, portions of the areas detected by the radar are increasingly filtered out, as illustrated in the upper right boxes in Fig. \ref{fig:qual_results_gt_choice_scene040}. Thus, a height threshold of about $0.5m$ or the removal of semantics up to the terrain level provide the best compromise of the compared methods. However, since the semantic information is only available for keyframes in the NuScenes dataset and because the $0.5m$ height threshold filter rivals the best semantic filter in its performance, it is proposed to use the height threshold-based filter to obtain the labels for further experiments.
\begin{figure}[htp!]
	\begin{center}
		\import{imgs/07_deep_ism_exp/choice_of_gt}{mIouGtVerification.tex}
		\caption{\label{fig:miou_results_gt_choice}Results of mIoU between different evidential occupancy maps create with variations of geometric \gls{ilm}s and the geometric \gls{irm} computed in the mapped area.}
	\end{center}
\end{figure}
\todo{mIoU here doesn't support above mIoU}
%======================================%
%
%======================================%
\subsection{Discussion}
\label{subsec:discussion_gt}
The experiments show that the proposed labeling of dynamic detections of lidar sweeps based on bounding boxes does not cover all detections. However, it has also been shown that the remainder of the dynamic detections can be filtered out using occupancy mapping with an \gls{ilm}, specifically tuned for the task.
\\
On another note, the occupancy maps created using the proposed \gls{irm} variants have shown an improvement in mIoU of $7\%$ compared to the basic ray casting approach, measured by the mIoU with the lidar maps. However, even though overall overlap between the modalities could be increased, there is still a large space for improvement. This is especially true when it comes to estimating occupied and unknown space using the \gls{irm}. 
\\
Finally, the comparison of lidar ground plane removal methods demonstrates that the removal of detections up to the terrain level increases the overlap between lidar and radar sensing modalities. Additionally, it can be seen that the application of the simple height threshold removal method can reach similar performance when it comes to aligning the sensing overlap. Thus, since the lidar labels are only provided on the sample level and an interpolation down to the sweep level is non-trivial, it is proposed to use the height threshold method for the further experiments.
%==========================================================================%
%
%==========================================================================%
\section{Choice of UNet Architecture}
\label{sec:choice_of_unet_arch}
%======================================%
%
%======================================%
\subsection{Experimental Setup}
\label{subsec:exp_setup_unet_arch}
Since the focus of this work lies on the investigation of radar \gls{ism}s, the architecture search is performed based on radar input images with occupancy map patches for targets (see Sec. \ref{subsec:def_of_targets_n_inputs}). More specifically, radar \gls{bev} images based on one sweep's information are used, since they contain the least information and, thus, provide the most difficult task for the network to handle. Moreover, the considered UNet (see Sec. \ref{subsec:method_deep_ism_architecture}) is trained in SoftNet configuration (see Sec. \ref{subsec:method_al_uncert_in_deep_isms}), since it is the baseline configuration as proposed by the literature.
\\
The hyperparameters searched with the following procedure are the downsample factor $D$ of each ResNet layer in the UNet and the amount of filters for each stage $C_k, k \in [0,4]$. With regards to the filters, the approach proposed in the literature to double the amount after each encoder and halve it after each decoder stage respectively up to a maximum number of filters is adapted in this work (see Sec. \ref{subsec:architecture}). Thus, reducing the filter search to the initial amount of filters $C_0$ and the maximum amount of filter $C_{\max}$. These hyperparameters shall be investigated in a two stepped approach. First, $D$ is set to $0.5$ which is half of the most conservative compression rate reported to work without loss in performance (see Sec. \ref{subsec:architecture}). On the other hand, for $C_0$, the following variations are evaluated $[4,8,16,24,32,40]$ with $C_{\max} = \infty$.\\
Given the results of these variations, a configuration with a good trade-off between inference speed and accuracy is chosen for further optimization. Here, based on personal experience and the architectures reported in the literature, $C_{\max}$ is set to $128$ filters. Additionally, $D$ is successively halved starting from $0.25$, which is still reported throughout the literature to work without significant loss of accuracy, up to the point of collapse in performance.
\\   
All these experiments are conducted using Tensorflow 2.1 \cite{abadi2016tensorflow}, the ADAM optimizer \cite{kingma2014adam} with a learning rate of $0.001$ and a dropout rate of $0.3$. The layers are initialized using the HeNormal initializer \cite{he2015delving} and trained until convergence, as indicated by the validation set, to remove the bias due to the random initialization. The experiments are conducted on two NVIDIA Tesla V100 (32GB) GPUs and evaluated on a single core of an Intel Core Processor i7-10750H CPU.
%======================================%
%
%======================================%
\subsection{Experimental Results}
\label{subsec:exp_results_unet_arch}
As explained above, the architecture search is split into two parts. Here, the first part fixes the ResNet layer's downsample rate $D$ and alternates the initial number of filters $C_0$ with $C_{\max} = \infty$. These hyperparameter configurations are summarized in the experiments $1-6$ in \ref{tab:networking_tuning_exp_setup} while the results are shown in Fig. \ref{fig:network_tuning} marked in orange. The experiments show an exponential decrease in inference time to obtain improve the mIoU. With regards to the time till convergence of training, the time remains about the same up to $C_0 = 24$. Afterwards, or $C_0 = 32$, it more than triples and doubles again for $C_0 = 40$. The behavior of the training and inference time are as expected since \todo{show that the complexity of a resnet layer is squared with filter size}.\\
Based on these results, $C_0 = 32$ is chosen and further used to fine tune $D$ for the following reasons. First, further increasing $C_0$ shows to move the inference speed too far away from the goal of 100 Hz as defined in R\ref{subreq:resource_efficient_inference}. However, increasing $C_0$ from $24$ to $32$ showed a significant improvement in qualitative predictions as can be seen in Fig. \ref{fig:gt_exp_qual_results}.
\begin{center}
\begin{tabular}{c|c|c|c}
	experiment number & $C_0$ & $C_{\max}$ & $D$ \\
	\hline
	1 & 4 & $\infty$ & 0.5 \\
	2 & 8 & $\infty$ & 0.5 \\
	3 & 16 & $\infty$ & 0.5 \\
	4 & 24 & $\infty$ & 0.5 \\
	5 & 32 & $\infty$ & 0.5 \\
	6 & 40 & $\infty$ & 0.5 \\
	\hline
	7 & 32 & 128 & 0.25 \\
	8 & 32 & 128 & 0.125
\end{tabular}
\captionof{table}{\label{tab:networking_tuning_exp_setup}Hyperparameter setting for all network tuning experiments}
\end{center}
To fine tune the capacity of the network, $C_0$ is fixed to 32 while an upper limited on the number of filters is set to $C_{\max}=128$. Given this setup, $D$ is halved, starting from 0.5 up to the point where the performance collapses. It can be seen that this is the case after the second reduction as shown by the blue dots in the left plot of Fig. \ref{fig:network_tuning}. The parameter settings for the two experiments labeled 7 and 8 are shown in the lower part of \ref{tab:networking_tuning_exp_setup}. The experimental results show that a reduction of $D$ from 0.5 to 0.25 brings the network about $6\%$ closer to the goal of 100 Hz while roughly keeping its performance. Also, the interpolation capability seems to deteriorate only after the second reduction step, as shown qualitatively in Fig. \ref{fig:gt_exp_qual_results}. At the same time, the first reduction more than halves the training time.
\begin{figure}[H]
	\begin{center}
		\import{imgs/07_deep_ism_exp/choice_of_gt}{qualitative_comparison.pdf_tex}
		\caption{\label{fig:gt_exp_qual_results}Qualitative results of the different models trained in the experiments as numbered in Fig. \ref{tab:networking_tuning_exp_setup} with the radar input shown on the right. It can be seen that up to $C_0=32$ in experiments $\#5$ only little interpolation of occupied space is provided by the models. A prominent example is the interpolated edge marked by the white box. Increasing $C_0$ further provides even clearer edges. Also, shrinking the bottle neck $D$ seems to keep the interpolation capability in $\#7$ while loosing it in experiment $\#8$.}
	\end{center}
\end{figure}
\begin{center}
\begin{minipage}{.5\textwidth}
	\import{imgs/07_deep_ism_exp/network_tuning}{mIoU_network_tuning.tex}
\end{minipage}
%
\hfill
\begin{minipage}{.47\textwidth}
	\import{imgs/07_deep_ism_exp/network_tuning}{train_time_network_tuning.tex} 
\end{minipage}
	\captionof{figure}{\label{fig:network_tuning}Experimental results of the network tuning experiments. On the left-hand-side, the mIoU over the CPU inference time is shown while on the right-hand-side, the training time is plotted for each experiment. Here, orange marks the first part of the experiments in which the downsamlping rate $D$ is fixed and the initial amount of filters $C_0$ is searched. On the other hand, blue marks the second stage of the parameter search in which $D$ is finetuned.}
\end{center}
%======================================%
%
%======================================%
\subsection{Discussion}
\label{subsec:discussion_unet_arch}
First of all, the choice of only conducting each experiment once shall be discussed. It can shall be argued that this choice is justified since the trend of the experiments clearly follows the expected and in the literature reported behavior for experiments $\#1$ - $\#7$. Moreover, all models have been trained until convergence, thereby reducing the influence of random initialization. Regarding experiment $\#8$, the severe drop as shown is also expected due to the increased reduction in information through the reduced bottleneck. However, the effect might also be due to an outlier behavior. In any case, since the mean inference time did not significantly increase over it predecessor, the configuration in $\#7$ is chosen and $\#8$ is not repeated.  
\\
The experiments to fine-tune the architecture for this work clearly verify the results as stated in the state-of-the-art. It can be seen that the UNet architecture is capable to learn geometric interpolations (see interpolated corner in white box in Fig. \ref{fig:gt_exp_qual_results}). Also, the choice of network depth, and with it the network's receptive field, suffice to learn the concept of occluded areas. This can be seen in Fig. \ref{fig:gt_exp_qual_results} by the unknown areas (blue) behind occluded areas (green) given the reference point is the image center. Therefore, the model can clearly utilize the spatial coherence in huge amounts of data and, thus, suffices R\ref{subreq:input_output}, \ref{subreq:big_data} and \ref{subreq:ev_rep}.
\\
Moreover, it has been shown that the reduction in the ResNet layer's bottleneck can be done up to factor of $0.25$ while almost keeping the performance, which validates the results reported in the state-of-the-art. A further reduction leads to serious performance degradation (might be due to outlier behavior). Here, the variant $\#7$ is still able to predict geometric interpolations while providing an inference speed close enough to suffice R\ref{subreq:resource_efficient_inference}, thus answering RQ\ref{requ:network_search}.
\\
Thus, the architecture as parameterized in experiment $\#7$ will be used as a baseline for the following experiments.
%==========================================================================%
%
%==========================================================================%
\section{Aleatoric Uncertainties in deep ISMs}
\label{sec:al_uncert_in_deep_isms}
%======================================%
%
%======================================%
\subsection{Experimental Setup}
\label{subsec:exp_setup_aleat_uncert}
To investigate how to best learn and shift the aleatoric uncertainty as described in RQ\ref{requ:how_to_sep_uncertainty}, the basic UNet architecture as tuned in Sec. \ref{sec:choice_of_unet_arch} is modified and trained in the SoftNet, ShiftNet and DirNet configuration as described in Sec. \ref{subsec:method_al_uncert_in_deep_isms}. These configurations are trained using radar inputs and occupancy map targets as described in Sec. \ref{subsec:def_of_targets_n_inputs}. To better investigate the effect of data uncertainty, radar inputs based on a single timestep's information $R_1$ and based on accumulation of 20 timesteps $R_20$ is compared. Additionally, the geo \gls{irm}'s performance for both radar inputs is evaluated to provide a reference. The evaluation is based on the confusion matrix variant as explained in Sec. \ref{subsec:confusion_matrix}.
%======================================%
%
%======================================%
\subsection{Experimental Results}
\label{subsec:exp_results_aleat_uncert}
For the following interpretation of quantitative results, it shall be noted that unknown mass in other classes is not seen as false predictions but rather as an indicator for certainty. Thus, the overall false rate only equals the sum over the red scores per row for each class.
\\ 
Starting with the R$_1$ scores, the usage of the modified confusion matrix, as opposed to the mIoU used in \ref{sec:choice_of_gt}, allows a more detailed analysis of the geo \gls{irm}. It can be seen that the geo \gls{irm} provides less then $1\%$ correct predictions both for dynamic and occupied space due to the sparseness in detections, which can also be seen in Fig. \ref{fig:irm_qual_comp}. On top of that, the false free space predictions in these two categories are almost $20\%$ showing the effect of rays cutting through occupied space due to multi-path reflections. On the other hand, while the free space predictions are sparse with about $70\%$ of the space being predicted as unknown, the rest is predicted correctly. In an overall comparison with the deep \gls{irm} variants, the geo \gls{irm} provides the least false rates in all categories but also the smallest positive rates. This is experimentally shows the reason for the approach to use a deep \gls{irm} to initialize the maps and later converge to the geo \gls{irm}.
\\\\
Next, the SoftNet configuration treats the aleatoric uncertainty by equally distributing mass into the two classes between which the uncertainty occurs, as stated in H \ref{hyp:sota_not_model_unc}. In this case, the majority of uncertainty in the visible area is at the boundaries between occupied and free areas leading to huge portions of the actually free and occupied class respectively being estimated as dynamic. This effect can be seen in Fig. \ref{fig:irm_qual_comp} by the blurriness of the occupied space. Due to this bias towards the dynamic class, the true rate for dynamic objects is also the best among the investigated methods. In occluded areas, in addition to huge false rates of dynamic objects, an increase of unknown mass can be observed over all classes. This might be due to a combination of bias towards the unknown class in occluded areas and the fact, that the aleatoric uncertainty now also occurs at boundaries between free, occupied and the unknown class. 
\\\\
In contrast to SoftNet, DirNet is capable of shifting the aleatoric uncertainty into the unknown class which can be seen by less than half of the overall false dynamic predictions in the free and occupied categories while at the same time increasing the unknown mass portion over all classes. Moreover, in the occluded area, where more aleatoric uncertainty can be expected, larger portions are shifted into the unknown class which also results in smaller false rates for free and occupied cells. These effects can also been seen in Fig. \ref{fig:irm_qual_comp} by the clearer boundaries between free and occupied space and overall more unknown space in occluded regions. This is in contrast to the almost steady false rates in the visible and occluded area for SoftNet, additionally highlighting the uncertainty awareness of the DirNet configuration. On top of that, DirNet surpasses the positive rates of SoftNet in all but the biased dynamic class. The improvement in performance might be due to the effect that by modeling the aleatoric uncertainty, the loss function is weighted in a way to increasingly ignore predictions for which the network cannot find a sufficient solution. This leads to more network capacity being focused on the majority of data which leads to better average performance and, thus, better scores. It shall be noted that, while the scores improve, this behavior might lead to a neglection of edge cases.
\\\\
Finally, ShiftNet demonstrates even better capabilities in shifting the aleatoric uncertainties to the unknown class as compared to DirNet which is indicated by the highest unknown mass rates of all model variants. This uncertainty-awareness can again also be observed by higher unknown mass rates for the occluded as compared to the visible areas. The effect of shifted uncertainty is so dominant that it can even be seen in the qualitative results in Fig. \ref{fig:irm_qual_comp} by unknown space (blue) around all occupied boundaries. Regarding the occupied and free space, ShiftNet provides the least false rates and best free space predictions for all classes compared to all learned \gls{irm}s. However, it lacks the capability to properly model the occupied space.  
\\\\
For the R$_{20}$ scores, an expected overall improvement of all models can be observed. Here, the free space predictions of SoftNet even improve to the point of surpassing the other variants. With regards to aleatoric uncertainty, the unknown mass portions for the DirNet and ShiftNet decrease compared to the R$_1$ scores as is desired in the light of more input information. Additionally, similar to the R$_1$ scores, an increase in unknown mass can be observed in occluded as compared to visible areas. Another notable change is the improvement in occupied predictions given by ShiftNet, now surpassing SoftNet and almost closing the gap to DirNet. The geo IRM is also capable of improving the dynamic true rate by about seven times and the occupied rate by about 13 times while reducing the respective overall false rates. This is as expected since it can leverage the information of 20 times as many timesteps. Also, the free space true rate reaches about 50$\%$ while only minimally increasing the false rates. These general improvements over the R$_1$-based predictions are also clearly visible in Fig. \ref{fig:irm_qual_comp}.
%==========================%
% R01
%==========================%
\begin{figure}[H]
	\footnotesize
	\begin{tabular}{c|c|cccc|cccc|cccc}
		&$k$ & $d$ & $f$ & $o$ & $u$ & $d$ & $f$ & $o$ & $u$ & $d$ & $f$ & $o$ & $u$\\
		\hline
		\parbox[t]{2mm}{\multirow{4}{*}{\rotatebox[origin=c]{90}{\scriptsize{geo IRM}}}}&$p(k|d)$ & \textcolor{mygreen}{0.6} & \textcolor{myred}{19.6} & \textcolor{myred}{0.4} & 79.3 & \textcolor{mygreen}{0.6} & \textcolor{myred}{34.3} & \textcolor{myred}{0.4} & 64.5 & \textcolor{mygreen}{0.6} & \textcolor{myred}{12.6} & \textcolor{myred}{0.4} & 86.3\\
		&$p(k|f)$ & \textcolor{myred}{0.0} & \textcolor{mygreen}{28.9} & \textcolor{myred}{0.0} & 70.9 & \textcolor{myred}{0.0} & \textcolor{mygreen}{35.9} & \textcolor{myred}{0.0} & 63.9 & \textcolor{myred}{0.0} & \textcolor{mygreen}{9.6} & \textcolor{myred}{0.0} & 90.2\\
		&$p(k|o)$ & \textcolor{myred}{0.2} & \textcolor{myred}{18.4} & \textcolor{mygreen}{0.8} & 80.5 & \textcolor{myred}{0.3} & \textcolor{myred}{30.6} & \textcolor{mygreen}{0.9} & 68.1 & \textcolor{myred}{0.2} & \textcolor{myred}{14.5} & \textcolor{mygreen}{0.8} & 84.4\\
		&$p(k|u)$ & 0.0 & 5.4 & 0.1 & 94.4 & - & - & - & - & 0.0 & 5.4 & 0.1 & 94.5\\
		\hline
%		\multicolumn{13}{c}{}\\
%		\hline
		\parbox[t]{2mm}{\multirow{4}{*}{\rotatebox[origin=c]{90}{\scriptsize{SoftNet}}}}&$p(k|d)$ & \textcolor{mygreen}{49.7} & \textcolor{myred}{16.7} & \textcolor{myred}{10.5} & 23.2 & \textcolor{mygreen}{52.5} & \textcolor{myred}{18.8} & \textcolor{myred}{11.5} & 17.2 & \textcolor{mygreen}{48.8} & \textcolor{myred}{15.8} & \textcolor{myred}{9.6} & 25.8\\
		&$p(k|f)$ & \textcolor{myred}{35.8} & \textcolor{mygreen}{37.0} & \textcolor{myred}{3.3} & 24.0 & \textcolor{myred}{36.2} & \textcolor{mygreen}{42.6} & \textcolor{myred}{3.1} & 18.2 & \textcolor{myred}{35.5} & \textcolor{mygreen}{21.5} & \textcolor{myred}{4.0} & 39.1\\
		&$p(k|o)$ & \textcolor{myred}{37.8} & \textcolor{myred}{9.6} & \textcolor{mygreen}{17.2} & 35.5 & \textcolor{myred}{43.7} & \textcolor{myred}{11.5} & \textcolor{mygreen}{20.0} & 24.8 & \textcolor{myred}{35.9} & \textcolor{myred}{9.0} & \textcolor{mygreen}{16.2} & 38.9\\
		&$p(k|u)$ & 25.7 & 8.7 & 4.5 & 61.0 & - & - & - & - & 25.7 & 8.7 & 4.5 & 61.1\\
		\hline
%		\multicolumn{13}{c}{}\\
%		\hline
		\parbox[t]{2mm}{\multirow{4}{*}{\rotatebox[origin=c]{90}{\scriptsize{DirNet}}}}&$p(k|d)$ & \textcolor{mygreen}{31.0} & \textcolor{myred}{21.9} & \textcolor{myred}{12.8} & 34.3 & \textcolor{mygreen}{35.4} & \textcolor{myred}{27.4} & \textcolor{myred}{13.0} & 24.2 & \textcolor{mygreen}{29.0} & \textcolor{myred}{19.9} & \textcolor{myred}{12.3} & 38.7\\
		&$p(k|f)$ & \textcolor{myred}{13.6} & \textcolor{mygreen}{47.5} & \textcolor{myred}{5.6} & 33.3 & \textcolor{myred}{15.5} & \textcolor{mygreen}{56.8} & \textcolor{myred}{4.6} & 23.2 & \textcolor{myred}{9.0} & \textcolor{mygreen}{22.3} & \textcolor{myred}{8.4} & 60.3\\
		&$p(k|o)$ & \textcolor{myred}{13.5} & \textcolor{myred}{9.7} & \textcolor{mygreen}{20.7} & 56.1 & \textcolor{myred}{22.4} & \textcolor{myred}{14.3} & \textcolor{mygreen}{22.9} & 40.3 & \textcolor{myred}{10.8} & \textcolor{myred}{8.4} & \textcolor{mygreen}{19.9} & 60.9\\
		&$p(k|u)$ & 2.7 & 3.7 & 12.0 & 81.6 & - & - & - & - & 2.7 & 3.6 & 12.0 & 81.7\\
		\hline
%		\multicolumn{13}{c}{}\\
%		\hline
		\parbox[t]{2mm}{\multirow{4}{*}{\rotatebox[origin=c]{90}{\scriptsize{ShiftNet}}}}&$p(k|d)$ & \textcolor{mygreen}{31.8} & \textcolor{myred}{19.0} & \textcolor{myred}{7.2} & 41.9 & \textcolor{mygreen}{30.7} & \textcolor{myred}{23.3} & \textcolor{myred}{7.5} & 38.6 & \textcolor{mygreen}{33.3} & \textcolor{myred}{17.2} & \textcolor{myred}{6.9} & 42.7\\
		&$p(k|f)$ & \textcolor{myred}{7.4} & \textcolor{mygreen}{48.6} & \textcolor{myred}{2.0} & 42.0 & \textcolor{myred}{7.5} & \textcolor{mygreen}{56.0} & \textcolor{myred}{1.6} & 34.9 & \textcolor{myred}{7.6} & \textcolor{mygreen}{28.1} & \textcolor{myred}{3.2} & 61.1\\
		&$p(k|o)$ & \textcolor{myred}{8.4} & \textcolor{myred}{15.1} & \textcolor{mygreen}{13.7} & 62.8 & \textcolor{myred}{10.0} & \textcolor{myred}{19.2} & \textcolor{mygreen}{14.9} & 55.9 & \textcolor{myred}{7.9} & \textcolor{myred}{13.9} & \textcolor{mygreen}{13.4} & 64.8\\
		&$p(k|u)$ & 4.0 & 9.4 & 4.7 & 81.9 & - & - & - & - & 4.0 & 9.3 & 4.7 & 82.0\\
		\hline
		\multicolumn{2}{c|}{\textbf{R$_1$ Scores}} & \multicolumn{4}{c|}{overall} & \multicolumn{4}{c|}{visible} & \multicolumn{4}{c}{occluded}\\
		\multicolumn{13}{c}{}\\
%==========================%
% R20
%==========================%
		&$k$ & $d$ & $f$ & $o$ & $u$ & $d$ & $f$ & $o$ & $u$ & $d$ & $f$ & $o$ & $u$\\
		\hline
		\parbox[t]{2mm}{\multirow{4}{*}{\rotatebox[origin=c]{90}{\scriptsize{geo IRM}}}}&$p(k|d)$ & \textcolor{mygreen}{7.1} & \textcolor{myred}{24.5} & \textcolor{myred}{6.8} & 61.4 & \textcolor{mygreen}{4.5} & \textcolor{myred}{43.7} & \textcolor{myred}{7.5} & 44.1 & \textcolor{mygreen}{8.2} & \textcolor{myred}{15.9} & \textcolor{myred}{6.5} & 69.3\\
		&$p(k|f)$ & \textcolor{myred}{0.4} & \textcolor{mygreen}{47.0} & \textcolor{myred}{0.7} & 51.7 & \textcolor{myred}{0.4} & \textcolor{mygreen}{57.8} & \textcolor{myred}{0.5} & 41.1 & \textcolor{myred}{0.6} & \textcolor{mygreen}{17.5} & \textcolor{myred}{1.2} & 80.6\\
		&$p(k|o)$ & \textcolor{myred}{2.6} & \textcolor{myred}{16.8} & \textcolor{mygreen}{13.5} & 67.0 & \textcolor{myred}{2.7} & \textcolor{myred}{31.0} & \textcolor{mygreen}{15.4} & 50.7 & \textcolor{myred}{2.5} & \textcolor{myred}{12.3} & \textcolor{mygreen}{13.0} & 72.1\\
		&$p(k|u)$ & 0.6 & 3.6 & 1.7 & 94.1 & - & - & - & - & 0.6 & 3.5 & 1.7 & 94.1\\
		\hline
		\parbox[t]{2mm}{\multirow{4}{*}{\rotatebox[origin=c]{90}{\scriptsize{SoftNet}}}}&$p(k|d)$ & \textcolor{mygreen}{52.4} & \textcolor{myred}{21.8} & \textcolor{myred}{12.0} & 13.7 & \textcolor{mygreen}{54.6} & \textcolor{myred}{25.2} & \textcolor{myred}{12.7} & 7.6 & \textcolor{mygreen}{51.9} & \textcolor{myred}{20.6} & \textcolor{myred}{11.3} & 16.2\\
		&$p(k|f)$ & \textcolor{myred}{21.6} & \textcolor{mygreen}{64.1} & \textcolor{myred}{2.0} & 12.2 & \textcolor{myred}{19.5} & \textcolor{mygreen}{72.0} & \textcolor{myred}{1.6} & 6.9 & \textcolor{myred}{28.1} & \textcolor{mygreen}{42.6} & \textcolor{myred}{3.3} & 26.0\\
		&$p(k|o)$ & \textcolor{myred}{36.2} & \textcolor{myred}{12.9} & \textcolor{mygreen}{22.8} & 28.1 & \textcolor{myred}{43.2} & \textcolor{myred}{15.3} & \textcolor{mygreen}{25.8} & 15.6 & \textcolor{myred}{33.9} & \textcolor{myred}{12.2} & \textcolor{mygreen}{21.9} & 32.0\\
		&$p(k|u)$ & 19.2 & 9.9 & 4.6 & 66.3 & - & - & - & - & 19.1 & 9.8 & 4.6 & 66.5\\
		\hline
%		\multicolumn{13}{c}{}\\
%		\hline
		\parbox[t]{2mm}{\multirow{4}{*}{\rotatebox[origin=c]{90}{\scriptsize{DirNet}}}}&$p(k|d)$ & \textcolor{mygreen}{37.1} & \textcolor{myred}{22.7} & \textcolor{myred}{17.4} & 22.8 & \textcolor{mygreen}{39.6} & \textcolor{myred}{29.2} & \textcolor{myred}{18.3} & 12.8 & \textcolor{mygreen}{36.2} & \textcolor{myred}{20.2} & \textcolor{myred}{16.5} & 27.1\\
		&$p(k|f)$ & \textcolor{myred}{10.9} & \textcolor{mygreen}{61.5} & \textcolor{myred}{5.1} & 22.4 & \textcolor{myred}{11.3} & \textcolor{mygreen}{71.4} & \textcolor{myred}{3.9} & 13.5 & \textcolor{myred}{10.6} & \textcolor{mygreen}{35.4} & \textcolor{myred}{8.3} & 45.7\\
		&$p(k|o)$ & \textcolor{myred}{15.2} & \textcolor{myred}{10.5} & \textcolor{mygreen}{31.8} & 42.6 & \textcolor{myred}{25.0} & \textcolor{myred}{15.1} & \textcolor{mygreen}{35.2} & 24.6 & \textcolor{myred}{12.0} & \textcolor{myred}{9.1} & \textcolor{mygreen}{30.7} & 48.2\\
		&$p(k|u)$ & 3.0 & 5.4 & 12.7 & 78.9 & - & - & - & - & 3.0 & 5.4 & 12.6 & 79.0\\
		\hline
%		\multicolumn{13}{c}{}\\
%		\hline
		\parbox[t]{2mm}{\multirow{4}{*}{\rotatebox[origin=c]{90}{\scriptsize{ShiftNet}}}}&$p(k|d)$ & \textcolor{mygreen}{38.5} & \textcolor{myred}{18.1} & \textcolor{myred}{12.7} & 30.7 & \textcolor{mygreen}{35.3} & \textcolor{myred}{22.9} & \textcolor{myred}{13.7} & 28.1 & \textcolor{mygreen}{40.9} & \textcolor{myred}{15.9} & \textcolor{myred}{12.0} & 31.2\\
		&$p(k|f)$ & \textcolor{myred}{4.7} & \textcolor{mygreen}{62.9} & \textcolor{myred}{3.2} & 29.2 & \textcolor{myred}{4.3} & \textcolor{mygreen}{71.1} & \textcolor{myred}{2.4} & 22.1 & \textcolor{myred}{6.1} & \textcolor{mygreen}{40.8} & \textcolor{myred}{5.3} & 47.8\\
		&$p(k|o)$ & \textcolor{myred}{6.0} & \textcolor{myred}{13.6} & \textcolor{mygreen}{28.5} & 51.8 & \textcolor{myred}{7.4} & \textcolor{myred}{17.6} & \textcolor{mygreen}{30.9} & 44.1 & \textcolor{myred}{5.7} & \textcolor{myred}{12.4} & \textcolor{mygreen}{27.8} & 54.1\\
		&$p(k|u)$ & 3.0 & 9.0 & 7.8 & 80.2 & - & - & - & - & 3.0 & 8.9 &7.8 & 80.3\\
		\hline
		\multicolumn{2}{c|}{\textbf{R$_{20}$ Scores}} & \multicolumn{4}{c|}{overall} & \multicolumn{4}{c|}{visible} & \multicolumn{4}{c}{occluded}
	\end{tabular}
	\caption{\label{tab:deep_ism_r20_results}Results of deep \gls{ism} variants trained on R$_1$ and R$_{20}$ images for visible, occluded and overall areas. Here, the visible area is defined as the area which is not unknown in the geometric \gls{ilm}. Thus, only a stochastically insignificant portion of unknown labels remains in the visible area, which is why these scores are excluded from consideration. The scores are given in the form of a confusion matrix for each model where each row shows the probabilities in percentage of estimates given the true class. Values in green correspond to true positives, red shows the percentages of false predictions. The unknown class predictions (black) are treated as the safe state and, as such, do not add to the false rates. Additionally, since the network is permitted to extrapolate the state in unknown areas, all predictions deviating from the unknown class given unknown labels should also not be treated as false.}
\end{figure} 
\begin{figure}[H]
	\begin{center}
		\import{imgs/07_deep_ism_exp/aleat_uncert}{irm_qual_comp.pdf_tex}
		\caption{\label{fig:irm_qual_comp}Qualitative results of the different compared \gls{irm}s (column-wise) for three validation scenes showing the predictions based on R$_1$ and R$_{20}$ in the respective rows with the corresponding label. Scene A illustrates the models capabilities to predict moving objects of any shape (e.g. two cars, one bus and a cyclist). Additionally, the lower box encloses an example of small distinct objects being situated close to each other. Scene B shows an effect of a bias in the radar data (marked in the box). Most of the time, moving objects only occlude parts of the scene temporarily leading to detections of the occluded areas at other times (e.g. upper box in scene A). Thus, if a wall is erroneously detected as moving and no further detections are provided behind it, the models predicts the area behind the wall as free. Finally, scene C illustrates the capacity of the methods to predict the occupancy state in a general parking space, which is one of the main application scenarios of the proposed system.}
	\end{center}
\end{figure}
%======================================%
%
%======================================%
\subsection{Discussion}
\label{subsec:discussion_aleat_uncert}
The experiments have shown that the current state-of-the-art deep \gls{ism} (SoftNet) indeed models occurring uncertainty as conflicting mass leading to a huge bias in the dynamic class (see Fig. \ref{tab:deep_ism_r20_results}), proving H\ref{hyp:sota_not_model_unc}. The two compared methods to shift the uncertainty to the unknown class both clearly reduce the amount of bias in the dynamic class. Additionally, the amount of shifted mass to the unknown class clearly correlates with the amount of certainty in the data which can be seen by comparing the unknown mass assigned in visible and occluded areas and also the overall assigned unknown mass between $R_1$ and $R_{20}$ inputs, providing an answer to RQ\ref{requ:how_to_sep_uncertainty}. 
\\
Since ShiftNet provides the least false rates per class over all classes for the $R_{20}$ compared to all models while providing similar true positive rates compared to DirNet it largely suffices R\ref{subreq:unknown_mass} and \ref{subreq:conflicting_mass}. Thus, ShiftNet will be used as the baseline method for further experiments and analysis.
\\
The disentanglement of the uncertainty and the dynamic class directly leads to better dynamic class predictions. To provide a partial answer of RQ\ref{requ:dyn_objects}, the qualitative results in Fig. \ref{fig:irm_qual_comp} Scene A shall be analyzed. These show that the $R_1$ inputs contain sometimes only a single detection belonging to a dynamic object. This leads the networks to predict a dynamic object prior with size and shape of a car oriented in the direction of the road (compare lower right dynamic object in upper marked box in input and ShiftNet prediction of Scene A in Fig. \ref{fig:irm_qual_comp}). For $R_{20}$, all of the dynamic object's detections over time are available leading to an overall improvement in dynamic class predictions. The approach to decay the dynamic detections over time to encode the motion direction seems to work for some vehicles (see ShiftNet prediction and label of car in lower left for Scene A) while other objects are elongated (see ShiftNet prediction bicycle in the lower right of the upper marked box in Scene A). Thus, further improvement needs to be done in the future to find better ways of encoding the dynamic detections over time. 
%==========================================================================%
%
%==========================================================================%
\section{Camera-Radar Fusion in deep ISMs}
\label{sec:cam_radar_fusion_in_deep_isms}
%======================================%
%
%======================================%
\subsection{Experimental Setup}
\label{subsec:exp_setup_cam_inputs}
To analyze the performance and occurring effects using a deep \gls{icm}, the homography projection into \gls{bev} of all camera images, their semantics and monocular depth estimates are considered as inputs (details on the chosen inputs are described in Sec. \ref{subsec:def_of_targets_n_inputs}). For reasons discussed in Sec. \ref{subsec:discussion_aleat_uncert}, the ShiftNet configuration is chosen for the experiments. Additionally, the fusion of camera and radar modalities shall be investigated to analyze the capability of the approach to enrich camera information with measurements of a depth sensor and to make use of the dynamic information in the radar signal. Here, only the monocular depth projection is used for the experiments since it is shown in Sec. \ref{subsec:exp_results_cam_inputs} to provide the best performance among the investigated camera input encodings. Since both signals are provided in the same projection, the fusion can be performed by concatenating the inputs to form a new one. 
%======================================%
%
%======================================%
\subsection{Experimental Results}
\label{subsec:exp_results_cam_inputs}
\begin{figure}
\begin{tabular}{c|c|cccc|cccc|cccc}
	&$k$ & $d$ & $f$ & $o$ & $u$ & $d$ & $f$ & $o$ & $u$ & $d$ & $f$ & $o$ & $u$\\
	\hline
	\parbox[t]{2mm}{\multirow{4}{*}{
\rotatebox[origin=c]{90}{\scriptsize{Homog. RGB}}}}&$p(k|d)$ & \textcolor{mygreen}{33.2} & \textcolor{myred}{23.0} & \textcolor{myred}{5.3} & 38.6 & \textcolor{mygreen}{28.3} & \textcolor{myred}{33.0} & \textcolor{myred}{5.6} & 33.0 & \textcolor{mygreen}{36.3} & \textcolor{myred}{18.9} & \textcolor{myred}{4.6} & 40.2\\
	&$p(k|f)$ & \textcolor{myred}{4.0} & \textcolor{mygreen}{58.9} & \textcolor{myred}{3.1} & 33.9 & \textcolor{myred}{3.1} & \textcolor{mygreen}{70.1} & \textcolor{myred}{2.7} & 24.1 & \textcolor{myred}{6.6} & \textcolor{mygreen}{29.8} & \textcolor{myred}{4.3} & 59.4\\
	&$p(k|o)$ & \textcolor{myred}{4.1} & \textcolor{myred}{14.6} & \textcolor{mygreen}{11.0} & 70.3 & \textcolor{myred}{4.3} & \textcolor{myred}{22.6} & \textcolor{mygreen}{12.2} & 60.8 & \textcolor{myred}{4.1} & \textcolor{myred}{12.2} & \textcolor{mygreen}{10.6} & 73.1\\
	&$p(k|u)$ & 3.2 & 4.7 & 5.0 & 87.2 & - & - & - & - & 3.2 & 4.6 & 4.9 & 87.3\\
	\hline
	\parbox[t]{2mm}{\multirow{4}{*}{
\rotatebox[origin=c]{90}{\scriptsize{Homog. SemSeg}}}}&$p(k|d)$ & \textcolor{mygreen}{38.1} & \textcolor{myred}{22.3} & \textcolor{myred}{4.6} & 35.0 & \textcolor{mygreen}{34.6} & \textcolor{myred}{30.2} & \textcolor{myred}{4.8} & 30.5 & \textcolor{mygreen}{40.9} & \textcolor{myred}{18.6} & \textcolor{myred}{4.2} & 36.2\\
	&$p(k|f)$ & \textcolor{myred}{4.0} & \textcolor{mygreen}{62.4} & \textcolor{myred}{2.5} & 31.2 & \textcolor{myred}{3.2} & \textcolor{mygreen}{73.9} & \textcolor{myred}{1.8} & 21.1 & \textcolor{myred}{6.3} & \textcolor{mygreen}{32.2} & \textcolor{myred}{4.2} & 57.3\\
	&$p(k|o)$ & \textcolor{myred}{4.8} & \textcolor{myred}{15.9} & \textcolor{mygreen}{11.2} & 68.1 & \textcolor{myred}{5.7} & \textcolor{myred}{23.7} & \textcolor{mygreen}{11.6} & 58.9 & \textcolor{myred}{4.6} & \textcolor{myred}{13.6} & \textcolor{mygreen}{11.0} & 70.8\\
	&$p(k|u)$ & 2.6 & 6.5 & 5.4 & 85.5 & - & - & - & - & 2.6 & 6.4 & 5.4 & 85.6\\
	\hline
	\parbox[t]{2mm}{\multirow{4}{*}{
\rotatebox[origin=c]{90}{\scriptsize{MonoDepth}}}}&$p(k|d)$ & \textcolor{mygreen}{40.8} & \textcolor{myred}{16.8} & \textcolor{myred}{7.0} & 35.3 & \textcolor{mygreen}{36.1} & \textcolor{myred}{25.5} & \textcolor{myred}{6.6} & 31.7 & \textcolor{mygreen}{44.3} & \textcolor{myred}{12.0} & \textcolor{myred}{7.1} & 36.6\\
	&$p(k|f)$ & \textcolor{myred}{3.4} & \textcolor{mygreen}{69.3} & \textcolor{myred}{2.4} & 25.0 & \textcolor{myred}{2.3} & \textcolor{mygreen}{81.4} & \textcolor{myred}{1.5} & 14.9 & \textcolor{myred}{6.4} & \textcolor{mygreen}{38.3} & \textcolor{myred}{4.7} & 50.5\\
	&$p(k|o)$ & \textcolor{myred}{6.9} & \textcolor{myred}{16.2} & \textcolor{mygreen}{16.0} & 60.9 & \textcolor{myred}{8.2} & \textcolor{myred}{25.2} & \textcolor{mygreen}{15.5} & 51.2 & \textcolor{myred}{6.6} & \textcolor{myred}{13.5} & \textcolor{mygreen}{16.1} & 63.7\\
	&$p(k|u)$ & 3.3 & 6.5 & 8.0 & 82.1 & - & - & - & - & 3.3 & 6.4 & 8.0 & 82.2\\
	\hline
	\parbox[t]{2mm}{\multirow{4}{*}{\rotatebox[origin=c]{90}{\scriptsize{MonoDepth \& R$_{20}$}}}}&$p(k|d)$ & \textcolor{mygreen}{42.4} & \textcolor{myred}{14.3} & \textcolor{myred}{12.6} & 30.7 & \textcolor{mygreen}{38.0} & \textcolor{myred}{20.6} & \textcolor{myred}{13.0} & 28.5 & \textcolor{mygreen}{45.7} & \textcolor{myred}{11.3} & \textcolor{myred}{12.3} & 30.7\\
	&$p(k|f)$ & \textcolor{myred}{2.9} & \textcolor{mygreen}{69.6} & \textcolor{myred}{2.5} & 25.0 & \textcolor{myred}{2.2} & \textcolor{mygreen}{80.0} & \textcolor{myred}{1.6} & 16.1 & \textcolor{myred}{4.9} & \textcolor{mygreen}{42.3} & \textcolor{myred}{4.9} & 47.9\\
	&$p(k|o)$ & \textcolor{myred}{5.0} & \textcolor{myred}{12.0} & \textcolor{mygreen}{28.8} & 54.1 & \textcolor{myred}{6.4} & \textcolor{myred}{17.2} & \textcolor{mygreen}{30.0} & 46.4 & \textcolor{myred}{4.7} & \textcolor{myred}{10.5} & \textcolor{mygreen}{28.4} & 56.4\\
	&$p(k|u)$ & 2.0 & 8.1 & 8.0 & 81.9 & - & - & - & - & 1.9 & 8.0 & 8.0 & 82.1\\
	\hline
	\multicolumn{2}{c|}{\textbf{ShiftNet}} & \multicolumn{4}{c|}{overall} & \multicolumn{4}{c|}{visible} & \multicolumn{4}{c}{occluded}
\end{tabular}
\caption{\label{tab:deep_icm_results} For a detailed explanation of the table format kindly refer to \ref{tab:deep_ism_r20_results}.}
\end{figure}
Based on the false rates, the monocular depth input model slightly outperforms the remaining purely camera-based counterparts with an accumulated overall false rate of 52.7 as compared to 54.1, both for the RGB and semantic projections. Looking at the true rates, this distinction becomes even clearer with the RGB-based model providing the worst true rates in all categories (true rate sum of 103.1), followed by the semantic model (true rate sum 111.7) and, finally, with the mono depth model as the best purely camera-based model (true rate sum 126.1). This overall better performance of the MonoDepth \gls{ism} is also reflected in the overall decreased unknown mass compared to the other purely camera-based \gls{ism}s. This is also reflected in the qualitative results in Fig. \ref{fig:icm_qual_comp} by the more strongly highlighted occupied space and the increased correctness of free space shape of the MonoDepth model compared to the remaining purely camera-based models. 

The only prominent deficit of the MonoDepth \gls{ism} lies in its increased false rates for $p(d|o)$ and $p(o|d)$. Therefore, MonoDepth is used as a camera input representation for the further fusion experiments with radar.

These experiments show that, compared with the MonoDepth \gls{ism}, the additional radar information leads to less unknown mass and an overall improvement of positive and false rates throughout all classes. While this improvement is only marginal for the free class, the occupied true positive rate is increased by $12.8\%$ while also reducing the false rates. The only worsening is observed with regards to false occupied predictions in the dynamic class. Here, the false rate is almost doubled compared to the other camera \gls{ism}s. Comparing to the purely radar-based \gls{ism}, the MonoDepth-radar fusion provides better performance in each of the overall scores without exception. A qualitative example for the improvement of the fused over the pure radar \gls{ism} can be seen in the white box in the second example in Fig. \ref{fig:icm_qual_comp}. Here, the MonoDepth provides the additional information that the wall proceeds around the corner, leading the model to assign the area behind the wall as unknown rather than free.
\begin{figure}[H]
\begin{center}
	\import{imgs/07_deep_ism_exp/aleat_uncert}{icm_qual_comp.pdf_tex}
	\caption{\label{fig:icm_qual_comp}}
\end{center}
\end{figure}
%======================================%
%
%======================================%
\subsection{Discussion}
\label{subsec:discussion_icm}
These problems of the purely camera-based \gls{ism}s to distinguish between occupied and dynamic objects can also be qualitatively observed in the upper white box of the first example in Fig. \ref{fig:icm_qual_comp}. There, the shape of a moving bus shall be predicted
\begin{itemize}
	\item bus is not correctly classified in semseg and therefore there is no chance of the bev semseg ism to correctly classify it
	\item monodepth overall better representation
	\item monodepth lacks info of what is moving and what is occupied
	\item monodepth fusion with radar R$_{20}$ images shows that the radar information mainly benefits in distinguishing dynamic and occupied cells. Free overall performance remains as is.
	\item Thus, providing additional radar information in the proposed way makes it clearer that there is indeed an object but not in which motion state the object is. 
\end{itemize}
%==========================================================================%
%
%==========================================================================%
\section{Lidar-Radar Fusion deep ISMs}
\label{sec:lidar_radar_fusion_in_deep_isms}
%======================================%
%
%======================================%
\subsection{Experimental Results}
\label{subsec:exp_results_sensor_fusion}
\begin{tabular}{c|c|cccc|cccc|cccc}
	&$k$ & $d$ & $f$ & $o$ & $u$ & $d$ & $f$ & $o$ & $u$ & $d$ & $f$ & $o$ & $u$\\
	\hline
	\parbox[t]{2mm}{\multirow{4}{*}{\rotatebox[origin=c]{90}{\scriptsize{geo ILM}}}}&$p(k|d)$ & \textcolor{mygreen}{4.5} & \textcolor{myred}{27.6} & \textcolor{myred}{1.7} & 66.0 & \textcolor{mygreen}{19.8} & \textcolor{myred}{73.2} & \textcolor{myred}{3.6} & 3.0 & \textcolor{mygreen}{0.0} & \textcolor{myred}{1.3} & \textcolor{myred}{0.0} & 98.6\\
	&$p(k|f)$ & \textcolor{myred}{0.0} & \textcolor{mygreen}{72.0} & \textcolor{myred}{0.2} & 27.5 & \textcolor{myred}{0.0} & \textcolor{mygreen}{98.7} & \textcolor{myred}{0.3} & 0.6 & \textcolor{myred}{0.0} & \textcolor{mygreen}{1.1} & \textcolor{myred}{0.0} & 98.9\\
	&$p(k|o)$ & \textcolor{myred}{0.1} & \textcolor{myred}{15.4} & \textcolor{mygreen}{7.7} & 76.7 & \textcolor{myred}{0.3} & \textcolor{myred}{60.6} & \textcolor{mygreen}{32.6} & 6.1 & \textcolor{myred}{0.0} & \textcolor{myred}{0.9} & \textcolor{mygreen}{0.0} & 99.1\\
	&$p(k|u)$ & 0.0 & 0.7 & 0.1 & 99.2 & - & - & - & - & 0.0 & 0.1 & 0.0 & 99.9\\
	\hline
%	\multicolumn{13}{c}{}\\	
%	\hline
	\parbox[t]{2mm}{\multirow{4}{*}{\rotatebox[origin=c]{90}{\scriptsize{deep ILM}}}}&$p(k|d)$ & \textcolor{mygreen}{47.3} & \textcolor{myred}{10.5} & \textcolor{myred}{16.3} & 25.8 & \textcolor{mygreen}{42.0} & \textcolor{myred}{15.1} & \textcolor{myred}{17.9} & 25.1 & \textcolor{mygreen}{51.1} & \textcolor{myred}{7.5} & \textcolor{myred}{16.4} & 25.0\\
	&$p(k|f)$ & \textcolor{myred}{2.4} & \textcolor{mygreen}{78.8} & \textcolor{myred}{1.9} & 16.9 & \textcolor{myred}{1.6} & \textcolor{mygreen}{89.3} & \textcolor{myred}{0.9} & 8.1 & \textcolor{myred}{4.5} & \textcolor{mygreen}{51.1} & \textcolor{myred}{4.6} & 39.8\\
	&$p(k|o)$ & \textcolor{myred}{8.7} & \textcolor{myred}{8.4} & \textcolor{mygreen}{43.6} & 39.3 & \textcolor{myred}{11.0} & \textcolor{myred}{10.8} & \textcolor{mygreen}{46.5} & 31.7 & \textcolor{myred}{8.1} & \textcolor{myred}{7.5} & \textcolor{mygreen}{42.9} & 41.6\\
	&$p(k|u)$ & 3.0 & 8.2 & 8.7 & 80.1 & - & - & - & - & 3.0 & 8.0 & 8.7 & 80.3\\
	\hline
	\parbox[t]{2mm}{\multirow{4}{*}{\rotatebox[origin=c]{90}{\scriptsize{deep ILRM}}}}&$p(k|d)$ & \textcolor{mygreen}{47.3} & \textcolor{myred}{9.7} & \textcolor{myred}{18.3} & 24.8 & \textcolor{mygreen}{43.5} & \textcolor{myred}{13.6} & \textcolor{myred}{20.6} & 22.2 & \textcolor{mygreen}{50.6} & \textcolor{myred}{7.4} & \textcolor{myred}{17.4} & 24.6\\
	&$p(k|f)$ & \textcolor{myred}{1.8} & \textcolor{mygreen}{80.8} & \textcolor{myred}{1.9} & 15.5 & \textcolor{myred}{1.2} & \textcolor{mygreen}{89.8} & \textcolor{myred}{1.1} & 7.9 & \textcolor{myred}{3.6} & \textcolor{mygreen}{56.8} & \textcolor{myred}{4.1} & 35.4\\
	&$p(k|o)$ & \textcolor{myred}{6.0} & \textcolor{myred}{7.6} & \textcolor{mygreen}{46.8} & 39.5 & \textcolor{myred}{8.1} & \textcolor{myred}{9.6} & \textcolor{mygreen}{52.8} & 29.4 & \textcolor{myred}{5.5} & \textcolor{myred}{6.8} & \textcolor{mygreen}{45.2} & 42.5\\
	&$p(k|u)$ & 1.6 & 7.6 & 7.4 & 83.3 & - & - & - & - & 1.6 & 7.5 & 7.3 & 83.5\\
	\hline
	\multicolumn{2}{c|}{\textbf{ShiftNet}} & \multicolumn{4}{c|}{overall} & \multicolumn{4}{c|}{visible} & \multicolumn{4}{c}{occluded}
\end{tabular}
\begin{itemize}
	\item as expected, lidar has best performance for free and occupied classification for all ShiftNet inputs.
	\item more surprisingly, lidar is capable to distinguish between dynamic and occupied objects from mere context and achieves better performance than all other inputs for ShiftNet. 
	\item when fused with R$_{20}$, the overall performance further increases over all classes both in positive and false rates.
\end{itemize}
\begin{figure}[H]
	\begin{center}
		\import{imgs/07_deep_ism_exp/aleat_uncert}{ilm_qual_comp.pdf_tex}
		\caption{\label{fig:ilm_qual_comp}}
	\end{center}
\end{figure}
%==========================================================================%
%
%==========================================================================%
\section{Discussion}
\label{sec:deep_ism_discussion}
\begin{enumerate}
	\item geo \gls{irm} least false rates but also least positive rates $\rightarrow$ initialize with deep \gls{ism} model and use geo \gls{irm} to converge to secure estimates
	\item DirNet not properly capable to model dynamic objects. maybe due to the fact that only few samples are provided filtered out by the uncertainty mechanism
	\item homog rgb results really bad, maybe because not enough network capacity
\end{enumerate}