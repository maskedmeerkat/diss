% !TeX root = ../main.tex
\chapter{Introduction}
\label{ch:introduction}
Beginning with the DARPA "Grand Challenge" \cite{thrun2006stanley} in 2004 and the following DARPA "Urban Challenge" \cite{Urmson-2007-9708} in 2005, the feasibility for autonomous driving has been shown. This opened up the discussion how to revolutionize mobility and tackle some of the biggest problems of our time. 
\\\\
The main challenge that can be addressed with autonomous vehicles is the reduction of the approximately 1.3 million people that die each year worldwide in traffic accidents according to WHO \cite{world2018global}. Here, autonomous vehicles are less prone to human errors like distraction, fatigue or speeding. Additionally, the robotic vehicle can provide faster reaction times with the increase of technology and a full 360$^{\circ}$ coverage of the surroundings given the correct sensor setup. Furthermore, autonomous vehicles offer the potential to reduce inner city parking demand \cite{zhang2020impacts} and decrease the cost of ride hailing enabling mobility e.g. for elder people and potentially reducing emissions by making personal cars obsolete \cite{severino2021autonomous}.  
\\\\
In order to realize autonomous driving, companies either chose the top-down or bottom-up approach. Here, the top-down approach consists in over-equipping the vehicle with a multitude of sensors and compute power followed by a phase of iterative reduction and refinement. Prominent examples of which are e.g. Waymo \cite{sun2020scalability} and ArgoAI \cite{chang2019argoverse}. On the other hand, automakers, already established before the hype of autonomous driving like e.g. Ford or VW, often focused on iteratively improving their already existing \gls{adas} functions while at the same time being open for collaborations with the top-down approaching companies \cite{fordargovw2020}. The majority of these \gls{adas} functions are aimed to be deployed in non-commercial passenger vehicles and delivers assistance for applications like parking features \cite{fordvaletparking2021} or highway automation \cite{fordlanekeeping2021}.
\\\\
Nevertheless, for both fully autonomous and \gls{adas} features, the system needs to be capable to perform the three major tasks of a robotic system. These steps consist in perceiving the environment, planning actions on the perceived information and using a controller to execute the planned actions to interact with the world (see Fig. \ref{fig:basic_robot_system}).
%
\begin{figure}[H]
	\begin{center}
		\resizebox{0.8\textwidth}{!}{
		\import{imgs/01_introduction}{basic_robot_system.pdf_tex}}
		\caption{\label{fig:basic_robot_system}Block diagram of a basic robotic system.}
	\end{center}
\end{figure} 
%
The focus of this work lies on improving the perception block for current and near future passenger vehicles. Here, in order to perceive all obstacle locations in the full 360$^{\circ}$ surrounding of the ego vehicle, sensors like lidars, radars, cameras and ultrasonic sensors can be deployed (see e.g. NuScenes sensor setup \cite{caesar2020nuscenes}). The obstacle locations, however, are only readily provided by spinning 360$^{\circ}$ lidars, which, nowadays, are too expensive to be deployed in passenger vehicles. An example of such a lidar is the Velodyne VLP16 (Puck) ranging currently around 4,000 \$ according to \cite{cnet2018} and \cite{velodyne2018}. Therefore, these lidars are currently only deployed by companies focusing on autonomous driving or to provide ground truth information to verify \gls{adas} features. For the remaining sensors, additional steps have to be performed like e.g. temporal accumulation to densify the measurements (mostly for radar and ultrasonic) or domain adaption as applied to transform camera measurements into \gls{bev}. It shall further be mentioned that, while being small, robust and cheap, ultrasonic sensors only provide information for narrow parking maneuvers since their range is restriction to about 6 m (e.g. \cite{boschultrasonic2022}). Therefore, they are still found in nowadays passenger vehicles but will not be the focus of this work.
\\\\
Besides manually defining the transformation from sensor measurements to object locations, \gls{nn} models are recently utilized to learn the transformation from data. This alleviates e.g. the problem of iteratively adapting the manually defined model to every detected edge case while at the same time ensuring that the newly implemented changes do not interfere with the former performance of the model. Instead, a once trained \gls{nn} model can be can be steadily improved by collecting edge cases, including them into the database and retraining the model. While this sounds tempting, there are also downsides to the application of \gls{nn}s. Some of these downside include the lack of \gls{nn} to identify situations for which they have not been trained ("unknown unknowns"), the predictions might not be based on semantically meaningful features and they are sensitive to distributional shifts \cite{safetyfirst2019}.
\\\\
To mitigate these problems of \gls{nn}s and verify them for production, the article "Safety First for Automated Driving" \cite{safetyfirst2019} proposes the following. The \gls{nn} can be extended to also predict its uncertainty. Here, the aleatoric or data inherent and the epistemic or model related uncertainty have to be distinguished. The combination of these uncertainties can help downstream function modules to decide to which extent the predictions can be trusted. Additionally, the epistemic uncertainty can be used as an indicator whether to collect a data point for later retraining or not. Another way to verify \gls{nn} predictions is the utilization of a so called observer. The observer checks the predictions of the \gls{nn} e.g. given a set of rules (e.g. "Does the predicted trajectory coincide with any obstacle locations?"). Eventually, a module can be run in parallel to the \gls{nn} and predict the same target to provide redundancy. This can e.g. be realized using a slightly differently trained \gls{nn} or one of the manually defined white box models.
\\\\
In this work, the \gls{sota} of learned and handcrafted inverse sensor models is summarized in Ch. \ref{ch:state_of_the_art} and analyzed with regards to exemplary requirements found for nowadays passenger vehicle applications in Ch. \ref{ch:research_approach}. Based on these findings, Ch. \ref{ch:research_approach} continues to detail extensions to potentially improve the security of the best suited \gls{sota} \gls{nn} candidate following the guideline of \cite{safetyfirst2019}. More specifically, both \gls{sota} as well as newly proposed uncertainty estimation techniques are defined to modify the \gls{nn}'s predictions. Furthermore, a novel temporal fusion approach is defined in Ch. \ref{ch:research_approach} to tackle the challenges for fusion posed by the learned environment models used in this work. At the end of Ch. \ref{ch:research_approach}, a novel alternative temporal fusion is defined in which the \gls{nn} estimates have to be first verified by the handcrafted model. In this scenario, the handcrafted model acts as an observer for the \gls{nn} predictions which should potentially further strengthen the security aspect according to \cite{safetyfirst2019}.
\\\\
To analyze the benefits and downsides, both learned and handcrafted benchmark models are tuned for the investigated task in Ch. \ref{ch:deep_ism_exp}. Afterwards, the benchmark models are compared against each other for each sensor modality and against the proposed uncertainty extensions. Similarly, Ch. \ref{ch:deep_ev_isms_as_prior_for_occmaps_exp} contains the experiments to compare the \gls{sota} temporal fusion with the proposed variants to highlight the need for novel methods as well as show their limits. Eventually, the work is concluded with Ch. \ref{ch:discussion_of_req_n_rq} by a discussion of the obtained results with respect to the imposed requirements and an outlook for further work.
\\\\
It is expected that these improvements have the most benefit in the passenger vehicle domain given that high accuracy, dense 360$^{\circ}$ lidar sensors cannot be deployed in this domain at least within the next decade. These findings are aimed to improve the perception block of the robotic pipeline (see Fig. \ref{fig:basic_robot_system}) and, as such, will improve the performance of all downstream functions making \gls{adas} features overall more robust.