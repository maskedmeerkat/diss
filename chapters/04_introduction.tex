% !TeX root = ../main.tex
\chapter{Introduction}
\label{ch:introduction}
With the DARPA "Grand Challenges" \cite{thrun2006stanley} in 2004 and 2005 and the following DARPA "Urban Challenge" \cite{Urmson-2007-9708} in 2007, the feasibility for automated driving up to level 4 as defined by the \gls{sae} \cite{SAE2017} has been shown in controlled environments. This opened up the discussion how to revolutionize mobility. 
\\\\
The main challenge that can be addressed with automated vehicles is the reduction of the approximately 1.3 million people that die each year worldwide in traffic accidents according to WHO \cite{world2018global}. Automated vehicles are less prone to human errors like distraction, fatigue or speeding. Additionally, the automated vehicle can provide faster reaction times with the increase of technology and a full 360$^{\circ}$ coverage of the surroundings given the correct sensor setup. Furthermore, automated vehicles offer the potential to reduce inner city parking demand \cite{zhang2020impacts} and may decrease the cost of ride hailing, enabling mobility e.g. for elder people and potentially reducing emissions by making personal cars obsolete \cite{severino2021autonomous}.  
\\\\
In order to realize automated driving, companies either chose the top-down or bottom-up approach. The top-down approach consists in over-equipping the vehicle with a multitude of sensors and compute power followed by a phase of iterative reduction and refinement to directly target \gls{sae} level 4-5 automation. Prominent examples of which are e.g. Waymo \cite{sun2020scalability} and ArgoAI \cite{chang2019argoverse}. On the other hand, most "traditional" automakers like Ford, VW or BMW, often focused on iteratively improving their already existing automated functions (\gls{sae} level 1-2) to higher levels of automation while at the same time being open for collaborations with the top-down approaching companies \cite{fordargovw2020}.
\\\\
In order to automate more sophisticated driving tasks (e.g. parking or lane changes), the vehicle has to be capable to perform the basic steps of robotic systems, namely sense, plan and act \cite{arkin1998behavior}. Based on the degree of planning, one can distinguish the reactive paradigm which uses fixed mappings between sense and act, hierarchical paradigm with online planning and hybrid paradigm with steps-wise online planning. An illustration of the hierarchical paradigm is depicted in Fig. \ref{fig:basic_robot_system}. However, for all of those paradigms the sensing block plays a crucial role as it provides the baseline for all of the upstream tasks. This sensing block can be further divided into sensory processing followed by world modeling (see \cite{arkin1998behavior}).
\begin{center}
	\import{imgs/01_introduction}{basic_robot_system.pdf_tex}
	\captionof{figure}{\label{fig:basic_robot_system}Block diagram of a hierarchical robotic system.}
\end{center}
The focus of this work lies on improving the world modeling capability of the sensing block for current and near future passenger vehicles. In order to perceive all obstacle locations in the full 360$^{\circ}$ surrounding of the ego vehicle, sensors like lidars, radars, cameras and ultrasonic sensors can be deployed (see e.g. NuScenes sensor setup \cite{caesar2020nuscenes}). The obstacle locations, however, are only readily provided by spinning 360$^{\circ}$ lidars, which, nowadays, are too expensive to be deployed in passenger vehicles. An example of such a lidar is the Velodyne VLP16 (Puck) ranging currently around 4,000 \$ according to \cite{cnet2018} and \cite{velodyne2018}. Therefore, these lidars are currently only deployed by companies focusing on automated driving (\gls{sae} level 4-5) or to provide ground truth information to verify automated driving features up to \gls{sae} level 3. For the remaining sensors, additional steps have to be performed like e.g. temporal accumulation to densify the measurements (mostly for radar and ultrasonic sensors) or domain adaption as applied to transform camera measurements into \gls{bev}. It shall further be mentioned that, while being small, robust and cheap, ultrasonic sensors only provide information for narrow parking maneuvers since their range is restriction to about 6 m (e.g. \cite{boschultrasonic2022}). Therefore, they are still found in nowadays passenger vehicles but will not be included in this work.
\\\\
Besides manually defining the transformation from sensor measurements to object locations, \gls{nn} models are recently utilized to learn the transformation from data. This alleviates e.g. the problem of iteratively adapting the handcrafted model to every possible edge case while at the same time ensuring that the newly implemented changes do not interfere with the former performance of the model. Instead, a once trained \gls{nn} model can be steadily improved by collecting edge cases, including them into the database and retraining the model. While this sounds tempting, there are also downsides to the application of \gls{nn}s. Some of these downside include the lack of \gls{nn}s to identify situations for which they have not been trained ("unknown unknowns"). In such situations, the predictions might not be based on semantically meaningful features and, as such, are sensitive to distributional shifts \cite{safetyfirst2019}.
\\\\
To mitigate these problems of \gls{nn}s and verify them for production, the article "Safety First for Automated Driving" \cite{safetyfirst2019} proposes the following. The \gls{nn} can be extended to also predict its uncertainty. In this context, the aleatoric or data inherent and the epistemic or model related uncertainty have to be distinguished. The combination of these uncertainties can help downstream function modules to decide to which extent the predictions can be trusted. Additionally, the epistemic uncertainty can be used as an indicator whether to collect a data point for later retraining or not. Another way to verify \gls{nn} predictions is the utilization of a so called observer. The observer checks the predictions of the \gls{nn} e.g. given a set of rules like "Does the predicted trajectory coincide with any obstacle locations?". Eventually, a module can be run in parallel to the \gls{nn} and predict the same target to provide redundancy. This can e.g. be realized using a slightly differently trained \gls{nn} or one of the manually defined white box models.
\\\\
In this work, the \gls{sota} of learned and handcrafted inverse sensor models is summarized in Chapter \ref{ch:state_of_the_art} and analyzed with regards to exemplary requirements found for nowadays passenger vehicle applications in Chapter \ref{ch:research_approach}. Based on these findings, Chapter \ref{ch:research_approach} continues to detail extensions to potentially improve the robustness of the best suited \gls{sota} \gls{nn} candidate following the guideline of \cite{safetyfirst2019}. More specifically, both \gls{sota} as well as newly proposed uncertainty estimation techniques are defined to modify the \gls{nn}'s predictions. Furthermore, a novel temporal fusion approach is defined in Chapter \ref{ch:research_approach} to tackle the challenges that occur when temporally accumulating the learned environment models used in this work. At the end of Chapter \ref{ch:research_approach}, a novel alternative temporal fusion is defined in which the \gls{nn} estimates have to be first verified by the handcrafted model. In this scenario, the handcrafted model acts as an observer for the \gls{nn} predictions which should potentially further strengthen the robustness aspect according to \cite{safetyfirst2019}.
\\\\
To analyze the benefits and downsides, both learned and handcrafted benchmark models are tuned for the investigated task in Chapter \ref{ch:deep_ism_exp}. Afterwards, the benchmark models are compared against each other for each sensor modality and against the proposed uncertainty extensions. Similarly, Chapter \ref{ch:deep_ev_isms_as_prior_for_occmaps_exp} contains the experiments to compare the \gls{sota} temporal fusion with the proposed variants to highlight the need for the proposed novel fusion approach as well as show its limitations. Eventually, Chapter \ref{ch:discussion_of_req_n_rq} concludes the work with a discussion of the obtained results with respect to the imposed requirements and an outlook for further work.
\\\\
It is expected that these improvements have the most benefit in the passenger vehicle domain under the assumption that high accuracy, dense 360$^{\circ}$ lidar sensors cannot be deployed in this domain at least within the next decade. These findings are aimed to improve the perception block of the robotic pipeline (see Fig. \ref{fig:basic_robot_system}) and, as such, will improve the performance of all downstream functions increasing the overall robustness of automated driving features.